"""Conformal prediction confidence scoring for attribution.

Wraps attribution confidence in conformal prediction sets to ensure
calibrated uncertainty quantification. When the system says "90% confident",
this module guarantees that the prediction set covers the true label at
least 90% of the time.

Implements the **Adaptive Prediction Sets (APS)** method:

1. Sort candidate labels by decreasing confidence.
2. Include labels in the prediction set until cumulative confidence
   reaches the target coverage level.
3. The resulting set size reflects true uncertainty: larger sets mean
   more ambiguity.

The ``CalibrationReport`` tracks Expected Calibration Error (ECE) using
equal-width binning to detect systematic over- or under-confidence.

Notes
-----
Implements the conformal prediction framework described in Teikari (2026),
Section 5.2. Based on the theoretical foundations of Vovk et al. (2005),
"Algorithmic Learning in a Random World."

References
----------
.. [1] Vovk, V., Gammerman, A., & Shafer, G. (2005). "Algorithmic
   Learning in a Random World." Springer.
.. [2] Romano, Y., Sesia, M., & Candes, E. (2020). "Classification
   with Valid and Adaptive Coverage." NeurIPS.

See Also
--------
music_attribution.attribution.aggregator : Upstream confidence aggregation.
music_attribution.schemas.attribution.ConformalSet : Pydantic model for prediction sets.
"""

from __future__ import annotations

import logging
from datetime import UTC, datetime

from pydantic import BaseModel, Field

from music_attribution.schemas.attribution import ConformalSet
from music_attribution.schemas.enums import CreditRoleEnum

logger = logging.getLogger(__name__)


class CalibrationReport(BaseModel):
    """Report on calibration quality using equal-width binning.

    Tracks Expected Calibration Error (ECE) and per-bin accuracy vs.
    confidence, enabling detection of systematic over- or
    under-confidence in the scoring model.

    Attributes
    ----------
    ece : float
        Expected Calibration Error -- weighted average of per-bin
        ``|accuracy - confidence|``. Lower is better; 0.0 means
        perfectly calibrated. Must be >= 0.0.
    marginal_coverage : float
        Achieved coverage (fraction of correct predictions) in [0, 1].
    target_coverage : float
        Target coverage level (e.g., 0.9 for 90% coverage).
    calibration_method : str
        Name of the calibration method (e.g., ``"APS"``).
    calibration_set_size : int
        Number of samples used for calibration.
    bin_accuracies : list[float]
        Per-bin accuracy values (10 bins by default).
    bin_confidences : list[float]
        Per-bin mean confidence values (10 bins by default).
    timestamp : datetime
        When the calibration was computed (UTC).
    """

    ece: float = Field(ge=0.0)  # Expected Calibration Error
    marginal_coverage: float = Field(ge=0.0, le=1.0)
    target_coverage: float = Field(ge=0.0, le=1.0)
    calibration_method: str
    calibration_set_size: int = Field(ge=0)
    bin_accuracies: list[float] = Field(default_factory=list)
    bin_confidences: list[float] = Field(default_factory=list)
    timestamp: datetime = Field(default_factory=lambda: datetime.now(UTC))


class ConformalScorer:
    """Conformal prediction scorer for attribution confidence.

    Uses the Adaptive Prediction Sets (APS) method to produce calibrated
    prediction sets at specified coverage levels. The prediction set
    includes the minimum number of candidate labels needed to achieve
    the target coverage.

    Notes
    -----
    A well-calibrated model produces small prediction sets (often just 1
    label) for high-confidence predictions and larger sets for ambiguous
    cases. The set size itself is a useful uncertainty signal.

    See Also
    --------
    music_attribution.attribution.aggregator.CreditAggregator :
        Produces the raw confidence scores that are calibrated here.
    """

    def score(
        self,
        predictions: list[tuple[CreditRoleEnum, float]],
        coverage: float = 0.90,
    ) -> ConformalSet:
        """Produce a conformal prediction set at the specified coverage level.

        Sorts candidate roles by confidence descending, then includes
        roles until cumulative confidence reaches the target coverage.
        If total confidence across all candidates is less than the target,
        all candidates are included.

        Parameters
        ----------
        predictions : list[tuple[CreditRoleEnum, float]]
            List of ``(role, confidence)`` tuples. Confidence values
            should sum to approximately 1.0 for well-calibrated models.
        coverage : float, optional
            Target coverage level. Default is 0.90 (90% coverage).

        Returns
        -------
        ConformalSet
            Prediction set with coverage metadata. The ``set_sizes``
            field indicates how many labels were needed to achieve
            coverage (smaller = more confident).
        """
        if not predictions:
            return ConformalSet(
                coverage_level=coverage,
                prediction_sets={},
                set_sizes={},
                marginal_coverage=coverage,
                calibration_error=0.0,
                calibration_method="APS",
                calibration_set_size=0,
            )

        # Sort by confidence descending
        sorted_preds = sorted(predictions, key=lambda x: x[1], reverse=True)

        # Build prediction set: include roles until cumulative confidence >= coverage
        prediction_set: list[CreditRoleEnum] = []
        cumulative = 0.0
        for role, conf in sorted_preds:
            prediction_set.append(role)
            cumulative += conf
            if cumulative >= coverage:
                break

        # If we still haven't reached coverage, include all
        if cumulative < coverage:
            prediction_set = [role for role, _ in sorted_preds]

        # Compute marginal coverage (achieved coverage)
        total_conf = sum(c for _, c in sorted_preds)
        marginal = min(cumulative / total_conf, 1.0) if total_conf > 0 else 0.0

        # Calibration error (simplified â€” deviation from target)
        cal_error = abs(marginal - coverage)

        return ConformalSet(
            coverage_level=coverage,
            prediction_sets={"default": prediction_set},
            set_sizes={"default": len(prediction_set)},
            marginal_coverage=marginal,
            calibration_error=cal_error,
            calibration_method="APS",
            calibration_set_size=len(predictions),
        )

    def calibrate(
        self,
        predictions: list[tuple[float, bool]],
    ) -> CalibrationReport:
        """Compute calibration metrics from predictions vs. actual outcomes.

        Uses equal-width binning (10 bins spanning [0, 1]) to compute:

        - **ECE** (Expected Calibration Error): weighted average of
          per-bin ``|accuracy - confidence|``.
        - **Per-bin accuracy**: fraction of correct predictions in each bin.
        - **Per-bin confidence**: mean predicted probability in each bin.
        - **Marginal coverage**: overall fraction of correct predictions.

        Parameters
        ----------
        predictions : list[tuple[float, bool]]
            List of ``(predicted_probability, actual_outcome)`` tuples.
            ``predicted_probability`` is in [0, 1]; ``actual_outcome``
            is ``True`` if the prediction was correct.

        Returns
        -------
        CalibrationReport
            Calibration metrics including ECE and per-bin breakdowns.
            Returns an empty report (ECE=0) if no predictions are provided.

        Notes
        -----
        A perfectly calibrated model has ECE=0 and bin accuracies that
        match bin confidences (i.e., the reliability diagram is a 45-degree
        line). See Naeini et al. (2015) for ECE methodology.
        """
        if not predictions:
            return CalibrationReport(
                ece=0.0,
                marginal_coverage=0.0,
                target_coverage=0.9,
                calibration_method="APS",
                calibration_set_size=0,
            )

        # Bin predictions into 10 bins
        n_bins = 10
        bin_sums: list[float] = [0.0] * n_bins
        bin_correct: list[int] = [0] * n_bins
        bin_counts: list[int] = [0] * n_bins

        for prob, actual in predictions:
            bin_idx = min(int(prob * n_bins), n_bins - 1)
            bin_sums[bin_idx] += prob
            bin_correct[bin_idx] += int(actual)
            bin_counts[bin_idx] += 1

        # Compute per-bin accuracy and confidence
        bin_accuracies: list[float] = []
        bin_confidences: list[float] = []
        ece = 0.0
        total = len(predictions)

        for i in range(n_bins):
            if bin_counts[i] > 0:
                acc = bin_correct[i] / bin_counts[i]
                conf = bin_sums[i] / bin_counts[i]
                bin_accuracies.append(acc)
                bin_confidences.append(conf)
                ece += (bin_counts[i] / total) * abs(acc - conf)
            else:
                bin_accuracies.append(0.0)
                bin_confidences.append(0.0)

        # Compute actual coverage
        correct_count = sum(1 for _, actual in predictions if actual)
        marginal_coverage = correct_count / total if total > 0 else 0.0

        return CalibrationReport(
            ece=ece,
            marginal_coverage=marginal_coverage,
            target_coverage=0.9,
            calibration_method="APS",
            calibration_set_size=total,
            bin_accuracies=bin_accuracies,
            bin_confidences=bin_confidences,
        )
