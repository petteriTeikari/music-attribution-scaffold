# Figure Alt Text Catalog — Music Attribution Scaffold

> SEO/GEO-optimized alt text for all repository documentation figures.
> Generated by the figure-processor skill. See [SKILL.md](../../.claude/skills/figure-processor/SKILL.md).
>
> **Last updated:** 2026-02-20 | **Figures:** 262 (238 generated + 24 pending generation)

---

## How to Use This Catalog

- **Copy-paste** the embed snippet when adding a figure to documentation
- **Alt text** is optimized for three audiences: screen readers, search engines (SEO), and LLMs (GEO)
- **Paths** are repo-root-relative — adjust if embedding from a different location

---

## Agentic UI (fig-agent-*)

### fig-agent-01: Agentic UI Full Stack

**Alt text:** Architecture diagram: end-to-end agentic UI stack for open-source music attribution showing CopilotKit sidebar sending messages via AG-UI Server-Sent Events to a FastAPI backend where a PydanticAI agent queries music metadata with four domain tools, then streams transparent confidence scoring responses back through bidirectional DuetUI context hooks.

**Caption:** *Full-stack agentic UI architecture for the Music Attribution Scaffold, tracing the complete request path from CopilotKit sidebar through AG-UI SSE protocol to a PydanticAI agent with four domain-specific tools and back via streamed state snapshots.*

**Embed:**
```markdown
![Architecture diagram: end-to-end agentic UI stack for open-source music attribution showing CopilotKit sidebar sending messages via AG-UI Server-Sent Events to a FastAPI backend where a PydanticAI agent queries music metadata with four domain tools, then streams transparent confidence scoring responses back through bidirectional DuetUI context hooks.](docs/figures/repo-figures/assets/fig-agent-01-full-stack.jpg)
```

---

### fig-agent-02: PydanticAI Agent Architecture

**Alt text:** Architecture diagram: PydanticAI agent for music attribution scaffold showing agent configuration with domain-specific system prompt, AgentDeps dependency injection for database access, four registered tools for transparent confidence scoring of music credits, and model routing via environment variable defaulting to Claude Haiku for cost-efficient attribution queries.

**Caption:** *PydanticAI agent architecture detailing the four domain tools (explain_confidence, search_attributions, suggest_correction, submit_feedback), their input/output signatures, and the AgentDeps dataclass that provides shared state and PostgreSQL session access.*

**Embed:**
```markdown
![Architecture diagram: PydanticAI agent for music attribution scaffold showing agent configuration with domain-specific system prompt, AgentDeps dependency injection for database access, four registered tools for transparent confidence scoring of music credits, and model routing via environment variable defaulting to Claude Haiku for cost-efficient attribution queries.](docs/figures/repo-figures/assets/fig-agent-02-pydantic-ai-architecture.jpg)
```

---

### fig-agent-03: AG-UI Protocol Flow

**Alt text:** Sequence diagram: AG-UI protocol flow for music attribution agentic UI showing the complete Server-Sent Events lifecycle from CopilotKit POST request through six SSE event types including chunked text streaming of confidence scoring explanations and a StateSnapshot that synchronizes music metadata state between the PydanticAI backend and the frontend sidebar.

**Caption:** *AG-UI SSE event sequence as implemented in the open-source attribution scaffold, showing the fixed order of RunStarted, TextMessageStart, chunked TextMessageContent (50-character segments), TextMessageEnd, StateSnapshot, and RunFinished events streamed from FastAPI to CopilotKit.*

**Embed:**
```markdown
![Sequence diagram: AG-UI protocol flow for music attribution agentic UI showing the complete Server-Sent Events lifecycle from CopilotKit POST request through six SSE event types including chunked text streaming of confidence scoring explanations and a StateSnapshot that synchronizes music metadata state between the PydanticAI backend and the frontend sidebar.](docs/figures/repo-figures/assets/fig-agent-03-agui-protocol-flow.jpg)
```

---

### fig-agent-04: CopilotKit Integration

**Alt text:** Integration diagram: CopilotKit bidirectional context for music attribution showing the provider with graceful degradation, sidebar component for credit queries, useCopilotReadable hooks that feed user role and selected music metadata to the agent, and four useCopilotAction hooks enabling the agent to navigate works, highlight credits, and show transparent confidence scoring diffs in the agentic UI.

**Caption:** *CopilotKit DuetUI integration in the attribution scaffold frontend, illustrating the bidirectional context loop where useCopilotReadable sends user role and selected attribution record to the PydanticAI agent while useCopilotAction lets the agent manipulate the UI in response.*

**Embed:**
```markdown
![Integration diagram: CopilotKit bidirectional context for music attribution showing the provider with graceful degradation, sidebar component for credit queries, useCopilotReadable hooks that feed user role and selected music metadata to the agent, and four useCopilotAction hooks enabling the agent to navigate works, highlight credits, and show transparent confidence scoring diffs in the agentic UI.](docs/figures/repo-figures/assets/fig-agent-04-copilotkit-integration.jpg)
```

---

### fig-agent-05: Agent Tool: explain_confidence

**Alt text:** Flow diagram: explain_confidence agent tool for transparent confidence scoring in the music attribution scaffold, tracing execution from work ID input through database lookup, extraction of source agreement and assurance level from music metadata, generation of human-readable confidence factors, shared state update, and natural language explanation output that makes attribution scores interpretable.

**Caption:** *Execution path of the explain_confidence PydanticAI tool, which decomposes a music attribution confidence score into human-readable factors -- source agreement level, source count and names, and assurance tier -- enabling transparent confidence scoring for music credits.*

**Embed:**
```markdown
![Flow diagram: explain_confidence agent tool for transparent confidence scoring in the music attribution scaffold, tracing execution from work ID input through database lookup, extraction of source agreement and assurance level from music metadata, generation of human-readable confidence factors, shared state update, and natural language explanation output that makes attribution scores interpretable.](docs/figures/repo-figures/assets/fig-agent-05-tool-get-work.jpg)
```

---

### fig-agent-06: Agent Tool: search_attributions

**Alt text:** Flow diagram: search_attributions agent tool for querying music metadata in the open-source attribution scaffold, showing hybrid text-and-vector search across attribution records, result formatting with confidence scoring percentages and assurance levels per hit, and state synchronization enabling the agentic UI to display music credits search results in real time.

**Caption:** *Execution path of the search_attributions PydanticAI tool, which uses the HybridSearchService to find up to 10 matching music attribution records and formats each result with title, artist, transparent confidence score, and assurance level.*

**Embed:**
```markdown
![Flow diagram: search_attributions agent tool for querying music metadata in the open-source attribution scaffold, showing hybrid text-and-vector search across attribution records, result formatting with confidence scoring percentages and assurance levels per hit, and state synchronization enabling the agentic UI to display music credits search results in real time.](docs/figures/repo-figures/assets/fig-agent-06-tool-explain-confidence.jpg)
```

---

### fig-agent-07: Model Failover Strategy

**Alt text:** Configuration diagram: PydanticAI FallbackModel failover strategy for the music attribution scaffold agent, showing environment variable configuration defaulting to Claude Haiku for fast and cost-efficient music metadata queries, automatic cascading to more capable models on failure, and retry mechanisms that ensure reliable transparent confidence scoring responses for music credits.

**Caption:** *Model failover strategy for the open-source attribution agent, illustrating the configuration chain from ATTRIBUTION_AGENT_MODEL environment variable through PydanticAI Settings to the FallbackModel cascade with automatic retry on transient failures.*

**Embed:**
```markdown
![Configuration diagram: PydanticAI FallbackModel failover strategy for the music attribution scaffold agent, showing environment variable configuration defaulting to Claude Haiku for fast and cost-efficient music metadata queries, automatic cascading to more capable models on failure, and retry mechanisms that ensure reliable transparent confidence scoring responses for music credits.](docs/figures/repo-figures/assets/fig-agent-07-model-failover.jpg)
```

---

### fig-agent-08: Agent Conversation Flow

**Alt text:** Conversation flow diagram: multi-turn agentic interaction for music attribution showing a user querying confidence for a track, the agent chaining search_attributions and explain_confidence tools to deliver transparent confidence scoring with source agreement breakdown, then suggesting a music credits correction with live state updates reflected in the CopilotKit sidebar at each step.

**Caption:** *Example two-turn conversation with the open-source attribution agent, demonstrating how tool calls chain (search, explain, suggest) with shared state propagation so the agentic UI updates in real time as the user explores music metadata confidence.*

**Embed:**
```markdown
![Conversation flow diagram: multi-turn agentic interaction for music attribution showing a user querying confidence for a track, the agent chaining search_attributions and explain_confidence tools to deliver transparent confidence scoring with source agreement breakdown, then suggesting a music credits correction with live state updates reflected in the CopilotKit sidebar at each step.](docs/figures/repo-figures/assets/fig-agent-08-conversation-flow.jpg)
```

---

### fig-agent-09: Voice Agent Upsell UI

**Alt text:** UI component mockup: voice agent Pro-tier upsell banner for the music attribution scaffold featuring a coral microphone icon, aspirational example query about music credits, and upgrade call-to-action, demonstrating the open-source project's product design philosophy of subtle feature gating without implementing actual voice processing for music metadata queries.

**Caption:** *VoiceAgentBanner component showing the aspirational Pro-tier upsell surface for voice-based music attribution queries, designed to be subtle and dismissable with no actual voice processing backend -- a UI-only feature gate in the attribution scaffold.*

**Embed:**
```markdown
![UI component mockup: voice agent Pro-tier upsell banner for the music attribution scaffold featuring a coral microphone icon, aspirational example query about music credits, and upgrade call-to-action, demonstrating the open-source project's product design philosophy of subtle feature gating without implementing actual voice processing for music metadata queries.](docs/figures/repo-figures/assets/fig-agent-09-voice-agent-upsell.jpg)
```

---

### fig-agent-10: Agent Testing Strategy

**Alt text:** Testing strategy diagram: deterministic test approach for the music attribution scaffold agentic UI showing 11 integration tests covering proficiency model thresholds, PostHog event schema validation, and feature flag mappings, plus backend mock agent pattern for endpoint testing, illustrating how to test transparent confidence scoring behavior around non-deterministic AI without requiring live LLM calls for music metadata queries.

**Caption:** *Agent testing strategy for the open-source attribution scaffold, demonstrating the principle of testing the deterministic ring (proficiency thresholds, event schemas, tool contracts) around the non-deterministic AI core, with mock agent injection via Python unittest.mock for backend endpoint tests.*

**Embed:**
```markdown
![Testing strategy diagram: deterministic test approach for the music attribution scaffold agentic UI showing 11 integration tests covering proficiency model thresholds, PostHog event schema validation, and feature flag mappings, plus backend mock agent pattern for endpoint testing, illustrating how to test transparent confidence scoring behavior around non-deterministic AI without requiring live LLM calls for music metadata queries.](docs/figures/repo-figures/assets/fig-agent-10-testing-strategy.jpg)
```

---

## Backend Architecture (fig-backend-*)

### fig-backend-01: ETL Pipeline Overview

**Alt text:** Pipeline diagram showing the open-source music attribution ETL pipeline where five music metadata sources — MusicBrainz, Discogs, AcoustID, tinytag file reader, and Artist Input — converge through token bucket rate limiting and a three-check data quality gate into a unified NormalizedRecord boundary object, enabling transparent confidence scoring downstream.

**Caption:** *Figure 1. The ETL pipeline extracts music metadata from five independent sources, each with distinct APIs and rate limits, normalizing all records into a common NormalizedRecord schema before entity resolution or confidence scoring occurs.*

**Embed:**
```markdown
![Pipeline diagram showing the open-source music attribution ETL pipeline where five music metadata sources — MusicBrainz, Discogs, AcoustID, tinytag file reader, and Artist Input — converge through token bucket rate limiting and a three-check data quality gate into a unified NormalizedRecord boundary object, enabling transparent confidence scoring downstream.](docs/figures/repo-figures/assets/fig-backend-01-etl-pipeline-overview.jpg)
```

---

### fig-backend-02: NormalizedRecord Schema

**Alt text:** Data schema diagram of the NormalizedRecord Pydantic model used in the music attribution scaffold, displaying 12 top-level fields including source confidence scoring, a nested IdentifierBundle with seven music metadata identifiers (ISRC, ISWC, ISNI, IPI, MBID, Discogs ID, AcoustID), SourceMetadata with eight fields, and three validation rules ensuring data integrity for transparent music credits.

**Caption:** *Figure 2. The NormalizedRecord (BO-1) is the universal boundary object that every ETL connector must produce, ensuring a consistent schema across MusicBrainz, Discogs, AcoustID, file metadata, and artist input sources.*

**Embed:**
```markdown
![Data schema diagram of the NormalizedRecord Pydantic model used in the music attribution scaffold, displaying 12 top-level fields including source confidence scoring, a nested IdentifierBundle with seven music metadata identifiers (ISRC, ISWC, ISNI, IPI, MBID, Discogs ID, AcoustID), SourceMetadata with eight fields, and three validation rules ensuring data integrity for transparent music credits.](docs/figures/repo-figures/assets/fig-backend-02-normalized-record-schema.jpg)
```

---

### fig-backend-03: Data Quality Gate

**Alt text:** Pipeline diagram of the music attribution data quality gate showing three batch validation checks — identifier coverage at 50% threshold, duplicate detection by source and ID, and source distribution capping any single source at 95% — with pass, warn, and fail outcome paths that prevent low-quality music metadata from entering the entity resolution pipeline.

**Caption:** *Figure 3. The DataQualityGate enforces three statistical checks on every batch of NormalizedRecords, gating entry to the entity resolution pipeline with configurable thresholds and a QualityReport that enables pipeline observability.*

**Embed:**
```markdown
![Pipeline diagram of the music attribution data quality gate showing three batch validation checks — identifier coverage at 50% threshold, duplicate detection by source and ID, and source distribution capping any single source at 95% — with pass, warn, and fail outcome paths that prevent low-quality music metadata from entering the entity resolution pipeline.](docs/figures/repo-figures/assets/fig-backend-03-data-quality-gate.jpg)
```

---

### fig-backend-04: Source-Specific Extraction

**Alt text:** Comparison diagram of five open-source music metadata ETL extractors — MusicBrainz (0.90 confidence, ISRC and credits), Discogs (0.85, personnel roles), AcoustID (variable, fingerprint matching), tinytag (0.70, local file tags), and Artist Input (0.60, declarations) — showing each connector's library, default source confidence, and returned data fields for transparent music attribution.

**Caption:** *Figure 4. Each of the five ETL connectors excels at different music metadata: MusicBrainz provides ISRCs and relationship credits, Discogs supplies detailed personnel roles, AcoustID enables audio fingerprint matching, while source confidence values reflect data authority rather than match scores.*

**Embed:**
```markdown
![Comparison diagram of five open-source music metadata ETL extractors — MusicBrainz (0.90 confidence, ISRC and credits), Discogs (0.85, personnel roles), AcoustID (variable, fingerprint matching), tinytag (0.70, local file tags), and Artist Input (0.60, declarations) — showing each connector's library, default source confidence, and returned data fields for transparent music attribution.](docs/figures/repo-figures/assets/fig-backend-04-source-specific-extraction.jpg)
```

---

### fig-backend-05: Rate Limiting Strategy

**Alt text:** Architecture diagram of the token bucket rate limiter used in the music attribution scaffold, showing per-source API rate configurations — MusicBrainz at 1 req/s, Discogs at 1 req/s authenticated, and AcoustID at 3 req/s — with async locking, exponential backoff retry strategy, and capacity settings ensuring compliant music metadata extraction.

**Caption:** *Figure 5. The TokenBucketRateLimiter enforces per-source API rate limits with asyncio locking and exponential backoff (2^attempt seconds), ensuring that the ETL pipeline remains compliant with MusicBrainz, Discogs, and AcoustID usage policies.*

**Embed:**
```markdown
![Architecture diagram of the token bucket rate limiter used in the music attribution scaffold, showing per-source API rate configurations — MusicBrainz at 1 req/s, Discogs at 1 req/s authenticated, and AcoustID at 3 req/s — with async locking, exponential backoff retry strategy, and capacity settings ensuring compliant music metadata extraction.](docs/figures/repo-figures/assets/fig-backend-05-rate-limiting-strategy.jpg)
```

---

### fig-backend-06: Resolution Orchestrator Flow

**Alt text:** Flow diagram of the multi-signal entity resolution pipeline in the music attribution scaffold, showing a cascade from exact identifier matching (ISRC, ISWC, ISNI, MBID) through fuzzy string similarity to per-group resolution using six weighted signals — identifier (1.0), Splink probabilistic linkage (0.8), embedding semantic match (0.7), graph evidence (0.75), string similarity (0.6), and LLM disambiguation (0.85) — producing ResolvedEntity output with assurance levels A0-A3.

**Caption:** *Figure 6. The ResolutionOrchestrator implements a cascade strategy — exact identifiers first, then string similarity for unmatched records — with six weighted signals contributing to a final resolution confidence that maps to assurance levels A0-A3.*

**Embed:**
```markdown
![Flow diagram of the multi-signal entity resolution pipeline in the music attribution scaffold, showing a cascade from exact identifier matching (ISRC, ISWC, ISNI, MBID) through fuzzy string similarity to per-group resolution using six weighted signals — identifier (1.0), Splink probabilistic linkage (0.8), embedding semantic match (0.7), graph evidence (0.75), string similarity (0.6), and LLM disambiguation (0.85) — producing ResolvedEntity output with assurance levels A0-A3.](docs/figures/repo-figures/assets/fig-backend-06-resolution-orchestrator-flow.jpg)
```

---

### fig-backend-07: String Similarity Methods

**Alt text:** Comparison diagram of two string similarity algorithms for music attribution entity resolution — Jaro-Winkler (via jellyfish) for typo detection in short names and token sort ratio (via thefuzz) for word reordering — with a music-domain normalization pipeline expanding abbreviations like "feat." and "prod.", using the maximum of both scores at a 0.85 match threshold for transparent confidence scoring of music credits.

**Caption:** *Figure 7. Two complementary string similarity methods handle different failure modes in music metadata: Jaro-Winkler catches single-character typos while token sort ratio handles name reordering, both preceded by domain-specific normalization of music abbreviations.*

**Embed:**
```markdown
![Comparison diagram of two string similarity algorithms for music attribution entity resolution — Jaro-Winkler (via jellyfish) for typo detection in short names and token sort ratio (via thefuzz) for word reordering — with a music-domain normalization pipeline expanding abbreviations like "feat." and "prod.", using the maximum of both scores at a 0.85 match threshold for transparent confidence scoring of music credits.](docs/figures/repo-figures/assets/fig-backend-07-string-similarity-methods.jpg)
```

---

### fig-backend-08: pgvector Embedding Match

**Alt text:** Architecture diagram of semantic embedding-based entity resolution for music attribution, showing entity names encoded by the all-MiniLM-L6-v2 sentence-transformer model into 768-dimensional vectors, stored in PostgreSQL pgvector HALFVEC columns, with cosine similarity search catching translations and alias variations that string matching misses — weighted at 0.7 in the open-source attribution scaffold's confidence scoring pipeline.

**Caption:** *Figure 8. When string similarity fails for translated or heavily aliased music entity names, semantic embeddings in pgvector HALFVEC(768) capture meaning-level similarity, enabling the attribution scaffold to resolve entities like "Tchaikovsky" and "Tschaikowski" that look different but refer to the same person.*

**Embed:**
```markdown
![Architecture diagram of semantic embedding-based entity resolution for music attribution, showing entity names encoded by the all-MiniLM-L6-v2 sentence-transformer model into 768-dimensional vectors, stored in PostgreSQL pgvector HALFVEC columns, with cosine similarity search catching translations and alias variations that string matching misses — weighted at 0.7 in the open-source attribution scaffold's confidence scoring pipeline.](docs/figures/repo-figures/assets/fig-backend-08-pgvector-embedding-match.jpg)
```

---

### fig-backend-09: Splink Probabilistic Linkage

**Alt text:** Pipeline diagram of Splink probabilistic record linkage for music attribution entity resolution, showing four stages — configure comparison columns with blocking rules, estimate Fellegi-Sunter match/non-match probabilities via EM algorithm, predict pairwise match probabilities using DuckDB backend, and cluster records above 0.85 threshold using union-find — weighted at 0.8 in the open-source attribution scaffold's transparent confidence scoring.

**Caption:** *Figure 9. Splink applies Fellegi-Sunter probabilistic theory to music entity records, estimating match and non-match probabilities via expectation-maximization before clustering matched pairs with union-find — providing a statistically principled signal for entity resolution alongside identifier and string similarity methods.*

**Embed:**
```markdown
![Pipeline diagram of Splink probabilistic record linkage for music attribution entity resolution, showing four stages — configure comparison columns with blocking rules, estimate Fellegi-Sunter match/non-match probabilities via EM algorithm, predict pairwise match probabilities using DuckDB backend, and cluster records above 0.85 threshold using union-find — weighted at 0.8 in the open-source attribution scaffold's transparent confidence scoring.](docs/figures/repo-figures/assets/fig-backend-09-splink-probabilistic-linkage.jpg)
```

---

### fig-backend-10: ResolvedEntity Schema

**Alt text:** Data schema diagram of the ResolvedEntity Pydantic model in the music attribution scaffold, showing core identity fields with assurance levels A0-A3, source provenance references tracking which MusicBrainz, Discogs, or AcoustID records contributed, per-method resolution confidence breakdown (string, embedding, graph, LLM), conflict detection with severity levels, and review flags for uncertain music credits requiring human verification.

**Caption:** *Figure 10. The ResolvedEntity (BO-2) carries full provenance from entity resolution — every contributing source record, the resolution method used, per-method confidence scores, and detected conflicts — enabling transparent confidence scoring in the downstream attribution engine.*

**Embed:**
```markdown
![Data schema diagram of the ResolvedEntity Pydantic model in the music attribution scaffold, showing core identity fields with assurance levels A0-A3, source provenance references tracking which MusicBrainz, Discogs, or AcoustID records contributed, per-method resolution confidence breakdown (string, embedding, graph, LLM), conflict detection with severity levels, and review flags for uncertain music credits requiring human verification.](docs/figures/repo-figures/assets/fig-backend-10-resolved-entity-schema.jpg)
```

---

### fig-backend-11: Attribution Engine Flow

**Alt text:** Pipeline diagram of the three-stage music attribution engine showing weighted credit aggregation with source reliability weights (MusicBrainz 0.95, Discogs 0.85, AcoustID 0.80, file metadata 0.70, Artist Input 0.60), conformal prediction calibration using Adaptive Prediction Sets at 90% coverage, and active learning review prioritization — producing the final AttributionRecord with transparent confidence scoring for open-source music credits.

**Caption:** *Figure 11. The Attribution Engine is the core scoring pipeline: it transforms resolved entities into calibrated, confidence-scored attribution records through weighted source aggregation, conformal prediction calibration, and review prioritization — ensuring that confidence scores are statistically meaningful.*

**Embed:**
```markdown
![Pipeline diagram of the three-stage music attribution engine showing weighted credit aggregation with source reliability weights (MusicBrainz 0.95, Discogs 0.85, AcoustID 0.80, file metadata 0.70, Artist Input 0.60), conformal prediction calibration using Adaptive Prediction Sets at 90% coverage, and active learning review prioritization — producing the final AttributionRecord with transparent confidence scoring for open-source music credits.](docs/figures/repo-figures/assets/fig-backend-11-attribution-engine-flow.jpg)
```

---

### fig-backend-12: Weighted Source Aggregation

**Alt text:** Mathematical diagram of the weighted source aggregation formula for music attribution confidence scoring, with a bar chart of five source reliability weights (MusicBrainz 0.95, Discogs 0.85, AcoustID 0.80, file metadata 0.70, Artist Input 0.60) and a worked example computing 0.845 per-credit confidence from three music metadata sources — demonstrating how the open-source attribution scaffold produces transparent, auditable confidence scores.

**Caption:** *Figure 12. Attribution confidence is computed as a weighted average of source reliabilities: high-authority sources like MusicBrainz contribute more to the final score than lower-authority sources, with the overall attribution confidence being the arithmetic mean of all per-credit confidences.*

**Embed:**
```markdown
![Mathematical diagram of the weighted source aggregation formula for music attribution confidence scoring, with a bar chart of five source reliability weights (MusicBrainz 0.95, Discogs 0.85, AcoustID 0.80, file metadata 0.70, Artist Input 0.60) and a worked example computing 0.845 per-credit confidence from three music metadata sources — demonstrating how the open-source attribution scaffold produces transparent, auditable confidence scores.](docs/figures/repo-figures/assets/fig-backend-12-weighted-source-aggregation.jpg)
```

---

### fig-backend-13: Conformal Calibration Pipeline

**Alt text:** Pipeline diagram of conformal prediction calibration for music attribution confidence scoring, showing the Adaptive Prediction Sets (APS) method accumulating role predictions sorted by confidence until 90% coverage is reached, with a reliability diagram plotting predicted versus actual accuracy and the Expected Calibration Error (ECE) formula — ensuring that confidence scores in the open-source attribution scaffold are statistically valid and not overconfident.

**Caption:** *Figure 13. Conformal prediction ensures that "90% confident" actually means 90% coverage: the APS method wraps attribution confidence in statistically valid prediction sets while the ECE metric across 10 bins monitors whether the system's confidence estimates are well-calibrated.*

**Embed:**
```markdown
![Pipeline diagram of conformal prediction calibration for music attribution confidence scoring, showing the Adaptive Prediction Sets (APS) method accumulating role predictions sorted by confidence until 90% coverage is reached, with a reliability diagram plotting predicted versus actual accuracy and the Expected Calibration Error (ECE) formula — ensuring that confidence scores in the open-source attribution scaffold are statistically valid and not overconfident.](docs/figures/repo-figures/assets/fig-backend-13-conformal-calibration-pipeline.jpg)
```

---

### fig-backend-14: AttributionRecord Schema

**Alt text:** Data schema diagram of the AttributionRecord Pydantic model — the final output of the music attribution scaffold — showing per-credit confidence scoring with 14 role types, conformal prediction sets with calibration metadata, a full provenance chain tracking six event types (fetch, resolve, score, review, update, feedback), assurance levels A0-A3, and review prioritization for transparent, auditable music credits.

**Caption:** *Figure 14. The AttributionRecord (BO-3) is the culminating boundary object of the pipeline, carrying per-credit confidence, conformal prediction sets, a complete provenance audit trail, and uncertainty metadata — serving both the REST API and the conversational agent interface.*

**Embed:**
```markdown
![Data schema diagram of the AttributionRecord Pydantic model — the final output of the music attribution scaffold — showing per-credit confidence scoring with 14 role types, conformal prediction sets with calibration metadata, a full provenance chain tracking six event types (fetch, resolve, score, review, update, feedback), assurance levels A0-A3, and review prioritization for transparent, auditable music credits.](docs/figures/repo-figures/assets/fig-backend-14-attribution-record-schema.jpg)
```

---

### fig-backend-15: Review Priority Queue

**Alt text:** Architecture diagram of the active learning review priority queue for music attribution, showing a five-factor weighted formula — boundary proximity at 30% peaking at 0.5 confidence, source disagreement at 25%, conformal set ambiguity at 15%, never-reviewed penalty at 15%, and staleness at 15% — designed to surface the most informative music credits for human expert review in the open-source attribution scaffold.

**Caption:** *Figure 15. The ReviewPriorityQueue uses an active learning strategy to rank attribution records for human review: boundary proximity (records near the 0.5 decision threshold) receives the highest weight, ensuring that expert attention is directed where it provides the greatest reduction in attribution uncertainty.*

**Embed:**
```markdown
![Architecture diagram of the active learning review priority queue for music attribution, showing a five-factor weighted formula — boundary proximity at 30% peaking at 0.5 confidence, source disagreement at 25%, conformal set ambiguity at 15%, never-reviewed penalty at 15%, and staleness at 15% — designed to surface the most informative music credits for human expert review in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-backend-15-review-priority-queue.jpg)
```

---

### fig-backend-16: FastAPI Route Map

**Alt text:** Architecture diagram of the FastAPI route map for the music attribution scaffold API, showing five router groups — health check, Prometheus metrics, attribution CRUD with hybrid search, MCP-compatible permission checks, and CopilotKit AG-UI streaming endpoint — with CORS middleware and async PostgreSQL session management, providing the REST interface for transparent music credit confidence scoring.

**Caption:** *Figure 16. The Music Attribution API exposes five router groups under a single FastAPI application, including attribution CRUD with hybrid search (text + vector + graph), permission checks for AI training rights, and a CopilotKit AG-UI endpoint for conversational agent access.*

**Embed:**
```markdown
![Architecture diagram of the FastAPI route map for the music attribution scaffold API, showing five router groups — health check, Prometheus metrics, attribution CRUD with hybrid search, MCP-compatible permission checks, and CopilotKit AG-UI streaming endpoint — with CORS middleware and async PostgreSQL session management, providing the REST interface for transparent music credit confidence scoring.](docs/figures/repo-figures/assets/fig-backend-16-fastapi-route-map.jpg)
```

---

### fig-backend-17: Database ERD

**Alt text:** Entity-relationship diagram of the music attribution scaffold's PostgreSQL database showing 8 tables — normalized_records, resolved_entities (central hub), attribution_records with JSONB credits and conformal sets, feedback_cards, permission_bundles, graph edges, entity_embeddings with pgvector HALFVEC(768) for semantic search, and audit_log — mapping the five boundary objects to persistent storage for transparent music metadata and confidence scoring.

**Caption:** *Figure 17. Eight PostgreSQL tables mirror the five boundary objects (BO-1 through BO-5) plus three infrastructure tables for graph relationships, vector embeddings, and permission auditing, using JSONB for flexible nested data and pgvector HALFVEC(768) for semantic search.*

**Embed:**
```markdown
![Entity-relationship diagram of the music attribution scaffold's PostgreSQL database showing 8 tables — normalized_records, resolved_entities (central hub), attribution_records with JSONB credits and conformal sets, feedback_cards, permission_bundles, graph edges, entity_embeddings with pgvector HALFVEC(768) for semantic search, and audit_log — mapping the five boundary objects to persistent storage for transparent music metadata and confidence scoring.](docs/figures/repo-figures/assets/fig-backend-17-database-erd.jpg)
```

---

### fig-backend-18: Hybrid Search Architecture

**Alt text:** Architecture diagram of the hybrid search system for the music attribution scaffold, combining three modalities — full-text LIKE search on music metadata fields, vector similarity via sentence-transformers and pgvector embeddings, and graph context from 1-hop entity relationships — fused with Reciprocal Rank Fusion (RRF, k=60) into a single ranked result list for comprehensive open-source music credit discovery.

**Caption:** *Figure 18. Hybrid search combines three complementary signals — text matching for exact keywords, vector similarity for semantic meaning, and graph neighbors for contextual relationships — ensuring that attribution records are discoverable through any search modality.*

**Embed:**
```markdown
![Architecture diagram of the hybrid search system for the music attribution scaffold, combining three modalities — full-text LIKE search on music metadata fields, vector similarity via sentence-transformers and pgvector embeddings, and graph context from 1-hop entity relationships — fused with Reciprocal Rank Fusion (RRF, k=60) into a single ranked result list for comprehensive open-source music credit discovery.](docs/figures/repo-figures/assets/fig-backend-18-hybrid-search-architecture.jpg)
```

---

### fig-backend-19: Alembic Migration Chain

**Alt text:** Flow diagram of the Alembic database migration chain for the music attribution scaffold, showing four sequential reversible migrations — initial schema with 3 boundary object tables, permissions and pgvector HALFVEC(768) embeddings adding 5 tables, uncertainty metadata column for attribution records, and display fields for work title and artist name — documenting PostgreSQL schema evolution for the open-source music metadata system.

**Caption:** *Figure 19. Four Alembic migrations build the full schema incrementally: initial boundary object tables, then permissions and vector infrastructure, then uncertainty metadata, then display fields — each migration is reversible and the chain forms a linear sequence tracking the schema evolution of the attribution database.*

**Embed:**
```markdown
![Flow diagram of the Alembic database migration chain for the music attribution scaffold, showing four sequential reversible migrations — initial schema with 3 boundary object tables, permissions and pgvector HALFVEC(768) embeddings adding 5 tables, uncertainty metadata column for attribution records, and display fields for work title and artist name — documenting PostgreSQL schema evolution for the open-source music metadata system.](docs/figures/repo-figures/assets/fig-backend-19-alembic-migration-chain.jpg)
```

---

### fig-backend-20: Dependency Injection

**Alt text:** Architecture diagram of the FastAPI dependency injection chain in the music attribution scaffold, showing the lifespan creating an async PostgreSQL engine and session factory stored on app.state, with request-scoped database sessions flowing through route handlers, repository layer (attribution and permission repositories), and service layer (hybrid search, credit aggregation) to produce JSON responses for music credit confidence scoring queries.

**Caption:** *Figure 20. Dependencies flow through app.state in a clean chain: the lifespan creates the async engine and session factory at startup, route handlers acquire request-scoped sessions, and repositories and services are composed without global state — enabling straightforward testing and extension of the attribution API.*

**Embed:**
```markdown
![Architecture diagram of the FastAPI dependency injection chain in the music attribution scaffold, showing the lifespan creating an async PostgreSQL engine and session factory stored on app.state, with request-scoped database sessions flowing through route handlers, repository layer (attribution and permission repositories), and service layer (hybrid search, credit aggregation) to produce JSON responses for music credit confidence scoring queries.](docs/figures/repo-figures/assets/fig-backend-20-dependency-injection.jpg)
```

---

## Architecture Decisions (fig-choice-*)

### fig-choice-01: Why PydanticAI over LangChain?

**Alt text:** Comparison chart: PydanticAI versus LangChain for music attribution AI framework, showing typed Pydantic-native agent code versus middleware-heavy chain composition, with PydanticAI selected for transparent confidence scoring with fewer dependencies and readable stack traces in an open-source attribution scaffold.

**Caption:** *PydanticAI provides typed, zero-middleware LLM integration for the music attribution scaffold, chosen over LangChain for its simplicity, single-dependency footprint, and native FallbackModel failover (PRD node: `ai_framework_strategy = direct_api_pydantic`).*

**Embed:**
```markdown
![Comparison chart: PydanticAI versus LangChain for music attribution AI framework, showing typed Pydantic-native agent code versus middleware-heavy chain composition, with PydanticAI selected for transparent confidence scoring with fewer dependencies and readable stack traces in an open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-01-pydanticai-over-langchain.jpg)
```

---

### fig-choice-02: Why CopilotKit + AG-UI?

**Alt text:** Architecture decision: CopilotKit with AG-UI protocol selected for music attribution agentic UI, comparing open-source streaming with 31 event types and MCP integration against Vercel AI SDK and custom WebSocket, enabling transparent confidence scoring through bidirectional shared state.

**Caption:** *CopilotKit + AG-UI was selected as the agentic UI framework for the music attribution scaffold, providing 31-event SSE streaming, native MCP permission queries, and bidirectional state sharing via `useCopilotReadable`/`useCopilotAction` hooks (PRD node: `agentic_ui_framework = copilotkit_agui`, P=0.50).*

**Embed:**
```markdown
![Architecture decision: CopilotKit with AG-UI protocol selected for music attribution agentic UI, comparing open-source streaming with 31 event types and MCP integration against Vercel AI SDK and custom WebSocket, enabling transparent confidence scoring through bidirectional shared state.](docs/figures/repo-figures/assets/fig-choice-02-copilotkit-agui.jpg)
```

---

### fig-choice-03: Why PostgreSQL + pgvector?

**Alt text:** Architecture decision: PostgreSQL with pgvector and Apache AGE unifying relational, graph, and vector capabilities in a single database for music attribution, compared against Pinecone, Chroma, and Supabase, with a hosting spectrum from Neon scale-to-zero for MVP through Ubicloud managed PostgreSQL on Hetzner for budget production to self-managed CloudNativePG for expert operators in the open-source attribution scaffold.

**Caption:** *PostgreSQL Unified provides relational SQL, graph queries (Apache AGE), and vector search (pgvector) in a single process. Hosting ranges from Neon (scale-to-zero for MVP, EU data residency) through Ubicloud managed PG on Hetzner (budget production) to self-managed CloudNativePG (expert bare-metal).*

**Embed:**
```markdown
![Architecture decision: PostgreSQL with pgvector and Apache AGE unifying relational, graph, and vector capabilities in a single database for music attribution, compared against Pinecone, Chroma, and Supabase, with a hosting spectrum from Neon scale-to-zero for MVP through Ubicloud managed PostgreSQL on Hetzner for budget production to self-managed CloudNativePG for expert operators in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-03-postgresql-pgvector.jpg)
```

---

### fig-choice-04: Why Conformal Prediction?

**Alt text:** Trade-off analysis: conformal prediction selected for transparent confidence scoring in music attribution, providing distribution-free coverage guarantees over Bayesian posteriors and bootstrap intervals, essential for heterogeneous music metadata where data quality varies across MusicBrainz, Discogs, and file sources.

**Caption:** *Conformal prediction delivers mathematically guaranteed coverage rates without distributional assumptions, making it the preferred confidence scoring method for the music attribution scaffold where metadata quality varies wildly across curated, community, and file-embedded sources (see SConU calibration in SSRN 6109087).*

**Embed:**
```markdown
![Trade-off analysis: conformal prediction selected for transparent confidence scoring in music attribution, providing distribution-free coverage guarantees over Bayesian posteriors and bootstrap intervals, essential for heterogeneous music metadata where data quality varies across MusicBrainz, Discogs, and file sources.](docs/figures/repo-figures/assets/fig-choice-04-conformal-prediction.jpg)
```

---

### fig-choice-05: Why Splink for Entity Linkage?

**Alt text:** Decision diagram: Splink selected for music attribution entity resolution using Fellegi-Sunter probabilistic linkage with EM-trained match weights, compared against dedupe.io and custom rules for matching artist identities across MusicBrainz, Discogs, and file metadata sources in the open-source attribution scaffold.

**Caption:** *Splink implements the Fellegi-Sunter model with EM-trained match weights for probabilistic entity resolution in the music attribution scaffold, enabling scalable and interpretable linkage of artist identities across heterogeneous music metadata sources.*

**Embed:**
```markdown
![Decision diagram: Splink selected for music attribution entity resolution using Fellegi-Sunter probabilistic linkage with EM-trained match weights, compared against dedupe.io and custom rules for matching artist identities across MusicBrainz, Discogs, and file metadata sources in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-05-splink-linkage.jpg)
```

---

### fig-choice-06: Why FastAPI over Django?

**Alt text:** Comparison chart: FastAPI versus Django for music attribution API serving, highlighting async-native Pydantic integration and MCP endpoint streaming for transparent confidence scoring, with FastAPI selected as lightweight modular monolith over Django's batteries-included ORM approach in the open-source scaffold.

**Caption:** *FastAPI's async-native, Pydantic-native design aligns with the music attribution scaffold's need for REST and MCP serving without ORM overhead, providing auto-generated OpenAPI docs and SSE streaming for the AG-UI agentic endpoint.*

**Embed:**
```markdown
![Comparison chart: FastAPI versus Django for music attribution API serving, highlighting async-native Pydantic integration and MCP endpoint streaming for transparent confidence scoring, with FastAPI selected as lightweight modular monolith over Django's batteries-included ORM approach in the open-source scaffold.](docs/figures/repo-figures/assets/fig-choice-06-fastapi-over-django.jpg)
```

---

### fig-choice-07: Why Next.js 15 App Router?

**Alt text:** Architecture decision: Next.js 15 App Router selected for music attribution frontend, driven by CopilotKit React dependency for agentic UI, compared against SvelteKit and HTMX alternatives, with server components and streaming SSR enabling transparent confidence display in the open-source attribution scaffold.

**Caption:** *Next.js 15 with App Router was selected as the frontend framework for the music attribution scaffold primarily because CopilotKit requires React, providing server components, nested layouts, and streaming SSR for real-time confidence scoring display.*

**Embed:**
```markdown
![Architecture decision: Next.js 15 App Router selected for music attribution frontend, driven by CopilotKit React dependency for agentic UI, compared against SvelteKit and HTMX alternatives, with server components and streaming SSR enabling transparent confidence display in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-07-nextjs-app-router.jpg)
```

---

### fig-choice-08: Why Jotai over Redux?

**Alt text:** Comparison chart: Jotai atomic state management selected for music attribution frontend over Redux and Zustand, showing minimal two-line atom definitions for theme, role mode, and music credits state with zero boilerplate in the open-source attribution scaffold.

**Caption:** *Jotai's atomic state model provides minimal-boilerplate state management for the music attribution scaffold's three client-side state domains (theme, role mode, works), chosen over Redux's action/reducer ceremony and Zustand's store-based approach for React concurrent compatibility.*

**Embed:**
```markdown
![Comparison chart: Jotai atomic state management selected for music attribution frontend over Redux and Zustand, showing minimal two-line atom definitions for theme, role mode, and music credits state with zero boilerplate in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-08-jotai-over-redux.jpg)
```

---

### fig-choice-09: Why Tailwind v4 + CSS Custom Properties?

**Alt text:** Design token architecture: CSS custom properties flowing through Tailwind v4 utilities for music attribution UI theming, showing zero-hardcoded-hex color tokens for confidence scoring tiers and music credits display, with critical text-var pitfall warning for the open-source attribution scaffold.

**Caption:** *CSS custom properties define all design tokens for the music attribution scaffold's confidence tier colors, assurance levels, and data source indicators, consumed via Tailwind v4 utility classes with a documented pitfall: `text-[var(--anything)]` is treated as color, not font-size.*

**Embed:**
```markdown
![Design token architecture: CSS custom properties flowing through Tailwind v4 utilities for music attribution UI theming, showing zero-hardcoded-hex color tokens for confidence scoring tiers and music credits display, with critical text-var pitfall warning for the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-09-tailwind-v4-css-properties.jpg)
```

---

### fig-choice-10: Why uv over pip/conda?

**Alt text:** Comparison chart: uv package manager selected exclusively for the music attribution scaffold over banned pip and conda, showing 100x faster Rust-based resolution, deterministic lockfiles, and native dependency groups for open-source music metadata project reproducibility.

**Caption:** *The music attribution scaffold enforces uv as the sole package manager, completely banning pip and conda for deterministic, fast dependency resolution via Rust-based tooling and a single `pyproject.toml` source of truth.*

**Embed:**
```markdown
![Comparison chart: uv package manager selected exclusively for the music attribution scaffold over banned pip and conda, showing 100x faster Rust-based resolution, deterministic lockfiles, and native dependency groups for open-source music metadata project reproducibility.](docs/figures/repo-figures/assets/fig-choice-10-uv-over-pip.jpg)
```

---

### fig-choice-11: Why MCP for Permissions?

**Alt text:** Decision diagram: MCP as consent infrastructure for music attribution, showing machine-readable AI training permission queries with transparent confidence responses, compared against custom REST API and blockchain approaches for the open-source attribution scaffold enabling attribution-by-design.

**Caption:** *Model Context Protocol (MCP) enables standardized, machine-readable permission queries for AI training consent in the music attribution scaffold, allowing AI platforms to programmatically check training rights with confidence-scored responses before use, not after (Teikari 2026, SSRN 6109087).*

**Embed:**
```markdown
![Decision diagram: MCP as consent infrastructure for music attribution, showing machine-readable AI training permission queries with transparent confidence responses, compared against custom REST API and blockchain approaches for the open-source attribution scaffold enabling attribution-by-design.](docs/figures/repo-figures/assets/fig-choice-11-mcp-permissions.jpg)
```

---

### fig-choice-12: Why Alembic for Migrations?

**Alt text:** Comparison chart: Alembic selected for music attribution database migrations with SQLAlchemy-native autogeneration from PostgreSQL schema models, compared against Django migrations and manual SQL for version-controlled music metadata schema evolution in the open-source attribution scaffold.

**Caption:** *Alembic provides SQLAlchemy-native migration management for the music attribution scaffold, ensuring the same model definitions that drive data access also drive schema evolution with autogenerated, version-controlled migration files.*

**Embed:**
```markdown
![Comparison chart: Alembic selected for music attribution database migrations with SQLAlchemy-native autogeneration from PostgreSQL schema models, compared against Django migrations and manual SQL for version-controlled music metadata schema evolution in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-12-alembic-migrations.jpg)
```

---

### fig-choice-13: Why tinytag over mutagen?

**Alt text:** Trade-off analysis: tinytag BSD-3 license selected over mutagen GPL-2.0 for music metadata extraction in the attribution scaffold, preserving permissive open-source licensing for the music attribution project while providing read-only ID3, FLAC, and MP4 tag support for music credits and ISRC identification.

**Caption:** *The license-driven decision to use tinytag (BSD-3) over mutagen (GPL-2.0) preserves the music attribution scaffold's permissive licensing, trading write capabilities and TIPL credits support for zero-dependency metadata reading of artist, title, album, duration, and ISRC fields (Issue #29).*

**Embed:**
```markdown
![Trade-off analysis: tinytag BSD-3 license selected over mutagen GPL-2.0 for music metadata extraction in the attribution scaffold, preserving permissive open-source licensing for the music attribution project while providing read-only ID3, FLAC, and MP4 tag support for music credits and ISRC identification.](docs/figures/repo-figures/assets/fig-choice-13-tinytag-over-mutagen.jpg)
```

---

### fig-choice-14: Observability -- PostHog + Sentry vs Alternatives

**Alt text:** Architecture decision: PostHog plus Sentry selected for music attribution observability, providing typed product analytics and error tracking for confidence scoring workflows, compared against Datadog, Grafana, and defunct Highlight.io in the open-source attribution scaffold.

**Caption:** *PostHog (product analytics with typed events and feature flags) paired with Sentry (error tracking and performance monitoring) provides cost-effective observability for the music attribution scaffold after Highlight.io's shutdown, with separate tools for separate concerns.*

**Embed:**
```markdown
![Architecture decision: PostHog plus Sentry selected for music attribution observability, providing typed product analytics and error tracking for confidence scoring workflows, compared against Datadog, Grafana, and defunct Highlight.io in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-choice-14-observability-posthog-sentry.jpg)
```

---

### fig-choice-15: Deployment Options -- Cost vs Complexity Landscape

**Alt text:** Corrected cost versus complexity scatter plot showing five deployment paths for the music attribution scaffold: Render (lowest complexity, moderate cost), Hetzner with Ubicloud managed Kubernetes (moderate complexity, low cost), Big Three hyperscalers AWS GCP Azure (moderate complexity, highest cost), and Hetzner bare-metal with self-managed Kubernetes (highest complexity, lowest cost), with a recommended migration arrow from Render to Hetzner plus Ubicloud.

**Caption:** *The music attribution scaffold deploys on Render (~€20--35/mo, git push), Hetzner+Ubicloud (~€20--60/mo, managed K8s), Big Three (~€200+/mo, full managed), or Hetzner bare-metal (~€7--30/mo, expert ops) — same Docker image, different operational realities. Recommended path: start on Render, migrate to Hetzner when savings justify the ops burden.*

**Embed:**
```markdown
![Corrected cost versus complexity scatter plot showing five deployment paths for the music attribution scaffold: Render (lowest complexity, moderate cost), Hetzner with Ubicloud managed Kubernetes (moderate complexity, low cost), Big Three hyperscalers AWS GCP Azure (moderate complexity, highest cost), and Hetzner bare-metal with self-managed Kubernetes (highest complexity, lowest cost), with a recommended migration arrow from Render to Hetzner plus Ubicloud.](docs/figures/repo-figures/assets/fig-choice-15-deployment-options.jpg)
```

---

### fig-choice-16: Object Storage Strategy -- Zero-Egress R2

**Alt text:** Architecture decision: Cloudflare R2 selected for music attribution scaffold object storage with zero egress fees, compared against Hetzner Object Storage, Backblaze B2, and AWS S3 in an egress cost table showing $0 versus $9,000 per month at 100TB scale -- a critical FinOps decision for audio-heavy workloads where preview serving dominates bandwidth costs in the open-source attribution platform.

**Caption:** *Cloudflare R2 eliminates egress fees entirely for the music attribution scaffold's audio workloads -- at 100TB/month, R2 saves $9,000 versus AWS S3. The S3-compatible API means switching requires only endpoint configuration, not code changes (PRD v2.1.0 node: object_storage).*

**Embed:**
```markdown
![Architecture decision: Cloudflare R2 selected for music attribution scaffold object storage with zero egress fees, compared against Hetzner Object Storage, Backblaze B2, and AWS S3 in an egress cost table showing $0 versus $9,000 per month at 100TB scale -- a critical FinOps decision for audio-heavy workloads where preview serving dominates bandwidth costs in the open-source attribution platform.](docs/figures/repo-figures/assets/fig-choice-16-object-storage-strategy.jpg)
```

---

### fig-choice-17: Why Pulumi over Terraform?

**Alt text:** Architecture decision: Pulumi recommended over Terraform for music attribution scaffold infrastructure-as-code, showing Python-native language alignment with the scaffold backend, official MCP server enabling Claude to manage deployments, and Apache 2.0 license versus Terraform's BSL after IBM's HashiCorp acquisition -- compared against OpenTofu fork and no-IaC PaaS options in the open-source attribution platform.

**Caption:** *Pulumi is recommended over Terraform for the music attribution scaffold: Python-native IaC (same language as the backend), an official MCP server for Claude-managed deployments, and Apache 2.0 licensing -- while Terraform's BSL after IBM's $6.4B HashiCorp acquisition creates vendor risk (PRD v2.1.0 node: iac_tooling, volatility: shifting).*

**Embed:**
```markdown
![Architecture decision: Pulumi recommended over Terraform for music attribution scaffold infrastructure-as-code, showing Python-native language alignment with the scaffold backend, official MCP server enabling Claude to manage deployments, and Apache 2.0 license versus Terraform's BSL after IBM's HashiCorp acquisition -- compared against OpenTofu fork and no-IaC PaaS options in the open-source attribution platform.](docs/figures/repo-figures/assets/fig-choice-17-pulumi-over-terraform.jpg)
```

---

### fig-choice-18: Cloud Sovereignty — EU-Owned vs US-Owned Providers

**Alt text:** Architecture decision: cloud sovereignty assessment for music attribution scaffold comparing four EU-sovereign providers — Hetzner (Germany, budget leader), OVHcloud (France, SecNumCloud enterprise), UpCloud (Finland, premium managed services with CISPE certification), and Scaleway (France, GPU and AI focus) — against US-headquartered providers exposed to the CLOUD Act and FISA Section 702, with four sovereignty tiers mapping MCP consent infrastructure and artist identity data to EU-only hosting requirements in the open-source attribution platform.

**Caption:** *Cloud sovereignty for music attribution: EU-sovereign providers (Hetzner, OVHcloud, UpCloud, Scaleway) eliminate CLOUD Act exposure while saving 60-93% versus hyperscalers. The scaffold's Docker architecture makes sovereignty a configuration change — same container image deploys on any provider (PRD geopolitical risk assessment, volatility: shifting).*

**Embed:**
```markdown
![Architecture decision: cloud sovereignty assessment for music attribution scaffold comparing four EU-sovereign providers — Hetzner (Germany, budget leader), OVHcloud (France, SecNumCloud enterprise), UpCloud (Finland, premium managed services with CISPE certification), and Scaleway (France, GPU and AI focus) — against US-headquartered providers exposed to the CLOUD Act and FISA Section 702, with four sovereignty tiers mapping MCP consent infrastructure and artist identity data to EU-only hosting requirements in the open-source attribution platform.](docs/figures/repo-figures/assets/fig-choice-18-cloud-sovereignty.jpg)
```

---

## Frontend (fig-frontend-*)

### fig-frontend-01: Frontend Architecture Overview

**Alt text:** Architecture diagram showing the four-layer Next.js 15 frontend for the open-source music attribution scaffold: App Router pages compose 19 domain-organized component modules, lib utilities including Jotai state and CopilotKit agent context, and three custom hooks bridging transparent confidence scoring UI to agentic workflows.

**Caption:** *Figure: High-level frontend architecture of the Music Attribution Scaffold, illustrating how four composition layers -- pages, components, lib, and hooks -- organize a Next.js 15 App Router application for transparent music credits and confidence-scored attribution workflows.*

**Embed:**
```markdown
![Architecture diagram showing the four-layer Next.js 15 frontend for the open-source music attribution scaffold: App Router pages compose 19 domain-organized component modules, lib utilities including Jotai state and CopilotKit agent context, and three custom hooks bridging transparent confidence scoring UI to agentic workflows.](docs/figures/repo-figures/assets/fig-frontend-01-architecture-overview.jpg)
```

---

### fig-frontend-02: Design System Token Architecture

**Alt text:** Design system diagram showing CSS custom property token architecture for the music attribution scaffold: 60-plus color tokens in globals.css flow through Tailwind CSS v4 utilities and editorial classes to components with zero hardcoded hex values, enforced by automated lint tests for consistent confidence scoring and music metadata visualization.

**Caption:** *Figure: Design token architecture enforcing zero hardcoded hex values across the music attribution UI, tracing the flow from CSS custom properties through Tailwind v4 utilities to component consumption with lint-test enforcement.*

**Embed:**
```markdown
![Design system diagram showing CSS custom property token architecture for the music attribution scaffold: 60-plus color tokens in globals.css flow through Tailwind CSS v4 utilities and editorial classes to components with zero hardcoded hex values, enforced by automated lint tests for consistent confidence scoring and music metadata visualization.](docs/figures/repo-figures/assets/fig-frontend-02-design-token-architecture.jpg)
```

---

### fig-frontend-03: Page Router Map

**Alt text:** Component diagram mapping the Next.js 15 App Router page tree for the open-source music attribution scaffold: five page files branch from a shared RootLayout with provider chain, rendering works catalog, work detail with transparent confidence scoring, AI-assisted review queue, and MCP permission consent management routes.

**Caption:** *Figure: App Router page hierarchy showing how five route segments compose domain-specific components for music metadata browsing, attribution review, and permission management under a shared layout with CopilotKit agent context.*

**Embed:**
```markdown
![Component diagram mapping the Next.js 15 App Router page tree for the open-source music attribution scaffold: five page files branch from a shared RootLayout with provider chain, rendering works catalog, work detail with transparent confidence scoring, AI-assisted review queue, and MCP permission consent management routes.](docs/figures/repo-figures/assets/fig-frontend-03-page-router-map.jpg)
```

---

### fig-frontend-04: Confidence Gauge Component

**Alt text:** UI component diagram of the ConfidenceGauge used in the music attribution scaffold: a 270-degree SVG arc with three transparent confidence scoring tiers -- green for high, amber for medium, red for low -- shown in three size variants with ARIA meter role accessibility attributes and motion-safe mount animation for music credits visualization.

**Caption:** *Figure: Anatomy of the ConfidenceGauge component, the primary visual indicator for music attribution confidence, illustrating tier thresholds (0.85/0.50), size variants, SVG arc construction, and WCAG 2.1 AA accessibility via role="meter".*

**Embed:**
```markdown
![UI component diagram of the ConfidenceGauge used in the music attribution scaffold: a 270-degree SVG arc with three transparent confidence scoring tiers -- green for high, amber for medium, red for low -- shown in three size variants with ARIA meter role accessibility attributes and motion-safe mount animation for music credits visualization.](docs/figures/repo-figures/assets/fig-frontend-04-confidence-gauge.jpg)
```

---

### fig-frontend-05: Assurance Badge System

**Alt text:** Design system diagram of the A0-A3 assurance badge system for music attribution provenance: four color-coded levels from gray (no data) through amber (single source) and blue (multi-source) to green (artist verified) provide transparent confidence scoring of music metadata trustworthiness across the open-source attribution scaffold UI.

**Caption:** *Figure: The four-level assurance badge system (A0-A3) visualizes provenance quality for music credits, mapping data source agreement to color-coded editorial-caps badges that appear throughout the works catalog, detail pages, and AI review queue.*

**Embed:**
```markdown
![Design system diagram of the A0-A3 assurance badge system for music attribution provenance: four color-coded levels from gray (no data) through amber (single source) and blue (multi-source) to green (artist verified) provide transparent confidence scoring of music metadata trustworthiness across the open-source attribution scaffold UI.](docs/figures/repo-figures/assets/fig-frontend-05-assurance-badge-system.jpg)
```

---

### fig-frontend-06: Works List Page Layout

**Alt text:** Interface mockup of the works catalog page in the music attribution scaffold: fixed left sidebar navigation alongside horizontal rows displaying confidence gauges, music credits titles, A0-A3 assurance badges, and source agreement percentages with Jotai-driven search and sort for transparent confidence scoring of music metadata.

**Caption:** *Figure: Layout of the /works catalog page showing editorial horizontal rows (not shadow-box cards) with per-work confidence gauges, assurance badges, and source agreement, driven by a Jotai atom chain for reactive filtering and sorting.*

**Embed:**
```markdown
![Interface mockup of the works catalog page in the music attribution scaffold: fixed left sidebar navigation alongside horizontal rows displaying confidence gauges, music credits titles, A0-A3 assurance badges, and source agreement percentages with Jotai-driven search and sort for transparent confidence scoring of music metadata.](docs/figures/repo-figures/assets/fig-frontend-06-works-list-layout.jpg)
```

---

### fig-frontend-07: Work Detail Page Layout

**Alt text:** Interface mockup of the work detail page in the music attribution scaffold: hero section with large confidence gauge and conformal prediction statistics, per-credit music metadata scores with source tags, Perplexity-style provenance panel with inline citations, and chronological timeline for transparent confidence scoring of music credits provenance.

**Caption:** *Figure: The /works/[workId] detail page surfaces per-field confidence, source provenance with inline citations, and conformal set statistics -- making the attribution scaffold's transparency philosophy visible at every level of musical credit detail.*

**Embed:**
```markdown
![Interface mockup of the work detail page in the music attribution scaffold: hero section with large confidence gauge and conformal prediction statistics, per-credit music metadata scores with source tags, Perplexity-style provenance panel with inline citations, and chronological timeline for transparent confidence scoring of music credits provenance.](docs/figures/repo-figures/assets/fig-frontend-07-work-detail-layout.jpg)
```

---

### fig-frontend-08: Review Queue Workflow

**Alt text:** UI wireframe of the AI-assisted review queue workflow in the music attribution scaffold: agent narration header, progress tracking bar, music credits items indexed with Roman numerals, expandable suggestion diffs showing before-and-after attribution changes, and batch approval for efficient transparent confidence scoring of music metadata.

**Caption:** *Figure: The review queue transforms one-by-one attribution review into a momentum-driven agentic UI workflow with AI-generated diffs, batch approval, and progress tracking -- the primary friction reducer for music credits verification.*

**Embed:**
```markdown
![UI wireframe of the AI-assisted review queue workflow in the music attribution scaffold: agent narration header, progress tracking bar, music credits items indexed with Roman numerals, expandable suggestion diffs showing before-and-after attribution changes, and batch approval for efficient transparent confidence scoring of music metadata.](docs/figures/repo-figures/assets/fig-frontend-08-review-queue-workflow.jpg)
```

---

### fig-frontend-09: Jotai State Architecture

**Alt text:** Component diagram of the Jotai state architecture for the music attribution scaffold: four store files manage theme, role mode, works catalog with filtered and sorted music metadata atoms, and a localStorage-persisted proficiency model that drives adaptive UI for transparent confidence scoring workflows.

**Caption:** *Figure: Jotai atom tree mapping primitive read-write atoms to derived read-only computations across four stores, showing how music attribution state flows from user interactions through reactive derivations to component renders.*

**Embed:**
```markdown
![Component diagram of the Jotai state architecture for the music attribution scaffold: four store files manage theme, role mode, works catalog with filtered and sorted music metadata atoms, and a localStorage-persisted proficiency model that drives adaptive UI for transparent confidence scoring workflows.](docs/figures/repo-figures/assets/fig-frontend-09-jotai-state-architecture.jpg)
```

---

### fig-frontend-10: Typography System

**Alt text:** Design system diagram of the three-font typography system for the music attribution scaffold: Instrument Serif for editorial display headings, Plus Jakarta Sans for body and UI text, and IBM Plex Mono for transparent confidence scoring data values, with four CSS utility classes enabling consistent music metadata presentation.

**Caption:** *Figure: Three font families serve distinct roles in the music attribution UI -- Instrument Serif creates editorial gravitas for headings, Plus Jakarta Sans handles body text and navigation, and IBM Plex Mono with tabular-nums makes confidence scores instantly scannable.*

**Embed:**
```markdown
![Design system diagram of the three-font typography system for the music attribution scaffold: Instrument Serif for editorial display headings, Plus Jakarta Sans for body and UI text, and IBM Plex Mono for transparent confidence scoring data values, with four CSS utility classes enabling consistent music metadata presentation.](docs/figures/repo-figures/assets/fig-frontend-10-typography-system.jpg)
```

---

### fig-frontend-11: Color System (Light/Dark)

**Alt text:** Design system diagram showing the complete light and dark mode color palettes for the open-source music attribution scaffold: warm cream and dark navy surface foundations, coral red brand accent, three-tier confidence scoring colors, A0-A3 assurance level colors, five music metadata source colors, and role accent tokens for artist and query modes.

**Caption:** *Figure: Side-by-side light and dark color systems with six token categories, anchored by warm cream (#f6f3e6) and dark navy (#1A1A2E) foundations that create an editorial, paper-like feel for music attribution workflows.*

**Embed:**
```markdown
![Design system diagram showing the complete light and dark mode color palettes for the open-source music attribution scaffold: warm cream and dark navy surface foundations, coral red brand accent, three-tier confidence scoring colors, A0-A3 assurance level colors, five music metadata source colors, and role accent tokens for artist and query modes.](docs/figures/repo-figures/assets/fig-frontend-11-color-system.jpg)
```

---

### fig-frontend-12: Responsive Layout Strategy

**Alt text:** UI wireframe showing the responsive layout strategy for the music attribution scaffold: desktop view with fixed 60px left sidebar and editorial content grid, mobile view with 48px top bar, and hamburger-triggered slide-over navigation panel, all built with Tailwind CSS v4 breakpoints for accessible music credits and transparent confidence scoring on any device.

**Caption:** *Figure: Responsive layout transformation at the md (768px) breakpoint, pivoting from a fixed left sidebar with rotated text navigation on desktop to a top bar with hamburger slide-over on mobile for consistent music attribution access.*

**Embed:**
```markdown
![UI wireframe showing the responsive layout strategy for the music attribution scaffold: desktop view with fixed 60px left sidebar and editorial content grid, mobile view with 48px top bar, and hamburger-triggered slide-over navigation panel, all built with Tailwind CSS v4 breakpoints for accessible music credits and transparent confidence scoring on any device.](docs/figures/repo-figures/assets/fig-frontend-12-responsive-layout.jpg)
```

---

### fig-frontend-13: Accessibility Architecture

**Alt text:** Architecture diagram of the two-layer WCAG 2.1 AA accessibility testing strategy for the music attribution scaffold: Vitest plus vitest-axe for fast component-level checks and Playwright plus axe-core for real-browser validation, with an ARIA pattern catalog covering confidence scoring meter roles, navigation landmarks, and motion-safe music metadata interfaces.

**Caption:** *Figure: Two-layer accessibility architecture ensuring WCAG 2.1 AA compliance for the music attribution UI, combining fast per-component axe checks (265 tests) with real-browser Playwright validation and documented ARIA patterns for all interactive elements.*

**Embed:**
```markdown
![Architecture diagram of the two-layer WCAG 2.1 AA accessibility testing strategy for the music attribution scaffold: Vitest plus vitest-axe for fast component-level checks and Playwright plus axe-core for real-browser validation, with an ARIA pattern catalog covering confidence scoring meter roles, navigation landmarks, and motion-safe music metadata interfaces.](docs/figures/repo-figures/assets/fig-frontend-13-accessibility-architecture.jpg)
```

---

### fig-frontend-14: PostHog Analytics Events

**Alt text:** Component diagram of the PostHog analytics event taxonomy for the music attribution scaffold: 12 type-safe events across review, agentic UI, interaction, and feedback categories track the complete music credits workflow from approval through agent chat to proficiency changes, with graceful no-op fallback for transparent confidence scoring analytics.

**Caption:** *Figure: Typed PostHog event taxonomy capturing the complete music attribution workflow, from review approvals and AG-UI agent interactions to proficiency-level transitions, with TypeScript generics ensuring compile-time safety for all 12 events.*

**Embed:**
```markdown
![Component diagram of the PostHog analytics event taxonomy for the music attribution scaffold: 12 type-safe events across review, agentic UI, interaction, and feedback categories track the complete music credits workflow from approval through agent chat to proficiency changes, with graceful no-op fallback for transparent confidence scoring analytics.](docs/figures/repo-figures/assets/fig-frontend-14-posthog-analytics.jpg)
```

---

### fig-frontend-15: Adaptive UI / Proficiency Model

**Alt text:** Component diagram of the adaptive UI proficiency model for the music attribution scaffold: user interactions with music credits review, feedback, and confidence reading feed a Jotai store with localStorage persistence, computeLevel produces novice-intermediate-expert tiers, and useFeatureFlags adapts UI density, batch operations, and tooltip visibility for experience-driven transparent confidence scoring.

**Caption:** *Figure: The proficiency model automatically adapts the music attribution UI as users gain experience -- novices see comfortable layouts with tooltips, experts unlock dense layouts with batch operations, and PostHog feature flags can remotely override any default.*

**Embed:**
```markdown
![Component diagram of the adaptive UI proficiency model for the music attribution scaffold: user interactions with music credits review, feedback, and confidence reading feed a Jotai store with localStorage persistence, computeLevel produces novice-intermediate-expert tiers, and useFeatureFlags adapts UI density, batch operations, and tooltip visibility for experience-driven transparent confidence scoring.](docs/figures/repo-figures/assets/fig-frontend-15-adaptive-ui-proficiency.jpg)
```

---

## How-To Guides (fig-howto-*)

### fig-howto-01: How to Add a New Data Source

**Alt text:** How-to guide: five-step workflow for adding a new music metadata data source to the open-source attribution scaffold ETL pipeline, covering extractor creation, BaseExtractor interface implementation, orchestrator registration, quality gate configuration, and test coverage -- each step maps to a specific module path ensuring transparent confidence scoring from ingestion onward.

**Caption:** *Five-step data source integration path for the Music Attribution Scaffold ETL pipeline. Each step corresponds to a specific module in the `src/music_attribution/etl/` package, enforcing the attribution-by-design principle that every new source must implement extraction, normalization, and validation before registration (Teikari, 2026).*

**Embed:**
```markdown
![How-to guide: five-step workflow for adding a new music metadata data source to the open-source attribution scaffold ETL pipeline, covering extractor creation, BaseExtractor interface implementation, orchestrator registration, quality gate configuration, and test coverage -- each step maps to a specific module path ensuring transparent confidence scoring from ingestion onward.](docs/figures/repo-figures/assets/fig-howto-01-add-new-data-source.jpg)
```

---

### fig-howto-02: How to Reproduce Paper Claims

**Alt text:** Tutorial diagram: three-column reproducibility map linking academic paper sections on music attribution to corresponding open-source code modules and test commands, covering confidence scoring, multi-source entity resolution, A0-A3 assurance levels, and MCP consent infrastructure -- every substantive claim is verifiable by running a single make test command.

**Caption:** *Paper-to-code reproducibility map for SSRN No. 6109087 (Teikari, 2026). Each row connects a manuscript section -- from transparent confidence scoring to the Oracle Problem -- to the specific module and test keyword that validates it, making the music attribution scaffold a fully auditable companion to the research.*

**Embed:**
```markdown
![Tutorial diagram: three-column reproducibility map linking academic paper sections on music attribution to corresponding open-source code modules and test commands, covering confidence scoring, multi-source entity resolution, A0-A3 assurance levels, and MCP consent infrastructure -- every substantive claim is verifiable by running a single make test command.](docs/figures/repo-figures/assets/fig-howto-02-reproduce-paper-claims.jpg)
```

---

### fig-howto-03: How to Query the API

**Alt text:** Step-by-step guide: API request lifecycle for querying music attribution data, from a curl command through FastAPI routing and the multi-source attribution engine to a JSON response containing per-field confidence scores, A0-A3 assurance levels, and source provenance -- demonstrating transparent confidence scoring for music credits in a single HTTP call.

**Caption:** *API request-response lifecycle for the Music Attribution Scaffold. A single GET request to `/api/v1/attribution/{isrc}` returns confidence-scored music metadata with source provenance, illustrating the attribution-by-design principle where every response carries its own quality evidence (Teikari, 2026).*

**Embed:**
```markdown
![Step-by-step guide: API request lifecycle for querying music attribution data, from a curl command through FastAPI routing and the multi-source attribution engine to a JSON response containing per-field confidence scores, A0-A3 assurance levels, and source provenance -- demonstrating transparent confidence scoring for music credits in a single HTTP call.](docs/figures/repo-figures/assets/fig-howto-03-query-the-api.jpg)
```

---

### fig-howto-04: How to Use the Agent Sidebar

**Alt text:** How-to guide: AI agent sidebar for natural-language music attribution queries, showing a split-panel layout with a works dashboard displaying confidence scores on the left and a three-step conversational flow on the right -- user asks about music credits, agent invokes attribution tools, and returns a transparent confidence-scored answer with source provenance.

**Caption:** *Agent sidebar interaction flow for the Music Attribution Scaffold. Non-technical users query music credits in natural language while the agent transparently invokes the same tools as the REST API, returning confidence-scored attribution data without requiring API knowledge (Teikari, 2026).*

**Embed:**
```markdown
![How-to guide: AI agent sidebar for natural-language music attribution queries, showing a split-panel layout with a works dashboard displaying confidence scores on the left and a three-step conversational flow on the right -- user asks about music credits, agent invokes attribution tools, and returns a transparent confidence-scored answer with source provenance.](docs/figures/repo-figures/assets/fig-howto-04-use-agent-sidebar.jpg)
```

---

### fig-howto-05: How to Check Permissions via MCP

**Alt text:** Workflow diagram: four-step MCP permission check for music attribution consent infrastructure, showing how an AI agent queries training rights for a music work, the MCP server evaluates rights holder preferences, and returns a structured ALLOW or DENY response with assurance level -- replacing ambiguous license text with machine-readable, transparent confidence-backed consent.

**Caption:** *MCP consent infrastructure workflow for the Music Attribution Scaffold. An AI agent submits a structured permission query, the MCP server evaluates it against declared rights holder preferences, and returns an explicit decision with provenance -- embodying the paper's principle that consent must be machine-readable, not buried in license text (Teikari, 2026).*

**Embed:**
```markdown
![Workflow diagram: four-step MCP permission check for music attribution consent infrastructure, showing how an AI agent queries training rights for a music work, the MCP server evaluates rights holder preferences, and returns a structured ALLOW or DENY response with assurance level -- replacing ambiguous license text with machine-readable, transparent confidence-backed consent.](docs/figures/repo-figures/assets/fig-howto-05-check-permissions-mcp.jpg)
```

---

### fig-howto-06: How to Run Tests (Which Tests, When)

**Alt text:** Tutorial diagram: decision tree for selecting the correct test suite in the open-source music attribution scaffold, branching from code change type -- Python backend, frontend components, or configuration -- to specific make commands for unit tests, integration tests, Vitest, Playwright E2E, and pre-commit hooks, ensuring transparent confidence in every code contribution.

**Caption:** *Test selection decision tree for the Music Attribution Scaffold. Contributors follow branching paths based on what they changed -- Python logic, database models, frontend components, or configuration -- to run the minimal necessary test suite, with pre-commit hooks as a universal quality gate before every commit (Teikari, 2026).*

**Embed:**
```markdown
![Tutorial diagram: decision tree for selecting the correct test suite in the open-source music attribution scaffold, branching from code change type -- Python backend, frontend components, or configuration -- to specific make commands for unit tests, integration tests, Vitest, Playwright E2E, and pre-commit hooks, ensuring transparent confidence in every code contribution.](docs/figures/repo-figures/assets/fig-howto-06-run-tests.jpg)
```

---

### fig-howto-07: How to Deploy to Production

**Alt text:** Step-by-step guide: five-step production deployment pipeline for the open-source music attribution scaffold, from Docker build through environment configuration and registry push to a branching deploy step with four paths -- Render PaaS, Kamal 2 Docker deployment, Hetzner with Ubicloud managed Kubernetes, and Big Three hyperscalers -- concluding with health check and Prometheus metrics verification, reflecting the scaffold philosophy that deployment paths vary by team archetype.

**Caption:** *Production deployment pipeline for the Music Attribution Scaffold. Four deployment paths reflect PRD v2.1.0: Render for PaaS simplicity, Kamal 2 for Docker-without-K8s, Hetzner+Ubicloud for managed infrastructure at budget prices, and Big Three for enterprise compliance -- all converging on the same health and metrics verification endpoints (Teikari, 2026).*

**Embed:**
```markdown
![Step-by-step guide: five-step production deployment pipeline for the open-source music attribution scaffold, from Docker build through environment configuration and registry push to a branching deploy step with four paths -- Render PaaS, Kamal 2 Docker deployment, Hetzner with Ubicloud managed Kubernetes, and Big Three hyperscalers -- concluding with health check and Prometheus metrics verification, reflecting the scaffold philosophy that deployment paths vary by team archetype.](docs/figures/repo-figures/assets/fig-howto-07-deploy-to-production.jpg)
```

---

### fig-howto-08: How to Create Figures (Nano Banana Pro)

**Alt text:** Workflow diagram: six-step figure creation pipeline for the music attribution scaffold documentation, from reading the figure plan and style guide through prompt composition and Nano Banana Pro image generation to quality verification and repository commit -- demonstrating content-style decoupling where plans define what to show and the style guide defines how, ensuring consistent open-source visual identity.

**Caption:** *Figure creation workflow for the Music Attribution Scaffold. The content-style decoupling principle separates semantic figure plans (what to communicate) from the visual style guide (how it should look), enabling reproducible, quality-gated figure generation via Nano Banana Pro with a minimum score threshold of 21/25 (Teikari, 2026).*

**Embed:**
```markdown
![Workflow diagram: six-step figure creation pipeline for the music attribution scaffold documentation, from reading the figure plan and style guide through prompt composition and Nano Banana Pro image generation to quality verification and repository commit -- demonstrating content-style decoupling where plans define what to show and the style guide defines how, ensuring consistent open-source visual identity.](docs/figures/repo-figures/assets/fig-howto-08-create-figures-nano-banana.jpg)
```

---

### fig-howto-09: How to Contribute

**Alt text:** How-to guide: six-step open-source contribution workflow for the music attribution scaffold, from fork and clone through branch creation, coding under CLAUDE.md behavioral rules, pre-commit hook verification, test execution, and pull request submission -- quality gates including ruff, mypy, and pytest are non-negotiable for both human and AI contributors to maintain transparent confidence in music metadata code.

**Caption:** *Contribution workflow for the Music Attribution Scaffold. The CLAUDE.md behavioral contract governs both human and AI contributors equally, with four mandatory quality gates (ruff check, ruff format, mypy, pytest) ensuring that every pull request meets the project's standards for code quality and attribution correctness (Teikari, 2026).*

**Embed:**
```markdown
![How-to guide: six-step open-source contribution workflow for the music attribution scaffold, from fork and clone through branch creation, coding under CLAUDE.md behavioral rules, pre-commit hook verification, test execution, and pull request submission -- quality gates including ruff, mypy, and pytest are non-negotiable for both human and AI contributors to maintain transparent confidence in music metadata code.](docs/figures/repo-figures/assets/fig-howto-09-how-to-contribute.jpg)
```

---

## Probabilistic PRD (fig-prd-*)

### fig-prd-01: Probabilistic PRD -- What Is It?

**Alt text:** Comparison chart: traditional product requirements document versus probabilistic PRD for music attribution scaffold, showing how weighted decision options replace fixed specs and adapt transparent confidence scoring to different team archetypes -- enabling open-source flexibility instead of one-size-fits-all architecture.

**Caption:** *Figure 1. A probabilistic PRD replaces fixed specifications with weighted options that shift depending on team constraints, enabling the same music attribution scaffold to serve engineer-heavy startups, musician-first teams, and solo hackers alike.*

**Embed:**
```markdown
![Comparison chart: traditional product requirements document versus probabilistic PRD for music attribution scaffold, showing how weighted decision options replace fixed specs and adapt transparent confidence scoring to different team archetypes -- enabling open-source flexibility instead of one-size-fits-all architecture.](docs/figures/repo-figures/assets/fig-prd-01-probabilistic-prd-eli5.jpg)
```

---

### fig-prd-02: Full Decision Network Overview

**Alt text:** Network visualization: complete Bayesian decision network for the music attribution scaffold with 30-plus nodes across five levels -- business, architecture, implementation, deployment, and operations -- connected by 60-plus conditional probability edges that govern transparent confidence scoring for music metadata and credits.

**Caption:** *Figure 2. The full probabilistic PRD decision network maps every architectural choice in the open-source music attribution scaffold, from L1 business strategy through L5 operations, revealing how upstream decisions conditionally shape downstream technology selection.*

**Embed:**
```markdown
![Network visualization: complete Bayesian decision network for the music attribution scaffold with 30-plus nodes across five levels -- business, architecture, implementation, deployment, and operations -- connected by 60-plus conditional probability edges that govern transparent confidence scoring for music metadata and credits.](docs/figures/repo-figures/assets/fig-prd-02-full-decision-network.jpg)
```

---

### fig-prd-03: Level 0-1 Foundation Decisions

**Alt text:** Decision diagram: four foundational business decisions -- build versus buy, target market, revenue model, and regulatory posture -- cascading into architecture-level choices for the music attribution scaffold, with skip connections showing how business strategy directly shapes music metadata infrastructure and confidence scoring design.

**Caption:** *Figure 3. The L1 business layer of the probabilistic PRD sets the probability landscape for all downstream technology decisions, with skip connections reaching directly from business strategy to database and compute choices.*

**Embed:**
```markdown
![Decision diagram: four foundational business decisions -- build versus buy, target market, revenue model, and regulatory posture -- cascading into architecture-level choices for the music attribution scaffold, with skip connections showing how business strategy directly shapes music metadata infrastructure and confidence scoring design.](docs/figures/repo-figures/assets/fig-prd-03-foundation-decisions.jpg)
```

---

### fig-prd-04: Level 2 Integration Decisions

**Alt text:** Architecture overview: integration-layer decisions in the music attribution scaffold showing how L2 architecture choices cascade into the selected implementation stack -- Anthropic LLM provider, PydanticAI routing, and CopilotKit agentic UI -- with conditional probability reinforcement between each open-source component selection.

**Caption:** *Figure 4. The scaffold's reference implementation selects a coherent integration stack where each choice reinforces the others: Anthropic primary, PydanticAI native routing, and CopilotKit AG-UI, all connected by Bayesian conditional probabilities in the decision network.*

**Embed:**
```markdown
![Architecture overview: integration-layer decisions in the music attribution scaffold showing how L2 architecture choices cascade into the selected implementation stack -- Anthropic LLM provider, PydanticAI routing, and CopilotKit agentic UI -- with conditional probability reinforcement between each open-source component selection.](docs/figures/repo-figures/assets/fig-prd-04-integration-decisions.jpg)
```

---

### fig-prd-05: Level 3-4 Operational Decisions

**Alt text:** Decision diagram: deployment and operations layers of the music attribution scaffold PRD v2.1.0, showing six L4 deployment nodes including new Object Storage with Cloudflare R2, expanded compute platform with Hetzner Ubicloud and bare-metal paths, Kamal 2 container strategy, Pulumi as recommended IaC, and L5 operations with active FinOps strategy -- the most team-archetype-sensitive decisions in the open-source probabilistic PRD.

**Caption:** *Figure 5. PRD v2.1.0 operational decisions with expanded deployment layer: Object Storage (Cloudflare R2), two Hetzner paths (Ubicloud managed vs bare-metal), Kamal 2 container strategy, and Pulumi as recommended IaC -- cascading into an active FinOps strategy with DevOps tax analysis.*

**Embed:**
```markdown
![Decision diagram: deployment and operations layers of the music attribution scaffold PRD v2.1.0, showing six L4 deployment nodes including new Object Storage with Cloudflare R2, expanded compute platform with Hetzner Ubicloud and bare-metal paths, Kamal 2 container strategy, Pulumi as recommended IaC, and L5 operations with active FinOps strategy -- the most team-archetype-sensitive decisions in the open-source probabilistic PRD.](docs/figures/repo-figures/assets/fig-prd-05-operational-decisions.jpg)
```

---

### fig-prd-06: Decision Node Anatomy

**Alt text:** Annotated diagram: anatomy of a single Bayesian decision node in the music attribution scaffold probabilistic PRD, using the LLM provider node as example -- showing options with prior probabilities, conditional dependencies, team archetype weight overrides, volatility classification, and domain applicability scores for transparent confidence scoring.

**Caption:** *Figure 6. Each decision node in the probabilistic PRD is a self-contained Bayesian unit: options with prior probabilities summing to 1.0, conditional tables linking to parent decisions, archetype-specific weight overrides, and volatility classification -- all defined in a machine-readable YAML schema.*

**Embed:**
```markdown
![Annotated diagram: anatomy of a single Bayesian decision node in the music attribution scaffold probabilistic PRD, using the LLM provider node as example -- showing options with prior probabilities, conditional dependencies, team archetype weight overrides, volatility classification, and domain applicability scores for transparent confidence scoring.](docs/figures/repo-figures/assets/fig-prd-06-decision-node-anatomy.jpg)
```

---

### fig-prd-07: Team Archetypes

**Alt text:** Comparison chart: four team archetypes for the music attribution scaffold -- Engineer-Heavy, Musician-First, Solo Hacker, and Well-Funded Startup -- each producing different probability distributions for database, compute, and build-versus-buy decisions, demonstrating how the open-source attribution scaffold adapts to diverse music credits workflows.

**Caption:** *Figure 7. Four team archetypes act as probability lenses on the same decision network: an engineer-heavy startup gravitates toward PostgreSQL and custom builds, while a musician-first team favors Supabase and managed services -- same scaffold, four fundamentally different instantiations.*

**Embed:**
```markdown
![Comparison chart: four team archetypes for the music attribution scaffold -- Engineer-Heavy, Musician-First, Solo Hacker, and Well-Funded Startup -- each producing different probability distributions for database, compute, and build-versus-buy decisions, demonstrating how the open-source attribution scaffold adapts to diverse music credits workflows.](docs/figures/repo-figures/assets/fig-prd-07-team-archetypes.jpg)
```

---

### fig-prd-08: Domain Overlay System

**Alt text:** Architecture overview: domain overlay system showing how the music attribution scaffold and Digital Product Passport traceability share an isomorphic core pipeline -- sources, entity resolution, unified record with confidence scoring, permissioned API, and agentic consumers -- while differing in music metadata sources, assurance levels (A0-A3 versus T0-T3), and regulatory requirements.

**Caption:** *Figure 8. The domain overlay system demonstrates the scaffold's generalizability: music attribution (A0-A3 assurance, MCP consent API) and supply chain traceability (T0-T3 assurance, Digital Link API) are isomorphic instantiations of the same open-source pipeline architecture.*

**Embed:**
```markdown
![Architecture overview: domain overlay system showing how the music attribution scaffold and Digital Product Passport traceability share an isomorphic core pipeline -- sources, entity resolution, unified record with confidence scoring, permissioned API, and agentic consumers -- while differing in music metadata sources, assurance levels (A0-A3 versus T0-T3), and regulatory requirements.](docs/figures/repo-figures/assets/fig-prd-08-domain-overlay-system.jpg)
```

---

### fig-prd-09: PRD Versioning Timeline

**Alt text:** Timeline visualization: evolution of the music attribution scaffold probabilistic PRD from v1.0 core foundation through v2.0 regulatory compliance, showing incremental growth from 15 to 40-plus decision nodes across seven versions -- demonstrating how open-source music metadata architecture decisions expand without disrupting existing Bayesian confidence scoring structures.

**Caption:** *Figure 9. The probabilistic PRD has grown incrementally from 15 initial nodes (v1.0) to 40-plus nodes (v2.0), adding agentic UI, LLM routing, commercial landscape, xOps, and regulatory compliance decisions -- each version extending the network without breaking prior conditional probability structures.*

**Embed:**
```markdown
![Timeline visualization: evolution of the music attribution scaffold probabilistic PRD from v1.0 core foundation through v2.0 regulatory compliance, showing incremental growth from 15 to 40-plus decision nodes across seven versions -- demonstrating how open-source music metadata architecture decisions expand without disrupting existing Bayesian confidence scoring structures.](docs/figures/repo-figures/assets/fig-prd-09-versioning-timeline.jpg)
```

---

### fig-prd-10: Scaffold vs Product

**Alt text:** Comparison chart: open-source music attribution scaffold versus production system -- the scaffold offers configurable decision paths, research-oriented trade-off exploration, and a probabilistic PRD with four team archetypes, while a production system deploys one fixed architecture, highlighting that this repository is a teaching and research framework for music credits and transparent confidence scoring.

**Caption:** *Figure 10. This repository is a scaffold -- a configurable research framework with weighted decision options, four team archetypes, and two domain overlays -- not a deployed production system. Companion code to Teikari (2026), SSRN No. 6109087.*

**Embed:**
```markdown
![Comparison chart: open-source music attribution scaffold versus production system -- the scaffold offers configurable decision paths, research-oriented trade-off exploration, and a probabilistic PRD with four team archetypes, while a production system deploys one fixed architecture, highlighting that this repository is a teaching and research framework for music credits and transparent confidence scoring.](docs/figures/repo-figures/assets/fig-prd-10-scaffold-vs-product.jpg)
```

---

## Repository Overview (fig-repo-*)

### fig-repo-01: Hero Overview

**Alt text:** Repository overview: split-panel infographic contrasting fragmented music metadata across siloed databases with an open-source attribution scaffold that unifies sources into a single record with 0.87 transparent confidence scoring, multi-source resolution, and A0-A3 assurance levels for music credits provenance.

**Caption:** *Figure 1. The Music Attribution Scaffold addresses the industry-wide problem of 40%+ incorrect or incomplete music metadata by providing open-source infrastructure for probabilistic confidence scoring, multi-source entity resolution, and machine-readable consent (companion code to Teikari, 2026, SSRN No. 6109087).*

**Embed:**
```markdown
![Repository overview: split-panel infographic contrasting fragmented music metadata across siloed databases with an open-source attribution scaffold that unifies sources into a single record with 0.87 transparent confidence scoring, multi-source resolution, and A0-A3 assurance levels for music credits provenance.](docs/figures/repo-figures/assets/fig-repo-01-hero-overview.jpg)
```

---

### fig-repo-02: Five-Pipeline Architecture

**Alt text:** Architecture diagram: five-pipeline data flow for music attribution showing ETL ingestion from Discogs and MusicBrainz, entity resolution with fuzzy matching, transparent confidence scoring engine, FastAPI REST and MCP permission server, and PydanticAI chat interface with Pydantic boundary objects ensuring type-safe handoffs across the open-source attribution scaffold.

**Caption:** *Figure 2. The five-pipeline architecture transforms raw music metadata into confidence-scored attribution records through sequential stages (ETL, Entity Resolution, Attribution Engine, API/MCP, Chat), with Pydantic boundary objects (NormalizedRecord, ResolvedEntity, AttributionRecord) enforcing type safety at each transition and PostgreSQL with pgvector as the shared persistence layer.*

**Embed:**
```markdown
![Architecture diagram: five-pipeline data flow for music attribution showing ETL ingestion from Discogs and MusicBrainz, entity resolution with fuzzy matching, transparent confidence scoring engine, FastAPI REST and MCP permission server, and PydanticAI chat interface with Pydantic boundary objects ensuring type-safe handoffs across the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-repo-02-five-pipeline-architecture.jpg)
```

---

### fig-repo-03: Directory Map

**Alt text:** Reference card: annotated directory tree of the open-source music attribution scaffold showing Python 3.13 backend in src/music_attribution with five pipeline modules, Next.js 15 frontend, 393 backend tests plus 265 frontend tests, Docker infrastructure, and pyproject.toml as the single source of truth for dependency management via uv.

**Caption:** *Figure 3. The repository follows a clear separation of concerns: backend Python source in src/music_attribution/ maps directly to the five-pipeline architecture, while tests mirror the source structure and all configuration converges on pyproject.toml as the single source of truth.*

**Embed:**
```markdown
![Reference card: annotated directory tree of the open-source music attribution scaffold showing Python 3.13 backend in src/music_attribution with five pipeline modules, Next.js 15 frontend, 393 backend tests plus 265 frontend tests, Docker infrastructure, and pyproject.toml as the single source of truth for dependency management via uv.](docs/figures/repo-figures/assets/fig-repo-03-directory-map.jpg)
```

---

### fig-repo-04: Technology Stack

**Alt text:** Reference card: full technology stack for the open-source music attribution scaffold listing Python 3.13 backend with FastAPI, SQLAlchemy, and PydanticAI for transparent confidence scoring, Next.js 15 frontend with Jotai and CopilotKit AG-UI, and shared infrastructure including Docker, GitHub Actions, PostgreSQL with pgvector, and uv package manager -- every dependency a deliberate PRD decision.

**Caption:** *Figure 4. The technology stack spans a modern Python 3.13 backend (FastAPI, SQLAlchemy 2.0, PydanticAI, Splink v4) and TypeScript frontend (Next.js 15, Tailwind CSS v4, CopilotKit), with every dependency documented in the probabilistic PRD decision network.*

**Embed:**
```markdown
![Reference card: full technology stack for the open-source music attribution scaffold listing Python 3.13 backend with FastAPI, SQLAlchemy, and PydanticAI for transparent confidence scoring, Next.js 15 frontend with Jotai and CopilotKit AG-UI, and shared infrastructure including Docker, GitHub Actions, PostgreSQL with pgvector, and uv package manager -- every dependency a deliberate PRD decision.](docs/figures/repo-figures/assets/fig-repo-04-technology-stack.jpg)
```

---

### fig-repo-05: Quickstart Flow

**Alt text:** Workflow diagram: four-step quickstart for the open-source music attribution scaffold using Makefile commands -- clone and install with uv, start Docker Compose development stack with PostgreSQL and FastAPI, verify health endpoints, and run 658 tests across backend pytest and frontend Vitest suites, demonstrating a five-minute path to a running local environment.

**Caption:** *Figure 5. The quickstart flow reduces onboarding to four commands: install dependencies via uv, launch the six-service Docker Compose stack, verify backend and frontend health endpoints, and run the full test suite -- requiring only git, Docker, and uv as prerequisites.*

**Embed:**
```markdown
![Workflow diagram: four-step quickstart for the open-source music attribution scaffold using Makefile commands -- clone and install with uv, start Docker Compose development stack with PostgreSQL and FastAPI, verify health endpoints, and run 658 tests across backend pytest and frontend Vitest suites, demonstrating a five-minute path to a running local environment.](docs/figures/repo-figures/assets/fig-repo-05-quickstart-flow.jpg)
```

---

### fig-repo-06: Make Commands Map

**Alt text:** Reference card: Makefile developer command interface for the music attribution scaffold organized into eight categories across Docker CI-parity and local fast-iteration modes, covering dependency management with uv, testing with pytest, linting with ruff and mypy, Next.js 15 frontend commands, and PydanticAI agent workflows -- a single entry point for all development tasks.

**Caption:** *Figure 6. The Makefile serves as the primary developer interface, organizing all commands into Docker-based (CI-parity) and local (fast-iteration) execution modes, ensuring contributors always use the correct environment for their workflow.*

**Embed:**
```markdown
![Reference card: Makefile developer command interface for the music attribution scaffold organized into eight categories across Docker CI-parity and local fast-iteration modes, covering dependency management with uv, testing with pytest, linting with ruff and mypy, Next.js 15 frontend commands, and PydanticAI agent workflows -- a single entry point for all development tasks.](docs/figures/repo-figures/assets/fig-repo-06-make-commands-map.jpg)
```

---

### fig-repo-07: Pre-commit Quality Gates

**Alt text:** Workflow diagram: seven pre-commit quality gates for the music attribution scaffold showing sequential hooks from whitespace trimming and YAML validation through uv lockfile sync, ruff linting, ruff formatting, mypy static type checking, and detect-secrets scanning -- any hook failure blocks the commit, enforcing zero-exception code quality on every contribution to the open-source project.

**Caption:** *Figure 7. Seven automated pre-commit hooks form an unbypassable quality gate chain: every commit to the music attribution scaffold must pass whitespace, config validation, lockfile sync, linting, formatting, type checking, and secret detection before it can land.*

**Embed:**
```markdown
![Workflow diagram: seven pre-commit quality gates for the music attribution scaffold showing sequential hooks from whitespace trimming and YAML validation through uv lockfile sync, ruff linting, ruff formatting, mypy static type checking, and detect-secrets scanning -- any hook failure blocks the commit, enforcing zero-exception code quality on every contribution to the open-source project.](docs/figures/repo-figures/assets/fig-repo-07-precommit-quality-gates.jpg)
```

---

### fig-repo-08: CI/CD Pipeline

**Alt text:** Workflow diagram: GitHub Actions CI pipeline for the music attribution scaffold with path-based filtering that conditionally triggers five jobs -- backend ruff lint, mypy type checking, and pytest with Codecov coverage, integration tests on real PostgreSQL via testcontainers, Claude constraint guardrails, frontend Vitest and ESLint, and Playwright E2E -- eliminating wasted compute on irrelevant changes.

**Caption:** *Figure 8. The GitHub Actions CI pipeline uses dorny/paths-filter to intelligently route changes: backend modifications (src/, tests/, pyproject.toml) trigger Python linting, typing, and testing jobs, while frontend changes trigger ESLint, TypeScript, Vitest, and Playwright jobs -- with cancel-in-progress concurrency to avoid redundant runs.*

**Embed:**
```markdown
![Workflow diagram: GitHub Actions CI pipeline for the music attribution scaffold with path-based filtering that conditionally triggers five jobs -- backend ruff lint, mypy type checking, and pytest with Codecov coverage, integration tests on real PostgreSQL via testcontainers, Claude constraint guardrails, frontend Vitest and ESLint, and Playwright E2E -- eliminating wasted compute on irrelevant changes.](docs/figures/repo-figures/assets/fig-repo-08-ci-cd-pipeline.jpg)
```

---

### fig-repo-09: Docker Architecture

**Alt text:** Architecture diagram: Docker Compose six-service development stack for the music attribution scaffold showing PostgreSQL 17 with pgvector for music metadata storage, PgBouncer connection pooling, Valkey cache, FastAPI backend with hot-reload, Next.js 15 frontend, and opt-in Prometheus plus Grafana monitoring -- all launched via a single make dev command for transparent confidence scoring development.

**Caption:** *Figure 9. The Docker Compose development stack provides six always-on services (PostgreSQL 17 with pgvector, PgBouncer, Valkey, FastAPI backend, Next.js frontend) with opt-in Prometheus and Grafana monitoring, ensuring every developer runs an identical local environment via docker-compose.dev.yml.*

**Embed:**
```markdown
![Architecture diagram: Docker Compose six-service development stack for the music attribution scaffold showing PostgreSQL 17 with pgvector for music metadata storage, PgBouncer connection pooling, Valkey cache, FastAPI backend with hot-reload, Next.js 15 frontend, and opt-in Prometheus plus Grafana monitoring -- all launched via a single make dev command for transparent confidence scoring development.](docs/figures/repo-figures/assets/fig-repo-09-docker-architecture.jpg)
```

---

### fig-repo-10: Testing Pyramid

**Alt text:** Quality assurance diagram: testing pyramid for the open-source music attribution scaffold with 658 total tests across four layers -- 351 pytest unit tests validating Pydantic schemas and confidence scoring at the base, 42 integration tests against real PostgreSQL via testcontainers, 265 Vitest frontend tests with WCAG accessibility checks, and Playwright E2E browser tests at the tip, ensuring robust music metadata quality.

**Caption:** *Figure 10. The testing pyramid demonstrates the project's quality commitment: 351 unit tests cover all Pydantic models and pipeline stages, 42 integration tests validate against real PostgreSQL with pgvector via testcontainers, and 265 frontend tests include 11 agent integration tests and component-level WCAG checks via vitest-axe.*

**Embed:**
```markdown
![Quality assurance diagram: testing pyramid for the open-source music attribution scaffold with 658 total tests across four layers -- 351 pytest unit tests validating Pydantic schemas and confidence scoring at the base, 42 integration tests against real PostgreSQL via testcontainers, 265 Vitest frontend tests with WCAG accessibility checks, and Playwright E2E browser tests at the tip, ensuring robust music metadata quality.](docs/figures/repo-figures/assets/fig-repo-10-testing-pyramid.jpg)
```

---

### fig-repo-11: Frontend Architecture

**Alt text:** Architecture diagram: Next.js 15 App Router frontend for the music attribution scaffold showing four route pages for works catalog, attribution review queue, and MCP permissions, 19 domain-organized component modules including confidence gauges and provenance badges, and cross-cutting layers of Jotai state management, Tailwind CSS v4 design tokens, CopilotKit AG-UI agent integration, and PostHog analytics for transparent music credits visualization.

**Caption:** *Figure 11. The frontend architecture organizes 19 component modules by domain (attribution, confidence, provenance, works, chat, permissions) rather than technical role, with four App Router pages and cross-cutting Jotai state, Tailwind CSS v4 design tokens, and CopilotKit AG-UI integration.*

**Embed:**
```markdown
![Architecture diagram: Next.js 15 App Router frontend for the music attribution scaffold showing four route pages for works catalog, attribution review queue, and MCP permissions, 19 domain-organized component modules including confidence gauges and provenance badges, and cross-cutting layers of Jotai state management, Tailwind CSS v4 design tokens, CopilotKit AG-UI agent integration, and PostHog analytics for transparent music credits visualization.](docs/figures/repo-figures/assets/fig-repo-11-frontend-architecture.jpg)
```

---

### fig-repo-12: Backend-Frontend Connection

**Alt text:** Architecture diagram: two communication channels connecting the music attribution scaffold frontend and backend -- REST JSON via FastAPI for music metadata CRUD operations on works, permissions, and health endpoints, and AG-UI Server-Sent Events for real-time PydanticAI agent conversation via CopilotKit with transparent confidence scoring queries, both served from a single FastAPI application on port 8000.

**Caption:** *Figure 12. The backend-frontend integration uses dual communication channels: traditional REST for attribution data CRUD and AG-UI SSE for streaming AI agent conversation, with CopilotKit hooks (useCopilotReadable, useCopilotAction) on the frontend connecting to a PydanticAI agent with four tools and claude-haiku-4-5 default model on the backend.*

**Embed:**
```markdown
![Architecture diagram: two communication channels connecting the music attribution scaffold frontend and backend -- REST JSON via FastAPI for music metadata CRUD operations on works, permissions, and health endpoints, and AG-UI Server-Sent Events for real-time PydanticAI agent conversation via CopilotKit with transparent confidence scoring queries, both served from a single FastAPI application on port 8000.](docs/figures/repo-figures/assets/fig-repo-12-backend-frontend-connection.jpg)
```

---

### fig-repo-13: Environment Variables

**Alt text:** Reference card: environment variable configuration map for the music attribution scaffold organized by service -- FastAPI backend requiring DATABASE_URL with psycopg3 async driver and CORS_ORIGINS, PostgreSQL 17 database credentials, PgBouncer transaction pooling settings, and Next.js 15 frontend API URL, following 12-factor app principles with detect-secrets pre-commit enforcement preventing credential leaks in the open-source repository.

**Caption:** *Figure 13. All configuration flows through environment variables following 12-factor app principles, with sensible development defaults in docker-compose.dev.yml and pydantic-settings managing validation -- detect-secrets pre-commit hooks ensure no credentials are ever committed.*

**Embed:**
```markdown
![Reference card: environment variable configuration map for the music attribution scaffold organized by service -- FastAPI backend requiring DATABASE_URL with psycopg3 async driver and CORS_ORIGINS, PostgreSQL 17 database credentials, PgBouncer transaction pooling settings, and Next.js 15 frontend API URL, following 12-factor app principles with detect-secrets pre-commit enforcement preventing credential leaks in the open-source repository.](docs/figures/repo-figures/assets/fig-repo-13-environment-variables.jpg)
```

---

### fig-repo-14: Database Schema

**Alt text:** Architecture diagram: PostgreSQL 17 entity-relationship schema for the music attribution scaffold showing Works table with ISRC and ISWC identifiers, Artists table with ISNI, Attribution Records junction table carrying per-field transparent confidence scores from 0.0 to 1.0, A0-A3 assurance levels, and JSONB source provenance arrays, plus a Permissions table for MCP machine-readable consent -- with pgvector VECTOR columns enabling semantic similarity search across music metadata.

**Caption:** *Figure 14. The database schema embeds attribution-by-design principles: every attribution record carries per-field confidence scores (FLOAT 0.0-1.0), A0-A3 assurance levels, and full JSONB source provenance, while pgvector VECTOR columns enable semantic similarity search across works and artists, all managed by Alembic migrations over SQLAlchemy 2.0 models.*

**Embed:**
```markdown
![Architecture diagram: PostgreSQL 17 entity-relationship schema for the music attribution scaffold showing Works table with ISRC and ISWC identifiers, Artists table with ISNI, Attribution Records junction table carrying per-field transparent confidence scores from 0.0 to 1.0, A0-A3 assurance levels, and JSONB source provenance arrays, plus a Permissions table for MCP machine-readable consent -- with pgvector VECTOR columns enabling semantic similarity search across music metadata.](docs/figures/repo-figures/assets/fig-repo-14-database-schema.jpg)
```

---

### fig-repo-15: CLAUDE.md Hierarchy

**Alt text:** Reference card: CLAUDE.md AI-assisted development configuration hierarchy for the music attribution scaffold showing layered governance from root behavior contract through five numbered domain rules covering code analysis and frontend design system, self-learning TDD skill workflows, and institutional knowledge memory -- with file-pattern context loading triggers that automatically activate relevant rules when editing Python 3.13 backend, Next.js 15 frontend, or test files in the open-source project.

**Caption:** *Figure 15. The AI assistant configuration system governs development behavior through four layers: root CLAUDE.md sets the behavior contract and forbidden actions, numbered rules add domain-specific guidance (AST-only code analysis, frontend design tokens), skills provide reusable TDD workflows, and memory files capture persistent failure patterns and institutional knowledge.*

**Embed:**
```markdown
![Reference card: CLAUDE.md AI-assisted development configuration hierarchy for the music attribution scaffold showing layered governance from root behavior contract through five numbered domain rules covering code analysis and frontend design system, self-learning TDD skill workflows, and institutional knowledge memory -- with file-pattern context loading triggers that automatically activate relevant rules when editing Python 3.13 backend, Next.js 15 frontend, or test files in the open-source project.](docs/figures/repo-figures/assets/fig-repo-15-claude-md-hierarchy.jpg)
```

---

## Theoretical Foundations (fig-theory-*)

### fig-theory-01: The Oracle Problem -- ELI5 (Mixing Paint Analogy)

**Alt text:** Concept diagram: three paint tubes labeled Artist A, B, and C flow into a blender representing AI model training, producing uniform purple output that cannot be unmixed -- illustrating the oracle problem in music attribution, where transparent confidence scoring is impossible after creative works are blended during generative AI training, a fundamental limitation for open-source attribution scaffolds.

**Caption:** *Figure 1. The Oracle Problem explained through a paint-mixing analogy: once creative works are blended inside an AI model through gradient descent and weight averaging, separating individual contributions is fundamentally impossible -- motivating the attribution-by-design approach used in this scaffold.*

**Embed:**
```markdown
![Concept diagram: three paint tubes labeled Artist A, B, and C flow into a blender representing AI model training, producing uniform purple output that cannot be unmixed -- illustrating the oracle problem in music attribution, where transparent confidence scoring is impossible after creative works are blended during generative AI training, a fundamental limitation for open-source attribution scaffolds.](docs/figures/repo-figures/assets/fig-theory-01-oracle-problem-eli5.jpg)
```

---

### fig-theory-02: The Oracle Problem -- Technical Pipeline

**Alt text:** Technical pipeline diagram: creator audio, MIDI, and music metadata enter a training black box where tokenization, gradient descent, and weight averaging destroy attribution identity at each stage -- showing the oracle problem's analog hole bypass that breaks digital provenance chains, a core challenge for transparent confidence in music attribution systems.

**Caption:** *Figure 2. The Oracle Problem at the technical level: attribution information is irreversibly destroyed through tokenization, gradient descent, and weight averaging during model training, while the analog hole further undermines provenance by allowing physical re-encoding to break digital identity chains.*

**Embed:**
```markdown
![Technical pipeline diagram: creator audio, MIDI, and music metadata enter a training black box where tokenization, gradient descent, and weight averaging destroy attribution identity at each stage -- showing the oracle problem's analog hole bypass that breaks digital provenance chains, a core challenge for transparent confidence in music attribution systems.](docs/figures/repo-figures/assets/fig-theory-02-oracle-problem-technical.jpg)
```

---

### fig-theory-03: Attribution-by-Design vs Post-Hoc Detection

**Alt text:** Comparison chart: post-hoc detection with broken arrows leading to no answer versus attribution-by-design with embedded provenance at ISRC and ISWC assurance levels A0-A3 leading to queryable music credits -- demonstrating why open-source music attribution scaffolds must embed provenance before AI training rather than attempting reverse engineering afterward.

**Caption:** *Figure 3. Attribution-by-design versus post-hoc detection: the left panel shows how the oracle problem blocks reverse engineering of AI-generated output, while the right panel shows how embedding provenance with industry identifiers (ISRC, ISWC, ISNI) before training preserves queryable attribution.*

**Embed:**
```markdown
![Comparison chart: post-hoc detection with broken arrows leading to no answer versus attribution-by-design with embedded provenance at ISRC and ISWC assurance levels A0-A3 leading to queryable music credits -- demonstrating why open-source music attribution scaffolds must embed provenance before AI training rather than attempting reverse engineering afterward.](docs/figures/repo-figures/assets/fig-theory-03-attribution-by-design-vs-posthoc.jpg)
```

---

### fig-theory-04: Deterrence Economics

**Alt text:** Theory visualization: deterrence economics formula p times d times F greater than or equal to g with tax audit analogy showing that even a 3 percent audit rate deters infringement when penalties are large -- applied to music attribution where imperfect detection still protects music credits through transparent confidence scoring and economic incentives rather than perfect surveillance.

**Caption:** *Figure 4. Deterrence economics applied to music attribution: the system does not need perfect detection to be effective, just as tax audits deter fraud at a 3% audit rate -- the expected penalty (p x d x F) need only exceed the expected gain (g) from infringement.*

**Embed:**
```markdown
![Theory visualization: deterrence economics formula p times d times F greater than or equal to g with tax audit analogy showing that even a 3 percent audit rate deters infringement when penalties are large -- applied to music attribution where imperfect detection still protects music credits through transparent confidence scoring and economic incentives rather than perfect surveillance.](docs/figures/repo-figures/assets/fig-theory-04-deterrence-economics.jpg)
```

---

### fig-theory-05: Assurance Levels A0-A3 -- ELI5 (ID Card Analogy)

**Alt text:** Concept diagram: four-step staircase showing music attribution assurance levels A0 through A3 using ID card analogies -- A0 as sticky note with no identification, A1 as business card with a single claim, A2 as passport with multiple sources agreeing, A3 as biometric scan with direct artist verification -- illustrating how transparent confidence scoring grades trust in music credits.

**Caption:** *Figure 5. Assurance levels A0-A3 explained through everyday ID card analogies: every attribution claim carries a trust level from no data (A0) to artist-verified (A3), enabling music industry professionals to assess how much evidence supports each music credit.*

**Embed:**
```markdown
![Concept diagram: four-step staircase showing music attribution assurance levels A0 through A3 using ID card analogies -- A0 as sticky note with no identification, A1 as business card with a single claim, A2 as passport with multiple sources agreeing, A3 as biometric scan with direct artist verification -- illustrating how transparent confidence scoring grades trust in music credits.](docs/figures/repo-figures/assets/fig-theory-05-assurance-levels-eli5.jpg)
```

---

### fig-theory-06: Assurance Levels -- Standards Mapping

**Alt text:** Mapping diagram: pyramid mapping music attribution assurance levels A0 through A3 to industry standard identifiers -- A0 with no identifier, A1 with ISRC only, A2 adding ISWC for cross-database agreement, A3 adding ISNI and IPI for artist-verified identity -- with analog hole warning showing that even A3 is not tamper-proof, supporting transparent confidence in music metadata.

**Caption:** *Figure 6. Assurance levels mapped to music industry standard identifiers (ISRC, ISWC, ISNI, IPI): each level requires progressively stronger evidence, from no provenance at A0 to artist-verified identity at A3, while the analog hole warning acknowledges that no level guarantees tamper-proof attribution.*

**Embed:**
```markdown
![Mapping diagram: pyramid mapping music attribution assurance levels A0 through A3 to industry standard identifiers -- A0 with no identifier, A1 with ISRC only, A2 adding ISWC for cross-database agreement, A3 adding ISNI and IPI for artist-verified identity -- with analog hole warning showing that even A3 is not tamper-proof, supporting transparent confidence in music metadata.](docs/figures/repo-figures/assets/fig-theory-06-assurance-standards-mapping.jpg)
```

---

### fig-theory-07: Assurance Level Decision Tree

**Alt text:** Decision tree: deterministic classification flowchart for music attribution assurance levels -- starting with an attribution record, checking artist verification for A3, then independent source agreement for A2, then any ISRC or ISWC identifier for A1, defaulting to A0 unknown as fail-safe -- showing how the open-source attribution scaffold assigns transparent confidence levels to music credits.

**Caption:** *Figure 7. The assurance level decision tree evaluates top-down from A3 to A0: artist verification yields A3, cross-database agreement yields A2, any standard identifier yields A1, and absence of all evidence defaults to A0 as a fail-safe classification.*

**Embed:**
```markdown
![Decision tree: deterministic classification flowchart for music attribution assurance levels -- starting with an attribution record, checking artist verification for A3, then independent source agreement for A2, then any ISRC or ISWC identifier for A1, defaulting to A0 unknown as fail-safe -- showing how the open-source attribution scaffold assigns transparent confidence levels to music credits.](docs/figures/repo-figures/assets/fig-theory-07-assurance-decision-tree.jpg)
```

---

### fig-theory-08: Two-Friction Taxonomy -- ELI5 (Airport Analogy)

**Alt text:** Comparison chart: split panel contrasting administrative friction (airport security line representing licensing forms and royalty calculations to automate) with discovery friction (record store browsing representing curation and collaboration to preserve) -- the two-friction taxonomy for music attribution that guides which processes an open-source scaffold should automate and which it should protect.

**Caption:** *Figure 8. The two-friction taxonomy: not all friction in music attribution is bad. Administrative friction (licensing forms, royalty calculations, multi-platform registration) should be automated, while discovery friction (DJ curation, artist collaboration, playlist taste-making) must be preserved because it creates cultural value.*

**Embed:**
```markdown
![Comparison chart: split panel contrasting administrative friction (airport security line representing licensing forms and royalty calculations to automate) with discovery friction (record store browsing representing curation and collaboration to preserve) -- the two-friction taxonomy for music attribution that guides which processes an open-source scaffold should automate and which it should protect.](docs/figures/repo-figures/assets/fig-theory-08-two-friction-eli5.jpg)
```

---

### fig-theory-09: Two-Friction Diagnostic Test

**Alt text:** Decision tree: three-question diagnostic flowchart for classifying friction in music attribution pipelines -- testing for human agency, artistic identity, and community building -- where any YES identifies discovery friction to preserve and all NO identifies administrative friction to automate, enabling transparent confidence in which processes the open-source scaffold should target.

**Caption:** *Figure 9. The friction diagnostic test: three questions (Does it involve human agency? Does it build identity? Does it create community?) classify any friction point in the music attribution pipeline, using OR logic where any YES means discovery friction to preserve.*

**Embed:**
```markdown
![Decision tree: three-question diagnostic flowchart for classifying friction in music attribution pipelines -- testing for human agency, artistic identity, and community building -- where any YES identifies discovery friction to preserve and all NO identifies administrative friction to automate, enabling transparent confidence in which processes the open-source scaffold should target.](docs/figures/repo-figures/assets/fig-theory-09-friction-diagnostic-test.jpg)
```

---

### fig-theory-10: Real-World Friction Examples

**Alt text:** Categorization chart: real-world music industry friction examples classified into administrative friction (sync licensing, PRO registration, ISRC assignment, royalty reporting to automate) and discovery friction (DJ curation, artist collaboration, playlist gatekeeping to preserve) -- concrete applications of the two-friction taxonomy for music attribution and transparent confidence in which processes deserve automation.

**Caption:** *Figure 10. Real-world friction examples classified using the two-friction taxonomy: administrative friction involves compliance and process (licensing forms, ISRC assignment, royalty calculations), while discovery friction involves taste and relationships (DJ sets, co-writing sessions, editorial curation).*

**Embed:**
```markdown
![Categorization chart: real-world music industry friction examples classified into administrative friction (sync licensing, PRO registration, ISRC assignment, royalty reporting to automate) and discovery friction (DJ curation, artist collaboration, playlist gatekeeping to preserve) -- concrete applications of the two-friction taxonomy for music attribution and transparent confidence in which processes deserve automation.](docs/figures/repo-figures/assets/fig-theory-10-friction-examples.jpg)
```

---

### fig-theory-11: Confidence vs Uncertainty -- ELI5 (Weather Analogy)

**Alt text:** Comparison chart: point estimate (80 percent rain, 0.85 music attribution confidence) versus interval estimate (70-90 percent rain, 0.78-0.92 confidence range) using weather forecast analogy -- showing why transparent confidence scoring with intervals is more honest than single numbers for music credits, where narrow ranges signal certainty and wide ranges signal need for more evidence.

**Caption:** *Figure 11. Confidence versus uncertainty: a single number (0.85) hides how much the system actually knows, while an interval (0.78-0.92) communicates both the estimate and its uncertainty -- this scaffold uses interval estimates because they are more honest and actionable for music attribution decisions.*

**Embed:**
```markdown
![Comparison chart: point estimate (80 percent rain, 0.85 music attribution confidence) versus interval estimate (70-90 percent rain, 0.78-0.92 confidence range) using weather forecast analogy -- showing why transparent confidence scoring with intervals is more honest than single numbers for music credits, where narrow ranges signal certainty and wide ranges signal need for more evidence.](docs/figures/repo-figures/assets/fig-theory-11-confidence-vs-uncertainty-eli5.jpg)
```

---

### fig-theory-12: Conformal Prediction Methodology

**Alt text:** Technical pipeline diagram: four-step conformal prediction methodology for music attribution -- calibration set to nonconformity scores to quantile threshold to prediction sets with coverage guarantee P(Y in C(X)) >= 1-alpha -- providing distribution-free, finite-sample valid confidence scoring that requires no model assumptions, enabling transparent confidence intervals for music credits in the open-source attribution scaffold.

**Caption:** *Figure 12. Conformal prediction pipeline: held-out calibration data produces nonconformity scores, which yield a quantile threshold that generates prediction sets with formal coverage guarantees -- applied to music attribution where X represents source evidence features and C(X_new) is the set of plausible creators for a query.*

**Embed:**
```markdown
![Technical pipeline diagram: four-step conformal prediction methodology for music attribution -- calibration set to nonconformity scores to quantile threshold to prediction sets with coverage guarantee P(Y in C(X)) >= 1-alpha -- providing distribution-free, finite-sample valid confidence scoring that requires no model assumptions, enabling transparent confidence intervals for music credits in the open-source attribution scaffold.](docs/figures/repo-figures/assets/fig-theory-12-conformal-prediction.jpg)
```

---

### fig-theory-13: Reliability Diagram

**Alt text:** Theory visualization: reliability diagram for evaluating music attribution confidence scoring calibration -- predicted confidence on x-axis versus observed frequency on y-axis with perfect calibration diagonal, overconfident curve below the diagonal, and underconfident region above -- plus panels defining calibration (ECE metric) and sharpness for transparent confidence in music metadata systems.

**Caption:** *Figure 13. Reliability diagram for confidence calibration: when the attribution system says "85% confident," it should be correct 85% of the time. Deviations below the diagonal indicate overconfidence, while deviations above indicate underconfidence -- both calibration and sharpness must be evaluated for trustworthy music attribution.*

**Embed:**
```markdown
![Theory visualization: reliability diagram for evaluating music attribution confidence scoring calibration -- predicted confidence on x-axis versus observed frequency on y-axis with perfect calibration diagonal, overconfident curve below the diagonal, and underconfident region above -- plus panels defining calibration (ECE metric) and sharpness for transparent confidence in music metadata systems.](docs/figures/repo-figures/assets/fig-theory-13-reliability-diagram.jpg)
```

---

### fig-theory-14: Source Agreement Scoring

**Alt text:** Scoring diagram: Venn-style overlap of five music metadata sources (MusicBrainz, Discogs, AcoustID, file metadata, artist input) with center labeled high confidence, plus bar chart showing transparent confidence scoring rising from 0.35 with one source to 0.94 with four sources plus artist verification -- demonstrating how source agreement drives Bayesian confidence in the music attribution scaffold.

**Caption:** *Figure 14. Source agreement scoring: confidence increases with independent corroboration across weighted data sources. A single file tag yields 0.35, while four sources plus artist verification reaches 0.94, with artist input carrying the highest authority weight in the attribution scaffold.*

**Embed:**
```markdown
![Scoring diagram: Venn-style overlap of five music metadata sources (MusicBrainz, Discogs, AcoustID, file metadata, artist input) with center labeled high confidence, plus bar chart showing transparent confidence scoring rising from 0.35 with one source to 0.94 with four sources plus artist verification -- demonstrating how source agreement drives Bayesian confidence in the music attribution scaffold.](docs/figures/repo-figures/assets/fig-theory-14-source-agreement-scoring.jpg)
```

---

### fig-theory-15: Entity Resolution -- ELI5 (Name Tag Analogy)

**Alt text:** Concept diagram: three conference name tags showing name variants I. Heap, Imogen Heap, and HEAP IMOGEN from different music metadata sources converging to a single unified person record with ISNI identifier -- illustrating entity resolution for music attribution where the same artist appears differently across MusicBrainz, Discogs, and file tags, ensuring correct music credits and payment.

**Caption:** *Figure 15. Entity resolution explained through a conference name tag analogy: the same artist appears as "I. Heap," "Imogen Heap," and "HEAP, IMOGEN" across different databases, and entity resolution connects these variants so the right person gets credited and paid.*

**Embed:**
```markdown
![Concept diagram: three conference name tags showing name variants I. Heap, Imogen Heap, and HEAP IMOGEN from different music metadata sources converging to a single unified person record with ISNI identifier -- illustrating entity resolution for music attribution where the same artist appears differently across MusicBrainz, Discogs, and file tags, ensuring correct music credits and payment.](docs/figures/repo-figures/assets/fig-theory-15-entity-resolution-eli5.jpg)
```

---

### fig-theory-16: Resolution Cascade

**Alt text:** Waterfall diagram: five-step entity resolution cascade for music attribution -- identifier match at near zero cost, string similarity via Jaro-Winkler, embedding match via cosine similarity, LLM judgment with structured output, and Splink probabilistic linking via Fellegi-Sunter model -- each step fires only when the previous is inconclusive, optimizing transparent confidence scoring by trying the cheapest method first.

**Caption:** *Figure 16. The resolution cascade: entity resolution proceeds from cheapest to most expensive -- exact identifier match (ISRC/ISWC), string similarity, embedding cosine distance, LLM contextual reasoning, and Splink probabilistic linking -- with early exit on match to minimize cost while maintaining confidence.*

**Embed:**
```markdown
![Waterfall diagram: five-step entity resolution cascade for music attribution -- identifier match at near zero cost, string similarity via Jaro-Winkler, embedding match via cosine similarity, LLM judgment with structured output, and Splink probabilistic linking via Fellegi-Sunter model -- each step fires only when the previous is inconclusive, optimizing transparent confidence scoring by trying the cheapest method first.](docs/figures/repo-figures/assets/fig-theory-16-resolution-cascade.jpg)
```

---

### fig-theory-17: Embedding Space Visualization

**Alt text:** Theory visualization: 2D t-SNE/UMAP projection of entity embeddings for music attribution showing three tight clusters for Imogen Heap, Brian Eno, and Bjork name variants -- within-cluster cosine distance below 0.1, between-cluster distance above 0.7, and an ambiguous zone that escalates to LLM or Splink -- demonstrating how vector space captures semantic similarity for entity resolution in music metadata beyond string matching.

**Caption:** *Figure 17. Embedding space visualization: name variants for the same artist cluster tightly in vector space (cosine distance < 0.1) even when string forms differ, while distinct entities remain well-separated (distance > 0.7), with an ambiguous zone (0.1-0.7) that escalates to more expensive resolution methods.*

**Embed:**
```markdown
![Theory visualization: 2D t-SNE/UMAP projection of entity embeddings for music attribution showing three tight clusters for Imogen Heap, Brian Eno, and Bjork name variants -- within-cluster cosine distance below 0.1, between-cluster distance above 0.7, and an ambiguous zone that escalates to LLM or Splink -- demonstrating how vector space captures semantic similarity for entity resolution in music metadata beyond string matching.](docs/figures/repo-figures/assets/fig-theory-17-embedding-space.jpg)
```

---

### fig-theory-18: Graph-Based Resolution

**Alt text:** Theory visualization: graph-based entity resolution for music attribution showing mention nodes for Imogen Heap and Brian Eno name variants connected by weighted edges from cascade scoring -- community detection identifies two clusters each resolving to a single ISNI identifier, with strong within-cluster edges above 0.85 and weak cross-community edges at 0.08 confirming distinct entities in the music metadata graph.

**Caption:** *Figure 18. Graph-based entity resolution: entity mentions form a weighted graph where edge weights come from the resolution cascade (string similarity, embeddings, LLM judgment), and community detection algorithms identify clusters of mentions that resolve to the same real-world artist with an ISNI identifier.*

**Embed:**
```markdown
![Theory visualization: graph-based entity resolution for music attribution showing mention nodes for Imogen Heap and Brian Eno name variants connected by weighted edges from cascade scoring -- community detection identifies two clusters each resolving to a single ISNI identifier, with strong within-cluster edges above 0.85 and weak cross-community edges at 0.08 confirming distinct entities in the music metadata graph.](docs/figures/repo-figures/assets/fig-theory-18-graph-resolution.jpg)
```

---

### fig-theory-19: MCP Consent -- ELI5 (Library Card Analogy)

**Alt text:** Concept diagram: library card analogy for machine-readable consent in music attribution -- comparing library permissions (borrow yes, photocopy no) with MCP permissions for Imogen Heap's music (streaming yes, AI voice cloning no, remix ask first) -- plus flow showing AI system querying a permission server that returns allow, deny, or ask, enabling transparent confidence in how music credits and rights are managed by the open-source scaffold.

**Caption:** *Figure 19. Machine-readable consent explained through a library card analogy: just as a library card specifies what you can and cannot do with borrowed books, MCP permissions let artists set specific rules for how AI systems can use their music -- set once, checked automatically every time.*

**Embed:**
```markdown
![Concept diagram: library card analogy for machine-readable consent in music attribution -- comparing library permissions (borrow yes, photocopy no) with MCP permissions for Imogen Heap's music (streaming yes, AI voice cloning no, remix ask first) -- plus flow showing AI system querying a permission server that returns allow, deny, or ask, enabling transparent confidence in how music credits and rights are managed by the open-source scaffold.](docs/figures/repo-figures/assets/fig-theory-19-mcp-consent-eli5.jpg)
```

---

### fig-theory-20: MCP Permission Flow

**Alt text:** Sequence diagram: MCP permission flow for music attribution showing an AI agent sending a structured check_permission query with ISRC work identifier and use type, the MCP server looking up permissions in PostgreSQL, and returning a DENY response with machine-readable reason and alternative allowed uses -- demonstrating how the open-source attribution scaffold enables transparent confidence in AI training consent for music metadata.

**Caption:** *Figure 20. The MCP permission flow: an AI agent sends a structured query specifying work ID and intended use, the MCP server looks up the artist's permissions in PostgreSQL, and returns ALLOW, DENY, or CONDITIONS with machine-readable terms and suggested alternatives when denying.*

**Embed:**
```markdown
![Sequence diagram: MCP permission flow for music attribution showing an AI agent sending a structured check_permission query with ISRC work identifier and use type, the MCP server looking up permissions in PostgreSQL, and returning a DENY response with machine-readable reason and alternative allowed uses -- demonstrating how the open-source attribution scaffold enables transparent confidence in AI training consent for music metadata.](docs/figures/repo-figures/assets/fig-theory-20-mcp-permission-flow.jpg)
```

---

### fig-theory-21: Consent Infrastructure Architecture

**Alt text:** Architecture diagram: consent infrastructure for music attribution with MCP server (machine-readable consent for AI agents) and FastAPI REST API (human management interface) as two interfaces connecting to a shared PostgreSQL permission store with permissions and audit log tables -- enabling transparent confidence in music credits through the open-source attribution scaffold's dual-interface design for both machines and humans.

**Caption:** *Figure 21. Consent infrastructure architecture: two interfaces (MCP for machine-to-machine queries, FastAPI REST for human management) share a single PostgreSQL permission store with full audit logging, embodying the principle of "two interfaces, one truth" for durable consent management.*

**Embed:**
```markdown
![Architecture diagram: consent infrastructure for music attribution with MCP server (machine-readable consent for AI agents) and FastAPI REST API (human management interface) as two interfaces connecting to a shared PostgreSQL permission store with permissions and audit log tables -- enabling transparent confidence in music credits through the open-source attribution scaffold's dual-interface design for both machines and humans.](docs/figures/repo-figures/assets/fig-theory-21-consent-infrastructure.jpg)
```

---

### fig-theory-22: Permission Matrix

**Alt text:** Matrix diagram: permission matrix for music attribution with five Imogen Heap songs as rows and five use types as columns (streaming, sync license, download, AI training, voice cloning) -- cells colored green for allow, red for deny, amber for conditional with fee or attribution requirements -- showing how machine-readable consent enables granular per-work permission profiles with transparent confidence in the open-source music credits scaffold.

**Caption:** *Figure 22. Permission matrix across works and use cases: permissions are not binary -- each musical work has a unique profile where streaming is typically allowed, voice cloning is typically denied, and the interesting policy decisions (AI training, sync licensing) vary per work with conditional terms.*

**Embed:**
```markdown
![Matrix diagram: permission matrix for music attribution with five Imogen Heap songs as rows and five use types as columns (streaming, sync license, download, AI training, voice cloning) -- cells colored green for allow, red for deny, amber for conditional with fee or attribution requirements -- showing how machine-readable consent enables granular per-work permission profiles with transparent confidence in the open-source music credits scaffold.](docs/figures/repo-figures/assets/fig-theory-22-permission-matrix.jpg)
```

---

### fig-repo-16: Music AI Attribution Landscape Overview

**Alt text:** Ecosystem map: four-tier music AI attribution landscape showing attribution infrastructure (Sureel, Musical AI, Vermillio, ProRata, this scaffold), AI music generation (Suno, Udio), licensing and certification (Fairly Trained, STIM, SoundExchange), and open-source tools (librosa, Splink, chromaprint) with 28 PRD node annotations and three integration archetypes differentiating engagement depth.

**Caption:** *The music AI attribution ecosystem mapped across four tiers with ~$500M+ in funding across proprietary players -- this scaffold is the only open-source, auditable alternative with calibrated confidence scoring and 28 ecosystem PRD nodes mapping to industry companies.*

**Embed:**
```markdown
![Ecosystem map: four-tier music AI attribution landscape showing attribution infrastructure (Sureel, Musical AI, Vermillio, ProRata, this scaffold), AI music generation (Suno, Udio), licensing and certification (Fairly Trained, STIM, SoundExchange), and open-source tools (librosa, Splink, chromaprint) with 28 PRD node annotations and three integration archetypes differentiating engagement depth.](docs/figures/repo-figures/assets/fig-repo-16-landscape-overview.jpg)
```

---

### fig-repo-17: MCP Security Threat Model

**Alt text:** MCP security threat model: four attack surfaces (tool manifest injection, communication MITM, resource path traversal, execution environment escalation) mapped to four defense layers (OAuth 2.0 authentication, three-stage input validation, capability sandbox with expiring grants, immutable audit trail for EU AI Act Art. 12 compliance) -- based on MCPSecBench finding 40.71% average attack success rate across MCP implementations.

**Caption:** *Figure 17. MCP security threat model mapping four attack surfaces to four defense-in-depth layers, based on MCPSecBench 2025 benchmarks showing 40.71% average attack success rate across implementations.*

**Embed:**
```markdown
![MCP security threat model: four attack surfaces mapped to four defense layers with 40.71% average attack success rate benchmark.](docs/figures/repo-figures/assets/fig-repo-17-mcp-security-threat-model.jpg)
```

---

### fig-repo-18: EU AI Act Compliance Timeline

**Alt text:** EU AI Act compliance timeline for music attribution: vertical timeline from August 2024 entry into force through February 2025 prohibited practices, August 2025 GPAI training data transparency obligations (currently active), August 2026 high-risk AI system conformity assessments, to August 2027 full enforcement with penalties up to EUR 35M or 7% of global turnover -- with the scaffold's A0-A3 provenance tracking, conformal prediction bounds, and audit infrastructure mapped to each compliance phase.

**Caption:** *Figure 18. EU AI Act compliance timeline mapping regulatory obligations to scaffold provenance capabilities, with GPAI training data transparency obligations active since August 2025.*

**Embed:**
```markdown
![EU AI Act compliance timeline for music attribution showing regulatory milestones and scaffold compliance capabilities.](docs/figures/repo-figures/assets/fig-repo-18-eu-ai-act-timeline.jpg)
```

---

### fig-repo-19: Agentic Commerce Ecosystem

**Alt text:** Agentic commerce ecosystem: three-tier protocol landscape showing MCP (Model Context Protocol, Linux Foundation) as foundation layer, four commerce protocols as transaction layer (ACP from OpenAI and Stripe for payment orchestration, AP2 from Google for enterprise shopping, TAP from Visa for agent identity verification, A2A from Linux Foundation for peer-to-peer agent coordination), and the music attribution MCP server feeding verified attribution data and permission checks into the commerce protocol stack -- projected market of $1-3.5T in agent-mediated transactions by 2030.

**Caption:** *Figure 19. The music attribution scaffold's position in the agentic commerce protocol landscape, feeding verified attribution data into commerce protocols handling projected $1-3.5T in agent-mediated transactions by 2030.*

**Embed:**
```markdown
![Agentic commerce ecosystem showing the scaffold's MCP server feeding attribution data into commerce protocols.](docs/figures/repo-figures/assets/fig-repo-19-agentic-commerce-ecosystem.jpg)
```

---

### fig-repo-20: MCP Attack Surface Taxonomy

**Alt text:** MCP attack surface taxonomy showing six attack vectors with measured success rates: tool poisoning at 72.8 percent from MCPTox, cross-server contamination at 85-plus percent from MCPSecBench, supply chain compromise at 16 to 65 percent from Guo et al., prompt injection at 40.71 percent average from MSB across 984 test cases, rug pull attacks at variable rates, and path traversal affecting 22 percent of servers -- six distinct vectors requiring defense-in-depth for any production MCP deployment.

**Caption:** *Figure 20. Six MCP attack vectors with measured attack success rates from four independent security benchmarks, demonstrating the breadth of the attack surface facing production MCP servers.*

**Embed:**
```markdown
![MCP attack surface taxonomy showing six attack vectors with measured success rates from four security benchmarks.](docs/figures/repo-figures/assets/fig-repo-20-mcp-attack-taxonomy.jpg)
```

---

### fig-repo-21: Inverse Scaling Paradox

**Alt text:** Conceptual scatter plot showing the inverse scaling paradox from MCPSecBench where attack success rate increases with model capability, demonstrating that more capable AI models like GPT-4o and Claude 3.5 Sonnet are more vulnerable to MCP-based attacks due to stronger instruction-following capabilities making them more susceptible to adversarial instructions embedded in tool descriptions -- a counterintuitive finding requiring infrastructure-level defense.

**Caption:** *Figure 21. The inverse scaling paradox: more capable models are more vulnerable to MCP attacks, per MCPSecBench (Cheng et al. 2025), necessitating infrastructure-level rather than model-level defense.*

**Embed:**
```markdown
![Inverse scaling paradox showing more capable AI models are more vulnerable to MCP attacks.](docs/figures/repo-figures/assets/fig-repo-21-inverse-scaling-paradox.jpg)
```

---

### fig-repo-22: MCP-Guard Three-Stage Defense

**Alt text:** MCP-Guard three-stage defense pipeline: Stage 1 static scanner under 2 milliseconds catching pattern-based attacks, Stage 2 neural classifier at 96.01 percent accuracy under 50 milliseconds handling novel patterns with 3.2 percent false positive rate, Stage 3 LLM arbiter at 89.07 percent F1 score for the 5 percent of ambiguous requests reaching it -- cascading architecture achieves 12x speedup over LLM-only approaches while maintaining high detection accuracy for music attribution MCP security.

**Caption:** *Figure 22. MCP-Guard's three-stage cascading defense pipeline (Shan et al. 2025), handling 95% of requests in under 50ms while achieving 96.01% detection accuracy.*

**Embed:**
```markdown
![MCP-Guard three-stage defense pipeline with static scanner, neural classifier, and LLM arbiter.](docs/figures/repo-figures/assets/fig-repo-22-mcp-guard-three-stage.jpg)
```

---

### fig-repo-23: MCP Authentication Crisis

**Alt text:** Horizontal bar chart showing MCP authentication distribution across 500-plus deployed servers from Guo et al. 2025: 32 percent with no authentication marked critical risk, 53 percent using insecure static API keys, 8.5 percent implementing recommended OAuth 2.0 or 2.1, 4 percent using mTLS certificate-based authentication -- highlighting that 85 percent of deployed MCP servers fail basic security standards despite OAuth 2.1 being mandated in the March 2025 specification.

**Caption:** *Figure 23. Authentication crisis in deployed MCP servers: 85% use inadequate methods despite the March 2025 spec mandating OAuth 2.1 (Guo et al. 2025).*

**Embed:**
```markdown
![MCP authentication crisis: 85 percent of 500-plus servers use insecure authentication.](docs/figures/repo-figures/assets/fig-repo-23-mcp-auth-crisis.jpg)
```

---

### fig-repo-24: Agentic Protocol Landscape

**Alt text:** Agentic protocol landscape showing three convergent layers: XAA identity layer from Okta for agent authentication and delegation chains, A2A coordination layer from Google via Linux Foundation for agent discovery and task lifecycle, MCP tool layer from Anthropic via AAIF for model-to-tool communication and resource access -- plus two vertical niches: ACP from OpenAI and Stripe for commerce and payment orchestration, and TAP from Visa for financial agent transactions.

**Caption:** *Figure 24. Five agentic protocols converging into a three-layer stack (tool + coordination + identity) with vertical commerce niches, as of February 2026.*

**Embed:**
```markdown
![Agentic protocol landscape: MCP, ACP, A2A, TAP, XAA across three convergent layers.](docs/figures/repo-figures/assets/fig-repo-24-protocol-landscape.jpg)
```

---

### fig-repo-25: MCP Protocol Evolution Timeline

**Alt text:** Timeline showing MCP protocol evolution from November 2024 initial release with JSON-RPC 2.0, through March 2025 OAuth 2.1 mandate, June 2025 streamable HTTP transport and elicitation requests, September 2025 MCP Registry preview for server discovery, November 2025 OAuth Resource Server classification with RFC 8707 resource indicators and SEP-1024 client security, to December 2025 AAIF donation under Linux Foundation with AWS, Anthropic, Google, Microsoft, and OpenAI as platinum members -- 13 months from single-company to multi-stakeholder governance.

**Caption:** *Figure 25. MCP protocol evolution: from Anthropic's internal tool standard to Linux Foundation-governed open protocol in 13 months, with security hardening lagging adoption.*

**Embed:**
```markdown
![MCP protocol evolution timeline from November 2024 to AAIF governance December 2025.](docs/figures/repo-figures/assets/fig-repo-25-mcp-evolution-timeline.jpg)
```

---

### fig-repo-26: Zero-Trust Architecture for MCP

**Alt text:** Split-panel comparison of current trust-on-first-use MCP model where clients trust self-declared server identity enabling typosquatting and supply chain attacks, versus target zero-trust architecture with Decentralized Identifiers and Verifiable Credentials for cryptographic server verification, Agent Name Service for DNS-like discovery, and MCP Server Cards for out-of-band security attestation -- noting that fewer than 5 percent of servers currently verify identity cryptographically per Guo et al. 2025.

**Caption:** *Figure 26. Zero-trust architecture for MCP: current trust-on-first-use model vs. target DID/VC model with Agent Name Service, based on CSA/MIT framework.*

**Embed:**
```markdown
![Zero-trust MCP architecture: trust-on-first-use vs DID/VC cryptographic verification.](docs/figures/repo-figures/assets/fig-repo-26-zero-trust-mcp.jpg)
```

---

### fig-repo-27: Real-World MCP CVE Gallery

**Alt text:** Gallery of three real-world MCP CVEs from 2025: CVE-2025-53967 affecting Figma MCP Server with SSRF vulnerability at CVSS 7.5 caused by insufficient URL input validation enabling internal network scanning, CVE-2025-6514 affecting mcp-remote package with 558000-plus npm downloads enabling remote code execution through missing server identity verification, and CVE-2025-68143 through 68145 affecting Anthropic Git MCP with command injection via unsanitized shell command construction from user-provided repository and branch names.

**Caption:** *Figure 27. Three production MCP CVEs from 2025 demonstrating that MCP security vulnerabilities are not theoretical — affecting major platforms and hundreds of thousands of users.*

**Embed:**
```markdown
![Real-world MCP CVE gallery: Figma SSRF, mcp-remote RCE, Anthropic Git command injection.](docs/figures/repo-figures/assets/fig-repo-27-mcp-cve-gallery.jpg)
```

---

### fig-repo-28: MCP Sandbox Isolation Patterns

**Alt text:** Three-column comparison of MCP sandbox isolation patterns: Cloudflare Workers with V8 isolates providing sub-millisecond cold start and automatic sandboxing but limited to JavaScript and TypeScript with 10ms CPU limit, Docker container isolation with any language support and configurable resource limits via cgroups v2 but seconds cold start latency, and Deno sandbox with granular permission controls and sub-millisecond startup but limited to TypeScript -- Docker recommended for the music attribution scaffold matching existing infrastructure.

**Caption:** *Figure 28. Three MCP sandbox isolation patterns compared: Cloudflare Workers (edge speed), Docker (full flexibility), and Deno (granular permissions), with Docker recommended for the scaffold.*

**Embed:**
```markdown
![MCP sandbox isolation: Cloudflare Workers vs Docker vs Deno compared.](docs/figures/repo-figures/assets/fig-repo-28-sandbox-isolation.jpg)
```

---

### fig-repo-29: OWASP Agentic Top 10 (2026)

**Alt text:** OWASP Agentic AI Top 10 2026 checklist for MCP servers showing seven of ten risks addressed by the scaffold four-layer defense architecture: prompt injection, broken access control, tool and function misuse, excessive agency, insecure tool configuration, data poisoning through tools, and insufficient logging -- with three risks requiring additional investment: supply chain vulnerabilities, agent identity spoofing, and memory and context manipulation.

**Caption:** *Figure 29. OWASP Agentic AI Top 10 (2026) mapped against the scaffold's four-layer defense — 7/10 risks addressed, 3/10 requiring zero-trust infrastructure investment.*

**Embed:**
```markdown
![OWASP Agentic AI Top 10 mapped to scaffold defenses: 7 addressed, 3 need investment.](docs/figures/repo-figures/assets/fig-repo-29-owasp-agentic-top10.jpg)
```

---

### fig-repo-30: MCP Production Observability Stack

**Alt text:** Split-panel MCP production observability architecture: data plane capturing structured logs with client identity and tool invocation details, OpenTelemetry traces with MCP semantic conventions for request waterfalls, and security metrics tracking blocked requests and authentication failures -- feeding into analysis plane with EU AI Act Article 12 compliance logging and retention, debugging tools with latency breakdown and dependency graphs, and threat detection with anomaly alerts and incident response for transparent confidence in production MCP deployments.

**Caption:** *Figure 30. Three-layer production observability for MCP servers: structured logs for EU AI Act compliance, OTel traces for debugging, and security metrics for threat detection.*

**Embed:**
```markdown
![MCP production observability: logs, traces, and security metrics across data and analysis planes.](docs/figures/repo-figures/assets/fig-repo-30-mcp-observability.jpg)
```

---

### fig-repo-31: AAIF Governance Structure

**Alt text:** AAIF governance structure showing Linux Foundation umbrella over Agentic AI Foundation formed December 2025, with five platinum members AWS, Anthropic, Google, Microsoft, and OpenAI, a Technical Steering Committee with elected maintainers operating through a formal RFC process for specification changes, and governance over two protocols: MCP for model-to-tool communication and A2A for agent-to-agent coordination -- representing a transition from single-company to multi-stakeholder governance in 13 months.

**Caption:** *Figure 31. AAIF governance structure: MCP moved from single-company (Anthropic) to multi-stakeholder governance under the Linux Foundation in 13 months, with five platinum members including all major AI platform providers.*

**Embed:**
```markdown
![AAIF governance: Linux Foundation umbrella with five platinum members governing MCP and A2A.](docs/figures/repo-figures/assets/fig-repo-31-aaif-governance.jpg)
```

---

## Decision Analysis (fig-scenario-*)

### fig-scenario-01: MVP Scenario Activation Path

**Alt text:** MVP scenario: 23 of 78 nodes activated as highest-probability path through PRD network

**Caption:** *The MVP scenario traces the highest-probability path through the 78-node decision network, activating 23 core infrastructure nodes while deferring all 28 ecosystem integration nodes.*

**Embed:**
```markdown
![MVP scenario: 23 of 78 nodes activated as highest-probability path through PRD network](docs/figures/repo-figures/assets/fig-scenario-01-mvp-scenario-activation.jpg)
```

---

### fig-scenario-02: Four Archetype Scenario Comparison

**Alt text:** Four archetypes activating different 78-node subsets with ecosystem sensitivity

**Caption:** *Four team archetypes produce dramatically different activation patterns across the 78-node decision network, from the Solo Hacker's minimal 18-node path to the Well-Funded Startup's 50+ node enterprise configuration.*

**Embed:**
```markdown
![Four archetypes activating different 78-node subsets with ecosystem sensitivity](docs/figures/repo-figures/assets/fig-scenario-02-four-archetype-comparison.jpg)
```

---

### fig-scenario-03: Decision Cascade: build_vs_buy_posture

**Alt text:** Decision cascade from build_vs_buy_posture: highest-influence node with 27 downstream edges

**Caption:** *build_vs_buy_posture cascades to 27 downstream nodes across all 5 levels of the decision network, making it the single most consequential architectural choice in the scaffold.*

**Embed:**
```markdown
![Decision cascade from build_vs_buy_posture: highest-influence node with 27 downstream edges](docs/figures/repo-figures/assets/fig-scenario-03-decision-cascade-build-vs-buy.jpg)
```

---

### fig-scenario-04: Volatility Heatmap: 78 Nodes

**Alt text:** Volatility heatmap: 78 PRD nodes showing core stable vs ecosystem volatile bifurcation

**Caption:** *The 78-node volatility heatmap reveals a clear bifurcation: core infrastructure nodes (50) cluster in the Stable/Shifting range, while ecosystem integration nodes (28) are predominantly Volatile -- encoding honest uncertainty about partnership and integration decisions.*

**Embed:**
```markdown
![Volatility heatmap: 78 PRD nodes showing core stable vs ecosystem volatile bifurcation](docs/figures/repo-figures/assets/fig-scenario-04-volatility-heatmap.jpg)
```

---

### fig-scenario-05: Network Growth: Nodes and Edges Over Time

**Alt text:** Network growth: 15 to 78 nodes across 10 versions maintaining disciplined edge-to-node ratio

**Caption:** *The decision network grew from 15 nodes to 78 nodes across 10 versions, with the edge:node ratio evolving alongside -- indicating disciplined, incremental growth rather than unconstrained sprawl.*

**Embed:**
```markdown
![Network growth: 15 to 78 nodes across 10 versions maintaining disciplined edge-to-node ratio](docs/figures/repo-figures/assets/fig-scenario-05-network-growth-over-time.jpg)
```

---

### fig-scenario-06: Ecosystem Node Dependency Graph

**Alt text:** Ecosystem dependency graph: partnership_model gating company nodes in activation chains

**Caption:** *The 28 ecosystem nodes form internal dependency chains with partnership_model as the primary gating node, creating conditional activation cascades from L2 architecture through L3 category and company nodes to L4/L5 ecosystem operations.*

**Embed:**
```markdown
![Ecosystem dependency graph: partnership_model gating company nodes in activation chains](docs/figures/repo-figures/assets/fig-scenario-06-ecosystem-dependency-graph.jpg)
```

---

### fig-scenario-07: L3 Component Node Clustering

**Alt text:** L3 component clustering: 24 nodes in TDA/ID, licensing, platform, infra, company groups

**Caption:** *The 24 L3 component nodes cluster into 5 functional groups -- TDA/Identification, Licensing, Platform/Commerce, Infrastructure, and Company-Specific -- separating capability decisions from concrete partnership commitments.*

**Embed:**
```markdown
![L3 component clustering: 24 nodes in TDA/ID, licensing, platform, infra, company groups](docs/figures/repo-figures/assets/fig-scenario-07-l3-component-clustering.jpg)
```

---

### fig-scenario-08: Strategic Ambiguity Encoding

**Alt text:** Strategic ambiguity: core nodes committed vs ecosystem nodes preserving 0.40-0.55 optionality

**Caption:** *Core infrastructure nodes have committed to specific options with "none" priors of 0.05-0.20, while ecosystem nodes deliberately preserve 0.40-0.55 "none" priors -- encoding strategic ambiguity as a first-class feature of the probabilistic PRD.*

**Embed:**
```markdown
![Strategic ambiguity: core nodes committed vs ecosystem nodes preserving 0.40-0.55 optionality](docs/figures/repo-figures/assets/fig-scenario-08-strategic-ambiguity-encoding.jpg)
```

---

## Ecosystem Integration (fig-ecosystem-*)

### fig-ecosystem-01: Ecosystem Integration Subgraph Overview

**Alt text:** Subgraph overview: 28 ecosystem integration nodes forming coherent cluster within 78-node PRD network

**Caption:** *The 28 ecosystem integration nodes form a coherent subgraph within the v3.0.0 decision network, spanning L2 architecture through L5 operations, connected to the core infrastructure by approximately 63 new edges.*

**Embed:**
```markdown
![Subgraph overview: 28 ecosystem integration nodes forming coherent cluster within 78-node PRD network](docs/figures/repo-figures/assets/fig-ecosystem-01-subgraph-overview.jpg)
```

---

### fig-ecosystem-02: Three Integration Archetypes

**Alt text:** Three integration archetypes from lightweight MCP adapters to institutional CMO federation

**Caption:** *Three integration archetypes determine engineering investment: Simple MCP adapters require weeks, Platform Integration requires months of bidirectional data exchange work, and CMO Federation demands quarters of institutional infrastructure development.*

**Embed:**
```markdown
![Three integration archetypes from lightweight MCP adapters to institutional CMO federation](docs/figures/repo-figures/assets/fig-ecosystem-02-three-integration-archetypes.jpg)
```

---

### fig-ecosystem-03: Partnership Model Decision Tree

**Alt text:** Partnership model decision tree cascading to six company node activations

**Caption:** *The partnership model decision tree shows how each strategy option cascades into different company node activations, with the "none" option holding the highest prior probability (0.40) reflecting honest uncertainty about partnership outcomes.*

**Embed:**
```markdown
![Partnership model decision tree cascading to six company node activations](docs/figures/repo-figures/assets/fig-ecosystem-03-partnership-model-decision-tree.jpg)
```

---

### fig-ecosystem-04: TDA Provider Landscape

**Alt text:** TDA provider landscape: training-time vs post-hoc attribution approaches compared

**Caption:** *Training-time attribution (Musical AI) provides highest accuracy by tracing data influence during model training, while post-hoc approaches (Sureel fingerprinting, embedding similarity) trade accuracy for broader applicability to any model output.*

**Embed:**
```markdown
![TDA provider landscape: training-time vs post-hoc attribution approaches compared](docs/figures/repo-figures/assets/fig-ecosystem-04-tda-provider-landscape.jpg)
```

---

### fig-ecosystem-05: CMO Licensing Integration Architecture

**Alt text:** CMO licensing architecture with scaffold providing attribution confidence for royalties

**Caption:** *The CMO licensing integration architecture positions the scaffold as the attribution confidence layer between collective management organizations and AI music platforms, enabling per-output royalty calculation based on confidence-scored creator identification.*

**Embed:**
```markdown
![CMO licensing architecture with scaffold providing attribution confidence for royalties](docs/figures/repo-figures/assets/fig-ecosystem-05-cmo-licensing-architecture.jpg)
```

---

### fig-ecosystem-06: Content ID System Comparison

**Alt text:** Content ID comparison: fingerprinting vs watermark detection vs embedding similarity

**Caption:** *Three content identification approaches address different aspects of the Oracle Problem: fingerprinting verifies exact copies post-hoc, watermark detection requires attribution-by-design at creation, and embedding similarity handles transformations with approximate matching.*

**Embed:**
```markdown
![Content ID comparison: fingerprinting vs watermark detection vs embedding similarity](docs/figures/repo-figures/assets/fig-ecosystem-06-content-id-comparison.jpg)
```

---

### fig-ecosystem-07: AI Music Platform Connector Design

**Alt text:** Platform connector design normalizing AI music platform metadata to NormalizedRecord

**Caption:** *Platform connectors ingest proprietary metadata from AI music platforms (Suno, Udio), map fields to the NormalizedRecord Pydantic schema, apply platform-specific confidence adjustments, and feed normalized records into the ETL pipeline.*

**Embed:**
```markdown
![Platform connector design normalizing AI music platform metadata to NormalizedRecord](docs/figures/repo-figures/assets/fig-ecosystem-07-platform-connector-design.jpg)
```

---

### fig-ecosystem-08: Metadata Registry Integration Map

**Alt text:** Metadata registry map: MusicBrainz, Discogs, SoundExchange, DDEX unified by resolution

**Caption:** *Four metadata registries provide complementary identifiers that the entity resolution layer unifies: MusicBrainz (MBID) and Discogs (release IDs) are currently integrated, while SoundExchange (126M+ ISRCs) and DDEX (industry messaging) are expansion nodes awaiting integration.*

**Embed:**
```markdown
![Metadata registry map: MusicBrainz, Discogs, SoundExchange, DDEX unified by resolution](docs/figures/repo-figures/assets/fig-ecosystem-08-metadata-registry-map.jpg)
```

---

### fig-ecosystem-09: Agent Interoperability Protocol Stack

**Alt text:** Protocol stack: MCP tool access to A2A coordination to agentic commerce layers

**Caption:** *Progressive protocol adoption for agent interoperability -- MCP tool access (current MVP) provides the foundation for A2A coordination (future scaling) and ACP agentic commerce (monetization), with each layer building on the previous and a feedback arc from commerce back to tool design.*

**Embed:**
```markdown
![Protocol stack: MCP tool access to A2A coordination to agentic commerce layers](docs/figures/repo-figures/assets/fig-ecosystem-09-agent-interop-protocol-stack.jpg)
```

---

### fig-ecosystem-10: Knowledge Graph Backend Options

**Alt text:** Knowledge graph backends: Apache AGE co-located vs Neo4j Aura vs LightRAG hybrid

**Caption:** *Three knowledge graph backend options for the attribution scaffold -- Apache AGE co-located in PostgreSQL (P=0.45), Neo4j Aura managed native graph (P=0.30), and LightRAG hybrid graph-RAG overlay (P=0.25) -- trading off co-location simplicity against feature richness.*

**Embed:**
```markdown
![Knowledge graph backends: Apache AGE co-located vs Neo4j Aura vs LightRAG hybrid](docs/figures/repo-figures/assets/fig-ecosystem-10-knowledge-graph-backend-options.jpg)
```

---

### fig-ecosystem-11: Fairly Trained Certification Flow

**Alt text:** Fairly Trained certification: binary audit signal complementing A0-A3 assurance levels

**Caption:** *Fairly Trained certification provides a binary certified/not-certified market signal via independent third-party training data audit, complementing the scaffold's graduated A0-A3 assurance levels with an external trust anchor. Founded by Ed Newton-Rex, 19 entities certified to date.*

**Embed:**
```markdown
![Fairly Trained certification: binary audit signal complementing A0-A3 assurance levels](docs/figures/repo-figures/assets/fig-ecosystem-11-fairly-trained-certification-flow.jpg)
```

---

### fig-ecosystem-12: Compliance Reporting Pipeline

**Alt text:** Compliance pipeline: audit logs and confidence scores into EU AI Act reports

**Caption:** *The compliance reporting pipeline aggregates MCP audit logs, attribution confidence scores, and permission decision records into three EU AI Act-compliant output reports -- Art. 12 logging, Art. 52 transparency, and GPAI Code of Practice technical documentation -- with penalties up to EUR 35M or 7% global turnover for non-compliance.*

**Embed:**
```markdown
![Compliance pipeline: audit logs and confidence scores into EU AI Act reports](docs/figures/repo-figures/assets/fig-ecosystem-12-compliance-reporting-pipeline.jpg)
```

---

### fig-ecosystem-13: Edge Inference Strategy Decision

**Alt text:** Edge inference decision: server-only MVP default vs edge deployment for latency

**Caption:** *Edge inference strategy decision -- server-only (P=0.50) is the MVP default with ~200ms fingerprint comparison latency and simple operations; edge deployment reduces latency to ~20ms but adds operational complexity across Cloudflare Workers AI, Deno Deploy, or Supabase Edge Functions.*

**Embed:**
```markdown
![Edge inference decision: server-only MVP default vs edge deployment for latency](docs/figures/repo-figures/assets/fig-ecosystem-13-edge-inference-strategy.jpg)
```

---

### fig-ecosystem-14: Attribution Evaluation Framework Maturity

**Alt text:** Eval framework maturity: manual spot-check to automated regression to CI golden datasets

**Caption:** *Attribution evaluation matures through three stages -- manual spot-checks (P=0.35, current), Promptfoo automated regression (P=0.45, recommended), and CI-integrated golden dataset evaluation with drift detection (P=0.20, target) -- with each stage building systematic coverage on top of the previous.*

**Embed:**
```markdown
![Eval framework maturity: manual spot-check to automated regression to CI golden datasets](docs/figures/repo-figures/assets/fig-ecosystem-14-attribution-eval-framework-maturity.jpg)
```

---

### fig-ecosystem-15: Company Node Activation Matrix

**Alt text:** Company node activation matrix: partnership model determines which 6 companies engage

**Caption:** *Company node activation matrix showing how 6 ecosystem company nodes activate conditionally based on partnership_model selection -- "none" (P=0.40) as default leaves all deactivated, while api_marketplace, strategic_alliance, and cmo_federation each activate 2-4 companies.*

**Embed:**
```markdown
![Company node activation matrix: partnership model determines which 6 companies engage](docs/figures/repo-figures/assets/fig-ecosystem-15-company-node-activation-matrix.jpg)
```

---

### fig-ecosystem-16: Ecosystem Node "None" Prior Heatmap

**Alt text:** None prior heatmap: ecosystem nodes show 0.40-0.55 strategic ambiguity encoding

**Caption:** *Heatmap of "none" prior probabilities across 28 ecosystem nodes (0.40-0.55 range) contrasted against core infrastructure nodes (0.15-0.35 range), encoding strategic ambiguity by design -- the scaffold captures the possibility space without committing to partnerships that haven't materialized.*

**Embed:**
```markdown
![None prior heatmap: ecosystem nodes show 0.40-0.55 strategic ambiguity encoding](docs/figures/repo-figures/assets/fig-ecosystem-16-none-prior-heatmap.jpg)
```

---

## Tech Trends 2026 (fig-trends-*)

### fig-trends-01: Agent Framework Consolidation 2025-2026

**Alt text:** Agent framework consolidation: PydanticAI, LangGraph, and CrewAI/AG2 in three tiers

**Caption:** *Agent framework consolidation into three tiers: PydanticAI for type-safe production (scaffold's choice), LangGraph for stateful orchestration, and CrewAI/AG2 for multi-agent coordination.*

**Embed:**
```markdown
![Agent framework consolidation: PydanticAI, LangGraph, and CrewAI/AG2 in three tiers](docs/figures/repo-figures/assets/fig-trends-01-agent-framework-consolidation.jpg)
```

---

### fig-trends-02: pgvector Performance Evolution

**Alt text:** pgvector evolution: 471 QPS at 99% recall validates PostgreSQL Unified architecture

**Caption:** *pgvector reached 471 QPS at 99% recall, validating the PostgreSQL Unified architecture where vector search, graph queries, and relational data share a single database process.*

**Embed:**
```markdown
![pgvector evolution: 471 QPS at 99% recall validates PostgreSQL Unified architecture](docs/figures/repo-figures/assets/fig-trends-02-pgvector-performance-evolution.jpg)
```

---

### fig-trends-03: Agent Observability: OTel GenAI Stack

**Alt text:** OTel GenAI stack: PydanticAI to Logfire to any OpenTelemetry-compatible backend

**Caption:** *PydanticAI agents emit OpenTelemetry spans via Logfire, flowing through standard OTLP pipelines to any compatible backend -- Grafana, Datadog, Honeycomb, or self-hosted Jaeger.*

**Embed:**
```markdown
![OTel GenAI stack: PydanticAI to Logfire to any OpenTelemetry-compatible backend](docs/figures/repo-figures/assets/fig-trends-03-otel-genai-stack.jpg)
```

---

### fig-trends-04: Evaluation Framework Maturity Spectrum

**Alt text:** Eval maturity spectrum: PydanticAI mocks to Promptfoo CI to Braintrust dataset eval

**Caption:** *Three-tier evaluation pyramid for agent quality: PydanticAI mocks (unit), Promptfoo (CI regression), Braintrust (dataset-driven). The scaffold currently operates at the base tier.*

**Embed:**
```markdown
![Eval maturity spectrum: PydanticAI mocks to Promptfoo CI to Braintrust dataset eval](docs/figures/repo-figures/assets/fig-trends-04-eval-framework-maturity-spectrum.jpg)
```

---

### fig-trends-05: Edge AI Platforms for Attribution

**Alt text:** Edge AI platforms: Cloudflare Workers AI, Deno Deploy, and Supabase Edge compared

**Caption:** *Three edge AI platforms compared for attribution workloads: Cloudflare Workers AI (inference), Deno Deploy (caching), Supabase Edge (database queries). All are future options -- current PRD prior favors server-only.*

**Embed:**
```markdown
![Edge AI platforms: Cloudflare Workers AI, Deno Deploy, and Supabase Edge compared](docs/figures/repo-figures/assets/fig-trends-05-edge-ai-platforms.jpg)
```

---

### fig-trends-06: Graph Knowledge Base Options

**Alt text:** Graph knowledge bases: Neo4j Aura Agent vs Apache AGE with LightRAG on PostgreSQL

**Caption:** *Two graph knowledge base approaches: Neo4j Aura Agent (managed, separate service) vs Apache AGE + LightRAG (PostgreSQL co-located, zero network hop). PRD favors AGE co-located (P=0.45).*

**Embed:**
```markdown
![Graph knowledge bases: Neo4j Aura Agent vs Apache AGE with LightRAG on PostgreSQL](docs/figures/repo-figures/assets/fig-trends-06-graph-knowledge-base-options.jpg)
```

---

### fig-trends-07: Music Rights Revenue Flow: STIM Model

**Alt text:** STIM revenue flow: AI output to confidence scoring to per-output royalty to rights holders

**Caption:** *STIM revenue flow: when AI generates music, the attribution scaffold scores training influence, STIM calculates per-output royalties, and rights holders receive payment under the world's first collective AI music licence.*

**Embed:**
```markdown
![STIM revenue flow: AI output to confidence scoring to per-output royalty to rights holders](docs/figures/repo-figures/assets/fig-trends-07-stim-revenue-flow.jpg)
```

---

### fig-trends-08: Scaffold-to-Trend Implication Matrix

**Alt text:** Trend-to-scaffold matrix: 10 tech trends mapped to PRD node impacts and reclassifications

**Caption:** *Scaffold-to-trend implication matrix: 10 technology trends mapped to PRD node impacts with 5 volatility reclassifications -- 2 stabilizing, 3 remaining volatile or shifting.*

**Embed:**
```markdown
![Trend-to-scaffold matrix: 10 tech trends mapped to PRD node impacts and reclassifications](docs/figures/repo-figures/assets/fig-trends-08-scaffold-to-trend-implication-matrix.jpg)
```

---

## Landscape Analysis (fig-landscape-*)

### fig-landscape-01: Music AI Problem Taxonomy: 12 Categories

**Alt text:** Taxonomy grid: 12 music AI problem categories organized into generative (content generation, voice cloning, recommendation, music understanding), analytical (audio detection, quality assessment, attribution/provenance, content ID), and infrastructure (rights management, metadata standardization, licensing, royalty distribution) with maturity levels from nascent to mature, showing that music attribution is one of 12 distinct challenges requiring different technical approaches.

**Caption:** *Music AI spans 12 distinct problem categories grouped into generative, analytical, and infrastructure domains -- each with different maturity levels and investment profiles -- establishing that music attribution with transparent confidence is one specific challenge within a broader landscape.*

**Embed:**
```markdown
![Taxonomy grid: 12 music AI problem categories organized into generative, analytical, and infrastructure domains with maturity levels from nascent to mature, showing that music attribution is one of 12 distinct challenges requiring different technical approaches.](docs/figures/repo-figures/assets/fig-landscape-01-problem-taxonomy.jpg)
```

---

### fig-landscape-02: Where the Money Went: $500M+ by Category

**Alt text:** Split-panel bar chart: $375M+ funding for AI-generated music platforms (Suno, Udio) versus less than $70M for music attribution infrastructure (ProRata, Vermillio, Musical AI, Sureel), revealing a 5:1 structural funding imbalance that defines both the risk landscape and the open-source attribution scaffold opportunity.

**Caption:** *Music AI funding reveals a 5:1 structural imbalance -- over $375M to generation platforms versus less than $70M to attribution infrastructure -- defining the market opportunity for open-source music attribution with transparent confidence scoring.*

**Embed:**
```markdown
![Split-panel bar chart: $375M+ funding for AI music generation versus less than $70M for attribution infrastructure, revealing a 5:1 structural funding imbalance.](docs/figures/repo-figures/assets/fig-landscape-02-funding-by-category.jpg)
```

---

### fig-landscape-03: Papers to Products: Academic-to-Industry Pipeline

**Alt text:** Flow diagram: three research streams -- influence functions (Koh & Liang 2017), embedding similarity (CLMR/CLAP), and attribution-by-design (Morreale et al. 2025) -- flowing from academic papers through a translation gap to commercial training data attribution products, showing most music attribution research never reaches deployment.

**Caption:** *Three research traditions flow from academia to industry through a visible translation gap: influence functions to gradient-tracking products, embedding similarity to content-matching services, and attribution-by-design to emerging inference-time provenance systems.*

**Embed:**
```markdown
![Flow diagram: three research streams flowing from academic papers through a translation gap to commercial training data attribution products, showing most music attribution research never reaches deployment.](docs/figures/repo-figures/assets/fig-landscape-03-papers-to-products.jpg)
```

---

### fig-landscape-04: AI Disruption Points in the Music Value Chain

**Alt text:** Value chain diagram: six stages of the music industry (creation, recording, distribution, discovery, licensing, royalty) with five AI disruption points dropping to corresponding attribution questions -- who made this, what is human-made, who gets credit, what rights exist, and how to split fairly -- showing why provenance and transparent confidence scoring are urgent.

**Caption:** *AI disrupts the music value chain at five points -- creation, distribution, discovery, licensing, and royalty -- each generating a corresponding attribution question that demands new provenance infrastructure and transparent confidence mechanisms.*

**Embed:**
```markdown
![Value chain diagram: six music industry stages with five AI disruption points dropping to attribution questions about provenance and transparent confidence scoring.](docs/figures/repo-figures/assets/fig-landscape-04-disruption-points.jpg)
```

---

### fig-landscape-05: Market Maturity Spectrum: TRL x Adoption x Regulation

**Alt text:** Three-axis spectrum: six music attribution approaches (unlearning TDA, influence functions, embedding similarity, watermarking, fingerprinting, content ID) plotted across Technology Readiness Level, market adoption, and regulatory demand, revealing a critical maturity mismatch where fingerprinting is TRL 8 but legally contested while training data attribution is TRL 2 but faces surging EU AI Act demand.

**Caption:** *Six attribution approaches plotted across three axes reveal a maturity mismatch: audio fingerprinting scores high on TRL but faces legal contestation, while training data attribution is technically nascent but faces surging regulatory demand from the EU AI Act.*

**Embed:**
```markdown
![Three-axis spectrum: six music attribution approaches plotted across TRL, adoption, and regulatory demand, revealing a critical maturity mismatch.](docs/figures/repo-figures/assets/fig-landscape-05-maturity-spectrum.jpg)
```

---

### fig-landscape-06: Founder Decision Framework

**Alt text:** Decision funnel diagram for music attribution scaffold showing 7 training data attribution methods filtered through 4 startup constraints (model access, compute budget, latency, IP strategy) to identify 1-2 viable approaches per team -- open-source founder decision tool for AI-generated music provenance.

**Caption:** *Seven attribution methods filtered through four constraints (model access, retraining budget, real-time requirement, IP strategy) to narrow to 1-2 viable options for any given startup -- a practical founder decision tool.*

**Embed:**
```markdown
![Decision funnel: 7 attribution methods filtered by 4 constraints for music attribution startups](docs/figures/repo-figures/assets/fig-landscape-06-founder-decision-framework.jpg)
```

---

### fig-landscape-07: Same Landscape, Four Perspectives

**Alt text:** Multi-panel diagram of four music attribution stakeholders -- artists, labels, platforms, regulators -- showing structurally misaligned incentives around AI-generated music provenance, with conflicts on granularity and control but alignment on the need for transparent confidence scoring.

**Caption:** *Four stakeholder perspectives reveal structural misalignment: artists want per-song credit, labels want catalog licensing, platforms want blanket licenses, regulators want transparency -- they conflict on granularity but align on provenance.*

**Embed:**
```markdown
![Four stakeholder panels showing misaligned music attribution incentives](docs/figures/repo-figures/assets/fig-landscape-07-misaligned-incentives.jpg)
```

---

### fig-landscape-08: Geographic Regulatory Fragmentation

**Alt text:** Five-panel comparison of AI music attribution regulatory approaches across EU (AI Act), US (DMCA case law), UK (TDM exception), Nordic (collective licensing), and Asia-Pacific (mixed regimes) -- illustrating geographic fragmentation that any global attribution scaffold must handle.

**Caption:** *Five jurisdictions take fundamentally different approaches to AI music regulation: EU (legislative), US (case-law), UK (in flux), Nordic (collective licensing), Asia-Pacific (mixed). Any global attribution system must handle all five.*

**Embed:**
```markdown
![Five jurisdiction panels showing regulatory fragmentation for AI music attribution](docs/figures/repo-figures/assets/fig-landscape-08-regulatory-fragmentation.jpg)
```

---

### fig-landscape-09: Seven TDA Methods: What Each Actually Measures

**Alt text:** Multi-panel comparison: seven training data attribution methods arranged on a causal-to-corroborative axis -- unlearning-based TDA, influence functions, token flow tracking, inference-time conditioning, embedding similarity, replication detection, and watermarking -- each measuring fundamentally different quantities with different model access requirements, demonstrating that similarity does not equal causation in music attribution.

**Caption:** *Seven training data attribution methods compared by what they fundamentally measure (causal contribution versus perceptual similarity versus binary presence), what model access they require, and how they scale -- the most common error in attribution discourse is conflating similarity with causation.*

**Embed:**
```markdown
![Multi-panel comparison: seven training data attribution methods on a causal-to-corroborative axis, demonstrating that similarity does not equal causation in music attribution.](docs/figures/repo-figures/assets/fig-landscape-09-seven-tda-methods.jpg)
```

---

### fig-landscape-10: Post-hoc vs By-Design: Two Paradigms Converging

**Alt text:** Split-panel diagram: post-hoc training data attribution (influence functions, retroactive analysis) and attribution-by-design (Morreale et al. 2025, inference-time provenance) converging toward a hybrid zone containing open research problems, with assurance levels A0-A3 bridging both paradigms for transparent confidence in music attribution.

**Caption:** *Two attribution paradigms -- post-hoc analysis and attribution-by-design -- converge toward a hybrid zone where the hardest open problems live, with A0-A3 assurance levels providing a unifying framework that spans both paradigms.*

**Embed:**
```markdown
![Split-panel diagram: post-hoc TDA and attribution-by-design converging toward a hybrid zone, with A0-A3 assurance levels bridging both paradigms.](docs/figures/repo-figures/assets/fig-landscape-10-two-paradigms.jpg)
```

---

### fig-landscape-11: Audio Watermarking: 22 Schemes, 22 Attacks, None Robust

**Alt text:** Multi-panel robustness survey: 22 audio watermarking schemes (AudioSeal, WavMark, SynthID Audio) tested against 22 attack types (time-stretch, pitch-shift, neural regeneration) with a sparse robustness matrix showing no scheme survives all attacks -- watermarking is necessary but insufficient for music attribution provenance, with 8 previously unknown highly effective attacks discovered.

**Caption:** *Systematic evaluation of 22 watermarking schemes against 22 attacks (Wen et al. 2025) reveals no scheme survives all attacks, establishing that watermarking is a necessary but insufficient evidence layer for music attribution with transparent confidence scoring.*

**Embed:**
```markdown
![Robustness survey: 22 audio watermarking schemes tested against 22 attacks showing no scheme survives all -- watermarking is necessary but insufficient for music attribution.](docs/figures/repo-figures/assets/fig-landscape-11-watermarking-robustness.jpg)
```

---

### fig-landscape-12: Content ID Evolution

**Alt text:** Stepped evolution diagram of four content identification generations for music attribution -- audio fingerprinting (Shazam), watermarking (AudioSeal/SynthID), embedding similarity (CLAP), and AI detection (Afchar ICASSP 2025) -- each solving prior failures but all coexisting, with the causation gap unsolved.

**Caption:** *Four generations of content identification -- fingerprinting, watermarking, embedding similarity, and AI detection -- each addresses failures of its predecessor. All four coexist; the fundamental causation gap remains unsolved.*

**Embed:**
```markdown
![Four generations of content ID from fingerprinting to AI detection for music attribution](docs/figures/repo-figures/assets/fig-landscape-12-content-id-evolution.jpg)
```

---

### fig-landscape-13: The Metadata Mess

**Alt text:** Hero diagram showing six music identity standards (ISRC, ISWC, ISNI, DDEX, MusicBrainz, C2PA) as disconnected islands with broken bridges -- no unified provenance chain exists, making entity resolution across standards the unsolved infrastructure problem for open-source music attribution.

**Caption:** *Six major music identity standards cover different aspects but lack interoperability. Cross-standard entity resolution remains the unsolved infrastructure problem for music attribution with transparent confidence.*

**Embed:**
```markdown
![Six music metadata standards as disconnected islands with no unified provenance chain](docs/figures/repo-figures/assets/fig-landscape-13-metadata-mess.jpg)
```

---

### fig-landscape-14: On-Chain vs Off-Chain Provenance Trade-offs

**Alt text:** Three-panel comparison of on-chain provenance (Mubert/Audius), off-chain standards (C2PA/DDEX), and EU pharmaceutical serialization (EU FMD) as a hybrid cross-domain precedent for music attribution -- demonstrating the immutability-versus-speed trade-off with ISRC-on-chain and metadata-off-chain as the proposed solution.

**Caption:** *On-chain provides immutability at cost penalties; off-chain offers speed but requires trust. The EU Falsified Medicines Directive solved an identical dilemma with a hybrid architecture -- a direct cross-domain precedent for music attribution.*

**Embed:**
```markdown
![On-chain vs off-chain provenance trade-offs with pharma hybrid precedent](docs/figures/repo-figures/assets/fig-landscape-14-onchain-offchain.jpg)
```

---

### fig-landscape-15: Evidence Chain

**Alt text:** Four-stage flowchart of music attribution evidence chain showing confidence degradation from AI detection (99.8%) through source identification (~70%) and attribution quantification (~40%) to legal claim -- detection does not equal attribution does not equal legal proof under transparent confidence scoring.

**Caption:** *The evidence chain from AI detection to legal claim involves four stages with degrading confidence: detection (99.8%), source identification (~70%), attribution quantification (~40%), and legal claim. High detection accuracy does not propagate to attribution or legal proof.*

**Embed:**
```markdown
![Four-stage evidence chain with confidence degradation for music attribution](docs/figures/repo-figures/assets/fig-landscape-15-evidence-chain.jpg)
```

---

### fig-landscape-16: UQ in Attribution

**Alt text:** Split-panel diagram mapping uncertainty quantification from three mature domains (medical diagnosis, autonomous driving, financial risk) to music attribution via shared methods -- conformal prediction (Vovk 2005), Bayesian updating, and calibration curves -- demonstrating structural isomorphism for transparent confidence in assurance levels A0-A3.

**Caption:** *Three mature UQ domains map structurally to music attribution through conformal prediction, Bayesian updating, and calibration curves. This is structural isomorphism: identical mathematical frameworks, different domain instantiation. A0-A3 assurance levels parallel risk tiers in finance.*

**Embed:**
```markdown
![Cross-domain UQ isomorphism from medicine and finance to music attribution](docs/figures/repo-figures/assets/fig-landscape-16-uq-cross-domain.jpg)
```

---

### fig-landscape-17: Five Licensing Models: Economics at Scale

**Alt text:** Multi-panel comparison: five AI-generated music licensing models -- flat settlement (Suno/Warner), per-output, cohort-based (Lemonaide), blanket license (STIM), and attribution-weighted -- each with viability indicators at 1K, 1M, and 1B track scale, showing that simple models break at scale while attribution-weighted licensing needs infrastructure that only pays off at scale.

**Caption:** *Five licensing models compared at three scale tiers reveal a paradox: simple models break at scale, while attribution-weighted licensing requires infrastructure investment that only pays off at scale -- defining the economic case for open-source attribution scaffolds.*

**Embed:**
```markdown
![Five AI music licensing models compared at three scale tiers showing simple models break at scale while attribution-weighted licensing needs infrastructure.](docs/figures/repo-figures/assets/fig-landscape-17-licensing-models.jpg)
```

---

### fig-landscape-18: CMO Digital Transformation: Four Phases

**Alt text:** Four ascending steps showing collective management organization evolution from traditional blanket licensing through digital streaming and AI licensing (STIM pilot Sep 2025) to multi-CMO federation -- each phase requiring progressively more attribution infrastructure for transparent confidence in royalty distribution.

**Caption:** *CMOs transform through four phases: traditional blanket licensing, digital streaming, AI licensing (STIM pilot Sep 2025), and multi-CMO federation -- each requiring progressively more attribution infrastructure and demonstrating why CMO modernization is critical for AI-era royalty distribution.*

**Embed:**
```markdown
![Four CMO transformation phases from blanket licensing to federated attribution](docs/figures/repo-figures/assets/fig-landscape-18-cmo-transformation.jpg)
```

---

### fig-landscape-19: Ethical Certification: Binary vs Graduated vs Regulatory

**Alt text:** Three-panel trust stack: binary certification (Fairly Trained, 19 entities), graduated assurance levels A0-A3 (none to artist-verified), and mandatory regulation (EU AI Act Article 50) shown as complementary layers for AI-generated music provenance, with cross-domain food safety analogy mapping to HACCP, restaurant health grades, and FDA mandates.

**Caption:** *Three complementary trust layers -- binary certification (Fairly Trained), graduated assurance (A0-A3), and mandatory regulation (EU AI Act Article 50) -- work together like HACCP, health grades, and FDA mandates in food safety.*

**Embed:**
```markdown
![Three-panel trust stack: Fairly Trained, A0-A3 assurance, and EU AI Act as complementary layers with food safety analogy.](docs/figures/repo-figures/assets/fig-landscape-19-ethical-certification.jpg)
```

---

### fig-landscape-19b: AI Attribution Trust Stacks: Music vs Code

**Alt text:** Split-panel isomorphism: music attribution's A0-A3 assurance levels mapped to code attribution's unknown/ai/mixed/human classification (Cursor Agent Trace), with three shared challenges -- the Oracle Problem, graduated confidence, and EU AI Act Article 50 -- demonstrating structural equivalence between music provenance and AI-generated code attribution trust stacks.

**Caption:** *Music attribution's A0-A3 assurance levels map exactly to code attribution's unknown/ai/mixed/human classification from Cursor's Agent Trace spec -- both domains face the Oracle Problem and EU AI Act pressure on AI-generated content transparency.*

**Embed:**
```markdown
![Split-panel isomorphism: music A0-A3 mapped to code unknown/ai/mixed/human with shared Oracle Problem and EU AI Act challenges.](docs/figures/repo-figures/assets/fig-landscape-19b-ai-code-landscape.jpg)
```

---

### fig-landscape-20: Voice Rights & Identity Protection

**Alt text:** Five-layer protocol stack diagram for voice rights in AI-generated music -- consent, cloning technology, detection, legal framework, and compensation -- with coverage matrix showing no single company spans all layers, exposing voice rights as the most legally underdeveloped area of music attribution.

**Caption:** *Voice rights require a five-layer stack: consent, cloning, detection, legal framework, and compensation. No single company spans all five, and voice rights remain the most legally underdeveloped area of music AI -- with no US federal standard for voice protection.*

**Embed:**
```markdown
![Five-layer voice rights stack with coverage gaps for music AI attribution](docs/figures/repo-figures/assets/fig-landscape-20-voice-rights-stack.jpg)
```

---

### fig-landscape-21: 25 Years of MIR

**Alt text:** Timeline diagram from 2000-2025 tracing Music Information Retrieval milestones (ISMIR, librosa, Chromaprint, CLAP) above and commercial products (Shazam, Spotify, Suno) below -- exposing the citation gap where companies build on open-source MIR research but rarely cite or fund the academic foundations.

**Caption:** *Twenty-five years of MIR: academic milestones from ISMIR (2000) through librosa and CLAP -- each enabling commercial products from Shazam to Suno. The "citation gap" reveals that companies rarely cite or fund the academic research enabling them.*

**Embed:**
```markdown
![25-year MIR timeline with citation gap between academia and commercial music AI](docs/figures/repo-figures/assets/fig-landscape-21-mir-history.jpg)
```

---

### fig-landscape-22: Platform Evolution

**Alt text:** Four ascending steps showing AI music platform maturation from consumer toy (Suno 2023) through legal pressure (UMG lawsuits 2024) and licensing deals (Warner-Suno 2025) to attribution-integrated professional tools (2026+) -- each transition driven by external pressure including training data attribution requirements.

**Caption:** *AI music platforms mature through four stages: consumer toy (2023), legal pressure (2024), licensing deals (2025), and attribution-integrated tools (2026+) -- each transition driven by external pressure from lawsuits, market demand, or attribution infrastructure.*

**Embed:**
```markdown
![Four stages of AI music platform maturation toward attribution integration](docs/figures/repo-figures/assets/fig-landscape-22-platform-evolution.jpg)
```

---

### fig-landscape-23: Founder's Build-vs-Buy-vs-Partner Decision Map

**Alt text:** Decision matrix mapping 12 music attribution infrastructure components across three team archetypes (solo hacker, funded startup, enterprise) with build/buy/partner recommendations -- open-source scaffold planning tool for attribution startups with strategic components built in-house for differentiation.

**Caption:** *Twelve infrastructure components mapped across three team archetypes: commodity components are bought, core components vary by funding stage, and strategic components (confidence scoring, MCP, TDA) are built in-house for differentiation.*

**Embed:**
```markdown
![Build-vs-buy decision matrix for 12 music attribution components and 3 team archetypes](docs/figures/repo-figures/assets/fig-landscape-23-build-vs-buy.jpg)
```

---

### fig-landscape-24: $7.4B Revenue Distribution

**Alt text:** Hero comparison of two revenue distribution scenarios for the projected $7.4B AI-generated music market (2035): without attribution 80% flows to platforms with artists at 5%, versus with transparent confidence-based attribution where artists receive 20% -- creating a $740M-$1.1B attribution infrastructure market opportunity.

**Caption:** *Projected $7.4B generative AI music market (2035) under two scenarios: without attribution (80% to platforms, 5% to artists) versus with attribution infrastructure (20% to artists) -- creating a $740M-$1.1B market opportunity.*

**Embed:**
```markdown
![Two scenarios showing $7.4B market split with and without music attribution](docs/figures/repo-figures/assets/fig-landscape-24-revenue-distribution.jpg)
```

---

### fig-landscape-25: Research Priority Forecast 2026-2028

**Alt text:** Four-panel research roadmap: multimodal attribution (beyond audio-only TDA, 2026 H2), real-time streaming attribution (sub-100ms latency, 2027 H1), AI-assisted analysis (LLM explanations for transparent confidence, 2027 H2), and federated attribution (privacy-preserving multi-CMO coordination, 2028), mapping academia-industry gaps in music attribution research priorities.

**Caption:** *Four research priorities for 2026-2028 map the gap between academic focus and industry deployment needs: multimodal attribution, real-time streaming, AI-assisted analysis using LLMs for transparent confidence explanations, and federated privacy-preserving cross-registry coordination.*

**Embed:**
```markdown
![Four-panel research roadmap for 2026-2028 mapping academia-industry gaps in music attribution priorities.](docs/figures/repo-figures/assets/fig-landscape-25-research-priorities.jpg)
```

---

### fig-landscape-26: Cross-Domain Method Transfer

**Alt text:** Hero diagram mapping four mature solved-problem domains to music attribution through structural isomorphisms: GS1 EPCIS entity resolution to ISRC/ISWC/ISNI matching, SOX audit trails to A0-A3 assurance levels, EU FMD pharma serialization to hybrid provenance chains, and Shapley values to proportional royalty distribution -- not metaphor, identical mathematics.

**Caption:** *Music attribution imports mature solutions from four domains through structural isomorphisms: GS1 EPCIS to ISRC/ISWC/ISNI resolution, SOX to A0-A3 assurance, EU FMD to hybrid provenance, and Shapley values to proportional royalty -- identical mathematical frameworks, not metaphors.*

**Embed:**
```markdown
![Four-domain cross-domain method transfer to open-source music attribution](docs/figures/repo-figures/assets/fig-landscape-26-cross-domain-transfer.jpg)
```

---

### fig-landscape-27: Agentic Music Attribution

**Alt text:** Flowchart of agentic music attribution pipeline where AI agents autonomously query MCP for consent, coordinate via A2A protocol, and pay via ACP commerce layer -- showing bowling-shoe versus BYO agent archetypes and A0-A3 assurance-level permission checking for automated training data attribution and royalty payment.

**Caption:** *Agentic attribution automates the consent-license-pay cycle through MCP (permission queries), A2A (agent coordination), and ACP (royalty negotiation), with bowling-shoe and BYO agent archetypes traversing the same trust stack differently.*

**Embed:**
```markdown
![Agentic attribution flowchart with MCP, A2A, ACP protocols for automated music licensing](docs/figures/repo-figures/assets/fig-landscape-27-agentic-attribution.jpg)
```

---

### fig-landscape-28: Five Emerging Service Categories 2026

**Alt text:** Five-panel market map of emerging 2026 service categories: Attribution-as-a-Service (Sureel), Ethical Certification (Fairly Trained), Voice Rights Management (Kits AI), CMO Federation Infrastructure (STIM), and AI Detection APIs (EU AI Act Article 50 mandate) -- each a distinct startup opportunity in the open-source attribution scaffold ecosystem.

**Caption:** *Five new service categories crystallizing in 2026: Attribution-as-a-Service, Ethical Certification-aaS, Voice Rights Management, CMO Federation Infrastructure, and AI Detection APIs -- each a distinct startup opportunity at the music-AI-attribution intersection.*

**Embed:**
```markdown
![Five emerging 2026 service categories for music AI attribution market](docs/figures/repo-figures/assets/fig-landscape-28-emerging-categories.jpg)
```

---

### fig-landscape-29: Regulatory Cascade

**Alt text:** Four-level cascade flowchart showing how EU AI Act legislation flows through codes of practice (GPAI Code), technical standards (ISO/IEC 42001, C2PA), and architecture requirements (logging, audit trails, content labeling, provenance chains) -- every music attribution scaffold architecture decision is an implicit regulatory bet.

**Caption:** *Regulatory requirements cascade through four levels: legislation (EU AI Act) to codes of practice (GPAI Code), technical standards (ISO/IEC 42001, C2PA), and architecture requirements -- making every scaffold architecture decision an implicit regulatory bet.*

**Embed:**
```markdown
![Four-level regulatory cascade from EU AI Act to attribution architecture requirements](docs/figures/repo-figures/assets/fig-landscape-29-regulatory-cascade.jpg)
```

---

### fig-landscape-30: The Convergence Thesis

**Alt text:** Three-circle Venn diagram showing music attribution as a new discipline emerging at the intersection of Music Information Retrieval (25 years, ISMIR), Explainable AI (LIME/SHAP), and Uncertainty Quantification (conformal prediction, Vovk 2005) -- with transparent confidence as the unifying framework for open-source attribution.

**Caption:** *Music attribution emerges at the intersection of MIR (25 years), XAI (post-2016 interpretability), and UQ (conformal prediction) -- the triple intersection defines "who made this, how confident are we, and can we explain why?" This convergence framing is novel.*

**Embed:**
```markdown
![Venn diagram: MIR + XAI + UQ converge into music attribution as new discipline](docs/figures/repo-figures/assets/fig-landscape-30-convergence-thesis.jpg)
```

---

### fig-landscape-31: Open Problems

**Alt text:** Three-panel research roadmap mapping open music attribution problems to investor time horizons: solvable in 2 years (metadata interoperability, binary AI detection), hard in 5 years (cross-model TDA, federated attribution), and fundamental at 10+ years (oracle problem, cultural context) -- with funding-difficulty inversion.

**Caption:** *Open problems mapped to investor horizons: solvable in 2 years (seed-stage), hard in 5 years (Series A), and fundamental at 10+ years (grant-funded, possibly unsolvable including the Oracle Problem) -- with an ironic funding-difficulty inversion.*

**Embed:**
```markdown
![Three-panel open problems by time horizon with funding-difficulty inversion](docs/figures/repo-figures/assets/fig-landscape-31-open-problems.jpg)
```

---

### fig-landscape-32: From Landscape to Product

**Alt text:** Six-step circular meta-loop flowchart showing how landscape scanning feeds market intelligence, updates probabilistic PRD priors via Bayesian updating on 30 decision nodes, drives music attribution scaffold architecture decisions, shapes product evolution, and generates market response that triggers the next scan.

**Caption:** *The meta-loop connects landscape analysis to product development through six continuous steps: scanning, intelligence, PRD updates (Bayesian on 30 nodes), architecture decisions, product evolution, and market response -- making the probabilistic PRD a living document.*

**Embed:**
```markdown
![Six-step meta-loop from landscape scanning to product through probabilistic PRD](docs/figures/repo-figures/assets/fig-landscape-32-meta-loop.jpg)
```

---

## Voice Agent (fig-voice-*)

### fig-voice-01: Voice Agent Full Stack Architecture

**Alt text:** Architecture diagram of a five-stage real-time voice agent pipeline showing Transport, STT, LLM, TTS, and output stages with semantic endpointing as the orchestration layer for music attribution voice interactions.

**Caption:** *Five-panel architecture diagram showing the complete voice agent pipeline from transport input through STT, LLM, TTS, and back to transport output, with semantic endpointing as the critical orchestration layer.*

**Embed:**
```markdown
![Architecture diagram of a five-stage real-time voice agent pipeline showing Transport, STT, LLM, TTS, and output stages with semantic endpointing as the orchestration layer for music attribution voice interactions.](docs/figures/repo-figures/assets/fig-voice-01-full-stack-architecture.jpg)
```

---

### fig-voice-02: Voice Agent Latency Budget

**Alt text:** Vertical latency budget diagram for a real-time voice agent pipeline showing VAD, speech-to-text, LLM, text-to-speech, and transport stages targeting under 500ms total, with best-available Feb 2026 stack achieving 484ms.

**Caption:** *Vertical timeline showing the voice agent latency budget across five pipeline stages, targeting under 500 milliseconds total with concrete Feb 2026 best-available stack achieving ~484ms.*

**Embed:**
```markdown
![Vertical latency budget diagram for a real-time voice agent pipeline showing VAD, speech-to-text, LLM, text-to-speech, and transport stages targeting under 500ms total, with best-available Feb 2026 stack achieving 484ms.](docs/figures/repo-figures/assets/fig-voice-02-pipeline-latency-budget.jpg)
```

---

### fig-voice-03: Cascaded vs Speech-to-Speech Architecture

**Alt text:** Split-panel comparison of cascaded voice agent pipeline (STT, LLM, TTS modules) versus monolithic speech-to-speech model, with the tandem Kame pattern combining both for optimal latency and reasoning in music attribution workflows.

**Caption:** *Split-panel comparison of cascaded modular pipeline versus speech-to-speech end-to-end architecture, with the tandem pattern combining both approaches for optimal latency and reasoning depth.*

**Embed:**
```markdown
![Split-panel comparison of cascaded voice agent pipeline (STT, LLM, TTS modules) versus monolithic speech-to-speech model, with the tandem Kame pattern combining both for optimal latency and reasoning in music attribution workflows.](docs/figures/repo-figures/assets/fig-voice-03-cascaded-vs-speech-to-speech.jpg)
```

---

### fig-voice-04: Voice Agent Framework Decision

**Alt text:** Decision tree for voice agent framework selection comparing open-source (Pipecat, LiveKit), managed (Retell AI, Vapi), and custom options, with Pipecat highlighted as the recommended Python-native MVP choice for music attribution.

**Caption:** *Decision tree mapping the voice agent framework choice from build-vs-buy through open-source and managed options, with Pipecat selected as the recommended MVP framework for Python-native PydanticAI compatibility.*

**Embed:**
```markdown
![Decision tree for voice agent framework selection comparing open-source (Pipecat, LiveKit), managed (Retell AI, Vapi), and custom options, with Pipecat highlighted as the recommended Python-native MVP choice for music attribution.](docs/figures/repo-figures/assets/fig-voice-04-framework-landscape.jpg)
```

---

### fig-voice-05: Dual Persona Voice System

**Alt text:** Dual persona voice agent system showing a shared pipeline branching into Attribution Agent (system voice for metadata gap-filling) and Digital Twin (artist voice clone), gated by progressive consent levels L1 through L3 for music attribution.

**Caption:** *Dual persona voice system showing how a shared pipeline branches into Attribution Agent and Digital Twin personas, with progressive consent levels (L1/L2/L3) gating access to each persona's features.*

**Embed:**
```markdown
![Dual persona voice agent system showing a shared pipeline branching into Attribution Agent (system voice for metadata gap-filling) and Digital Twin (artist voice clone), gated by progressive consent levels L1 through L3 for music attribution.](docs/figures/repo-figures/assets/fig-voice-05-dual-persona-system.jpg)
```

---

### fig-voice-06: Voice Transport: WebRTC vs WebSocket

**Alt text:** Split-panel comparison of WebRTC and WebSocket voice transport protocols for real-time voice agents, showing WebRTC recommended with sub-30ms latency and built-in echo cancellation versus WebSocket adding 50-100ms overhead.

**Caption:** *Split-panel comparison of WebRTC and WebSocket voice transport protocols, with WebRTC recommended for its sub-30ms latency and built-in audio processing capabilities.*

**Embed:**
```markdown
![Split-panel comparison of WebRTC and WebSocket voice transport protocols for real-time voice agents, showing WebRTC recommended with sub-30ms latency and built-in echo cancellation versus WebSocket adding 50-100ms overhead.](docs/figures/repo-figures/assets/fig-voice-06-webrtc-transport-layer.jpg)
```

---

### fig-voice-07: STT Model Landscape (Feb 2026)

**Alt text:** Four-quadrant positioning chart of speech-to-text models (Feb 2026) mapping accuracy versus cost, with Canary-Qwen leading accuracy at 5.63% WER, Deepgram Nova-3 for production, Faster-Whisper for self-hosted, and Moonshine for edge deployment.

**Caption:** *Four-quadrant STT model positioning chart showing accuracy versus cost for Feb 2026, with category leaders identified for production, self-hosted, accuracy-optimized, and edge deployment scenarios.*

**Embed:**
```markdown
![Four-quadrant positioning chart of speech-to-text models (Feb 2026) mapping accuracy versus cost, with Canary-Qwen leading accuracy at 5.63% WER, Deepgram Nova-3 for production, Faster-Whisper for self-hosted, and Moonshine for edge deployment.](docs/figures/repo-figures/assets/fig-voice-07-stt-model-landscape.jpg)
```

---

### fig-voice-08: STT Accuracy vs Latency Tradeoff (NOT GENERATED)

**Alt text:** Scatter plot of speech-to-text models positioned by accuracy (inverse WER) versus latency (RTFx), with a Pareto frontier connecting Canary-Qwen, Deepgram Nova-3, Faster-Whisper, and Moonshine as tradeoff leaders.

**Caption:** *Scatter plot showing STT models positioned by accuracy versus latency, with a coral Pareto frontier line connecting models where no alternative dominates on both axes simultaneously.*

**Embed:**
```markdown
![Scatter plot of speech-to-text models positioned by accuracy (inverse WER) versus latency (RTFx), with a Pareto frontier connecting Canary-Qwen, Deepgram Nova-3, Faster-Whisper, and Moonshine as tradeoff leaders.](docs/figures/repo-figures/assets/fig-voice-08-stt-accuracy-vs-latency.jpg)
```

---

### fig-voice-09: VAD and Turn Detection Evolution

**Alt text:** Three-generation evolution of voice activity detection and turn detection for voice agents: from silence thresholds (50% TPR) through deep learning VAD with Silero (87.7% TPR) to semantic endpointing achieving 260ms latency and 30% fewer false interruptions.

**Caption:** *Three-generation evolution of voice activity detection from silence thresholds through deep learning VAD to semantic endpointing, showing how each generation reduces false interruptions.*

**Embed:**
```markdown
![Three-generation evolution of voice activity detection and turn detection for voice agents: from silence thresholds (50% TPR) through deep learning VAD with Silero (87.7% TPR) to semantic endpointing achieving 260ms latency and 30% fewer false interruptions.](docs/figures/repo-figures/assets/fig-voice-09-vad-turn-detection-evolution.jpg)
```

---

### fig-voice-10: Conversational Speech Recognition: The Paradigm Shift

**Alt text:** Split-panel comparison of traditional fragmented speech recognition pipeline versus Deepgram Flux Conversational Speech Recognition unified model fusing acoustic and semantic streams for 260ms end-of-turn latency.

**Caption:** *Split-panel comparison of traditional fragmented speech recognition pipeline versus Deepgram Flux CSR unified model with acoustic and semantic stream fusion achieving 260ms end-of-turn latency.*

**Embed:**
```markdown
![Split-panel comparison of traditional fragmented speech recognition pipeline versus Deepgram Flux Conversational Speech Recognition unified model fusing acoustic and semantic streams for 260ms end-of-turn latency.](docs/figures/repo-figures/assets/fig-voice-10-conversational-speech-recognition.jpg)
```

---

### fig-voice-11: TTS Model Landscape (Feb 2026)

**Alt text:** Two-row text-to-speech model landscape for Feb 2026 comparing commercial APIs (ElevenLabs, Cartesia, Inworld, Rime) with open-source models (Chatterbox, Orpheus, Kokoro, CosyVoice) that crossed the quality threshold enabling 100x cost reduction.

**Caption:** *Two-row landscape of TTS models as of February 2026: commercial providers versus open-source models with parameter counts, latency, cost, and license for each.*

**Embed:**
```markdown
![Two-row text-to-speech model landscape for Feb 2026 comparing commercial APIs (ElevenLabs, Cartesia, Inworld, Rime) with open-source models (Chatterbox, Orpheus, Kokoro, CosyVoice) that crossed the quality threshold enabling 100x cost reduction.](docs/figures/repo-figures/assets/fig-voice-11-tts-model-landscape.jpg)
```

---

### fig-voice-12: TTS Arena Leaderboard (Feb 2026)

**Alt text:** Horizontal bar chart of TTS Arena blind preference leaderboard (Feb 2026) ranking seven speech synthesis models by Elo score, with Vocu V3.0 at #1 and ElevenLabs Flash displaced to #7, showing market fragmentation in text-to-speech quality.

**Caption:** *Horizontal bar chart of TTS Arena blind preference leaderboard as of February 2026, showing Vocu V3.0 at #1 and ElevenLabs Flash displaced to #7 by specialized competitors.*

**Embed:**
```markdown
![Horizontal bar chart of TTS Arena blind preference leaderboard (Feb 2026) ranking seven speech synthesis models by Elo score, with Vocu V3.0 at #1 and ElevenLabs Flash displaced to #7, showing market fragmentation in text-to-speech quality.](docs/figures/repo-figures/assets/fig-voice-12-tts-arena-leaderboard.jpg)
```

---

### fig-voice-13: The Open-Source TTS Revolution

**Alt text:** Three-panel comparison of open-source TTS models: Chatterbox (MIT, 350M params), Orpheus (Apache 2.0, emotion tags, 200ms streaming), and Kokoro (82M params, $0.06/hr self-hosted), demonstrating 100x cost reduction over commercial APIs.

**Caption:** *Three-panel showcase of the open-source TTS revolution: Chatterbox, Orpheus, and Kokoro reducing TTS cost by 100x through self-hosting.*

**Embed:**
```markdown
![Three-panel comparison of open-source TTS models: Chatterbox (MIT, 350M params), Orpheus (Apache 2.0, emotion tags, 200ms streaming), and Kokoro (82M params, $0.06/hr self-hosted), demonstrating 100x cost reduction over commercial APIs.](docs/figures/repo-figures/assets/fig-voice-13-open-source-tts-revolution.jpg)
```

---

### fig-voice-14: Voice Cloning Consent: PRAC3 to A0-A3 Mapping

**Alt text:** Flowchart mapping six PRAC3 voice cloning consent dimensions to A0-A3 music attribution assurance levels, showing voice consent as a progressive spectrum from no provenance to artist-verified voice prints.

**Caption:** *Flowchart mapping PRAC3 voice cloning consent dimensions to A0-A3 assurance levels, showing consent as a spectrum from no provenance to artist-verified voice prints.*

**Embed:**
```markdown
![Flowchart mapping six PRAC3 voice cloning consent dimensions to A0-A3 music attribution assurance levels, showing voice consent as a progressive spectrum from no provenance to artist-verified voice prints.](docs/figures/repo-figures/assets/fig-voice-14-voice-cloning-consent-framework.jpg)
```

---

### fig-voice-15: Speech LLM Architecture Taxonomy

**Alt text:** Taxonomy diagram of three Speech LLM architecture families from the ACL 2025 survey: Encoder-Decoder, Decoder-Only, and Hierarchical Codebook, with quality and latency tradeoff indicators.

**Caption:** *Three-family taxonomy of Speech LLM architectures from the ACL 2025 survey with quality and latency indicators.*

**Embed:**
```markdown
![Taxonomy diagram of three Speech LLM architecture families from the ACL 2025 survey: Encoder-Decoder, Decoder-Only, and Hierarchical Codebook, with quality and latency tradeoff indicators.](docs/figures/repo-figures/assets/fig-voice-15-speech-llm-taxonomy.jpg)
```

---

### fig-voice-16: Ultravox v0.7: The Speech-Native Breakthrough

**Alt text:** Hero figure showing Ultravox v0.7 achieving 97.7% on production benchmarks as the first speech-to-speech model to beat text-mode LLMs, compared against GPT Realtime (86.7%) and Gemini Live (86.0%).

**Caption:** *Hero figure showing Ultravox v0.7 achieving 97.7% versus GPT Realtime (86.7%) and Gemini Live (86.0%), with simplified architecture diagram.*

**Embed:**
```markdown
![Hero figure showing Ultravox v0.7 achieving 97.7% on production benchmarks as the first speech-to-speech model to beat text-mode LLMs, compared against GPT Realtime (86.7%) and Gemini Live (86.0%).](docs/figures/repo-figures/assets/fig-voice-16-ultravox-breakthrough.jpg)
```

---

### fig-voice-17: Full-Duplex Voice: Listen While Speaking

**Alt text:** Split-panel diagram comparing half-duplex turn-based voice interaction versus full-duplex simultaneous listen-and-speak architecture with backchanneling, featuring Moshi, SALMONN-omni, and PersonaPlex models.

**Caption:** *Split-panel comparing half-duplex turn-based voice interaction with full-duplex simultaneous listen-and-speak architecture.*

**Embed:**
```markdown
![Split-panel diagram comparing half-duplex turn-based voice interaction versus full-duplex simultaneous listen-and-speak architecture with backchanneling, featuring Moshi, SALMONN-omni, and PersonaPlex models.](docs/figures/repo-figures/assets/fig-voice-17-full-duplex-architecture.jpg)
```

---

### fig-voice-18: Pipecat Pipeline Anatomy

**Alt text:** Step-by-step architecture diagram of the Pipecat voice agent framework showing five layers: transport, FrameProcessor chain, 40+ service plugins, parallel pipeline, and Smart Turn with Pipecat Flows for state management.

**Caption:** *Step-by-step anatomy of a Pipecat voice agent pipeline showing transport, FrameProcessor chain, 40+ service plugins, parallel pipeline, and Smart Turn with Flows.*

**Embed:**
```markdown
![Step-by-step architecture diagram of the Pipecat voice agent framework showing five layers: transport, FrameProcessor chain, 40+ service plugins, parallel pipeline, and Smart Turn with Pipecat Flows for state management.](docs/figures/repo-figures/assets/fig-voice-18-pipecat-pipeline-anatomy.jpg)
```

---

### fig-voice-19: LiveKit Agents Architecture

**Alt text:** Architecture diagram of LiveKit Agents showing the agent-as-participant model where the voice agent joins a WebRTC room alongside users, with VoicePipelineAgent, pluggable services, telephony integration, and managed Cloud Agents deployment.

**Caption:** *LiveKit Agents architecture showing the agent-as-participant model in a WebRTC room with voice pipeline, pluggable services, and Cloud Agents deployment.*

**Embed:**
```markdown
![Architecture diagram of LiveKit Agents showing the agent-as-participant model where the voice agent joins a WebRTC room alongside users, with VoicePipelineAgent, pluggable services, telephony integration, and managed Cloud Agents deployment.](docs/figures/repo-figures/assets/fig-voice-19-livekit-agents-architecture.jpg)
```

---

### fig-voice-20: Voice Agent Deployment Options

**Alt text:** Decision tree flowchart for voice agent deployment showing four paths: managed platforms, framework + cloud APIs (highlighted as sweet spot), self-hosted open-source, and hybrid edge + cloud approaches.

**Caption:** *Decision tree for voice agent deployment showing four paths from fully managed through framework + APIs to self-hosted and hybrid approaches.*

**Embed:**
```markdown
![Decision tree flowchart for voice agent deployment showing four paths: managed platforms, framework + cloud APIs (highlighted as sweet spot), self-hosted open-source, and hybrid edge + cloud approaches.](docs/figures/repo-figures/assets/fig-voice-20-production-deployment-options.jpg)
```

---

### fig-voice-21: Build vs Buy: Voice Agent Decision Matrix (MISMATCH)

**Alt text:** Four-column decision matrix comparing voice agent deployment approaches across six dimensions: cost per minute, time to market, customization, scalability, vendor lock-in, and DevOps burden, with framework + APIs highlighted as the recommended sweet spot.

**Caption:** *Four-column decision matrix comparing managed platforms, framework + APIs (sweet spot), self-hosted, and hybrid deployment approaches.*

**Embed:**
```markdown
![Four-column decision matrix comparing voice agent deployment approaches across six dimensions: cost per minute, time to market, customization, scalability, vendor lock-in, and DevOps burden, with framework + APIs highlighted as the recommended sweet spot.](docs/figures/repo-figures/assets/fig-voice-21-managed-vs-self-hosted-decision.jpg)
```

---

### fig-voice-22: Voice Agent Evaluation: 2024 to 2026

**Alt text:** Timeline showing voice agent evaluation benchmark evolution across five phases from 2024 to 2026: component metrics, multi-dimensional scoring, agentic benchmarks, LLM-as-judge paradigms, and safety + emotion assessment.

**Caption:** *Timeline showing voice agent evaluation evolution from WER-only component metrics (2024) through multi-dimensional benchmarks to safety and emotion assessment (early 2026).*

**Embed:**
```markdown
![Timeline showing voice agent evaluation benchmark evolution across five phases from 2024 to 2026: component metrics, multi-dimensional scoring, agentic benchmarks, LLM-as-judge paradigms, and safety + emotion assessment.](docs/figures/repo-figures/assets/fig-voice-22-evaluation-evolution-timeline.jpg)
```

---

### fig-voice-23: The Fragmented Leaderboard Landscape

**Alt text:** Four-panel diagram showing fragmented voice agent evaluation landscape across STT, TTS, end-to-end, and voice cloning leaderboards, revealing no unified benchmark exists for full pipeline assessment.

**Caption:** *Four-panel diagram showing the fragmented voice agent leaderboard landscape with no unified benchmark capturing the full voice agent experience.*

**Embed:**
```markdown
![Four-panel diagram showing fragmented voice agent evaluation landscape across STT, TTS, end-to-end, and voice cloning leaderboards, revealing no unified benchmark exists for full pipeline assessment.](docs/figures/repo-figures/assets/fig-voice-23-leaderboard-landscape-fragmentation.jpg)
```

---

### fig-voice-24: aiewf-eval: Production Voice Agent Benchmark

**Alt text:** Two-panel aiewf-eval benchmark comparison of text-mode LLMs scoring 100% versus speech-to-speech models at 86-98%, illustrating the intelligence-latency tradeoff in voice agent evaluation.

**Caption:** *Two-panel comparison of aiewf-eval benchmark results showing text-mode LLMs versus speech-to-speech models evaluated on tool use, instruction following, and knowledge grounding.*

**Embed:**
```markdown
![Two-panel aiewf-eval benchmark comparison of text-mode LLMs scoring 100% versus speech-to-speech models at 86-98%, illustrating the intelligence-latency tradeoff in voice agent evaluation.](docs/figures/repo-figures/assets/fig-voice-24-aiewf-eval-benchmark-results.jpg)
```

---

### fig-voice-25: What Voice Agent Leaderboards Miss

**Alt text:** Radar chart displaying 10 critical voice agent evaluation dimensions where current leaderboards cover only WER, MOS, and task completion, leaving 8 production-critical gaps untested.

**Caption:** *Radar chart showing 10 critical evaluation dimensions for voice agents, revealing that current leaderboards cover only 3 while 8 remain untested.*

**Embed:**
```markdown
![Radar chart displaying 10 critical voice agent evaluation dimensions where current leaderboards cover only WER, MOS, and task completion, leaving 8 production-critical gaps untested.](docs/figures/repo-figures/assets/fig-voice-25-evaluation-gaps-radar.jpg)
```

---

### fig-voice-26: Voice Agent Cost: Anatomy of a Minute

**Alt text:** Stacked cost breakdown of voice agent operation per minute across three FinOps tiers showing Budget at $0.01-0.02, Mid at $0.05-0.08, and Premium at $0.15-0.20, revealing a 10x cost difference driven by LLM and TTS choices.

**Caption:** *Stacked cost breakdown across three tiers showing a 10x cost difference driven primarily by LLM and TTS choices.*

**Embed:**
```markdown
![Stacked cost breakdown of voice agent operation per minute across three FinOps tiers showing Budget at $0.01-0.02, Mid at $0.05-0.08, and Premium at $0.15-0.20, revealing a 10x cost difference driven by LLM and TTS choices.](docs/figures/repo-figures/assets/fig-voice-26-cost-per-minute-breakdown.jpg)
```

---

### fig-voice-27: Voice AI FinOps: Six Cost Levers

**Alt text:** Six-panel FinOps grid presenting voice AI cost optimization levers -- model selection, semantic caching, on-device STT/TTS, context management, tiered quality, and volume commitments -- combining for up to 16x total cost reduction.

**Caption:** *Six cost optimization levers for voice AI infrastructure, each independently quantified and combinable for up to 16x total cost reduction.*

**Embed:**
```markdown
![Six-panel FinOps grid presenting voice AI cost optimization levers -- model selection, semantic caching, on-device STT/TTS, context management, tiered quality, and volume commitments -- combining for up to 16x total cost reduction.](docs/figures/repo-figures/assets/fig-voice-27-finops-optimization-strategies.jpg)
```

---

### fig-voice-28: The AI Companion Cost Trap

**Alt text:** Split-panel diagram illustrating the AI companion cost trap with Character.AI's 11x loss ratio from $365M serving costs against $32M revenue, alongside five FinOps escape strategies for sustainable voice agent economics.

**Caption:** *Split-panel showing the AI companion cost trap and five escape strategies for sustainable voice agent economics.*

**Embed:**
```markdown
![Split-panel diagram illustrating the AI companion cost trap with Character.AI's 11x loss ratio from $365M serving costs against $32M revenue, alongside five FinOps escape strategies for sustainable voice agent economics.](docs/figures/repo-figures/assets/fig-voice-28-ai-companion-cost-trap.jpg)
```

---

### fig-voice-29: Voice Rights Stack: PRAC3 Meets Attribution (MISMATCH)

**Alt text:** Flowchart mapping PRAC3 voice rights framework with six dimensions to concrete music attribution implementations connected to A0-A3 assurance levels.

**Caption:** *Mapping of PRAC3 voice rights framework to music attribution implementations, showing how each dimension translates to technical mechanisms and A0-A3 assurance levels.*

**Embed:**
```markdown
![Flowchart mapping PRAC3 voice rights framework with six dimensions to concrete music attribution implementations connected to A0-A3 assurance levels.](docs/figures/repo-figures/assets/fig-voice-29-prac3-assurance-mapping.jpg)
```

---

### fig-voice-30: EU AI Act: Voice Agent Compliance Checklist

**Alt text:** Five-step EU AI Act compliance checklist for voice agent systems covering Article 50 disclosure, synthetic audio labeling, deepfake consent, transparency documentation, and risk classification, with enforcement timeline through August 2026.

**Caption:** *Five EU AI Act compliance requirements for voice agent systems with enforcement timeline through August 2026.*

**Embed:**
```markdown
![Five-step EU AI Act compliance checklist for voice agent systems covering Article 50 disclosure, synthetic audio labeling, deepfake consent, transparency documentation, and risk classification, with enforcement timeline through August 2026.](docs/figures/repo-figures/assets/fig-voice-30-eu-ai-act-voice-compliance.jpg)
```

---

### fig-voice-31: Recommended Voice Agent MVP Stack (MISMATCH)

**Alt text:** Six-layer recommended voice agent MVP stack for music attribution showing Pipecat, Deepgram Nova-3, PydanticAI, Cartesia Sonic, Daily.co WebRTC, and Smart Turn with $0.05-0.08/min operating cost.

**Caption:** *Recommended six-layer voice agent MVP stack for music attribution at $0.05-0.08/min operating cost.*

**Embed:**
```markdown
![Six-layer recommended voice agent MVP stack for music attribution showing Pipecat, Deepgram Nova-3, PydanticAI, Cartesia Sonic, Daily.co WebRTC, and Smart Turn with $0.05-0.08/min operating cost.](docs/figures/repo-figures/assets/fig-voice-31-recommended-mvp-stack.jpg)
```

---

### fig-voice-32: Voice Agent Implementation Roadmap

**Alt text:** Five-phase voice agent implementation roadmap for music attribution starting with STT-only input shipping in 2 months, progressing through system voice, turn detection, digital twin with consent, to premium features.

**Caption:** *Five-phase voice agent implementation roadmap from Phase I voice-to-text (Month 1-2) through Phase V premium features (Month 6+).*

**Embed:**
```markdown
![Five-phase voice agent implementation roadmap for music attribution starting with STT-only input shipping in 2 months, progressing through system voice, turn detection, digital twin with consent, to premium features.](docs/figures/repo-figures/assets/fig-voice-32-implementation-roadmap.jpg)
```

---

## Persona Coherence (fig-persona-*)

### fig-persona-01: Multi-Dimensional Persona Architecture

**Alt text:** Architecture diagram showing five persona dimensions arranged by graduated stability from immutable Core Identity to free Conversation Flow, divided into stable and adaptive layers by a stability boundary for persona coherence engineering.

**Caption:** *Multi-dimensional persona architecture showing five dimensions with graduated stability properties.*

**Embed:**
```markdown
![Architecture diagram showing five persona dimensions arranged by graduated stability from immutable Core Identity to free Conversation Flow, divided into stable and adaptive layers by a stability boundary for persona coherence engineering.](docs/figures/repo-figures/assets/fig-persona-01-multi-dimensional-persona-architecture.jpg)
```

---

### fig-persona-02: PersonaFuse Mixture-of-Experts Architecture

**Alt text:** System architecture diagram of PersonaFuse Mixture-of-Experts showing a three-stage training pipeline and inference flow where a Qwen2.5-0.5B situation-aware router composes 10 Big Five personality LoRA experts via weighted combination and Persona-CoT reasoning.

**Caption:** *PersonaFuse MoE architecture showing three-stage training pipeline and inference flow with situation-aware router.*

**Embed:**
```markdown
![System architecture diagram of PersonaFuse Mixture-of-Experts showing a three-stage training pipeline and inference flow where a Qwen2.5-0.5B situation-aware router composes 10 Big Five personality LoRA experts via weighted combination and Persona-CoT reasoning.](docs/figures/repo-figures/assets/fig-persona-02-personafuse-moe-architecture.jpg)
```

---

### fig-persona-03: Persona Vectors in Activation Space

**Alt text:** Visualization of persona trait vectors in LLM activation space with directions for helpfulness, sycophancy, hallucination, and evil radiating from an origin point, highlighting the Assistant Axis principal component and the steering mechanism formula.

**Caption:** *Persona vectors in activation space showing identifiable directions for traits, with the Assistant Axis principal component and steering formula.*

**Embed:**
```markdown
![Visualization of persona trait vectors in LLM activation space with directions for helpfulness, sycophancy, hallucination, and evil radiating from an origin point, highlighting the Assistant Axis principal component and the steering mechanism formula.](docs/figures/repo-figures/assets/fig-persona-03-persona-vectors-activation-space.jpg)
```

---

### fig-persona-04: The Intelligence Sandwich

**Alt text:** Architecture diagram of the Intelligence Sandwich pattern showing proprietary pre-processing and post-processing layers sandwiching a commodity swappable LLM core for music attribution systems.

**Caption:** *The Intelligence Sandwich architecture with proprietary value in pre/post-processing layers sandwiching a commodity LLM core.*

**Embed:**
```markdown
![Architecture diagram of the Intelligence Sandwich pattern showing proprietary pre-processing and post-processing layers sandwiching a commodity swappable LLM core for music attribution systems.](docs/figures/repo-figures/assets/fig-persona-04-intelligence-sandwich.jpg)
```

---

### fig-persona-05: Cognitive Memory Architecture

**Alt text:** Dual-row mapping diagram of the Atkinson-Shiffrin cognitive memory model to LLM persona memory implementation, showing four tiers mapped to Conversation Buffer, Session Context, Vector DB, and Persistent User Profile.

**Caption:** *Cognitive memory architecture mapping the Atkinson-Shiffrin model to LLM implementation tiers.*

**Embed:**
```markdown
![Dual-row mapping diagram of the Atkinson-Shiffrin cognitive memory model to LLM persona memory implementation, showing four tiers mapped to Conversation Buffer, Session Context, Vector DB, and Persistent User Profile.](docs/figures/repo-figures/assets/fig-persona-05-memory-architecture-cognitive.jpg)
```

---

### fig-persona-06: Persona Drift Cliff at 8 Turns

**Alt text:** Line chart of persona consistency score over 20 conversation turns showing a sharp drift cliff at turn 8 for text modality and turns 4-5 for voice modality, with root cause analysis of attention decay on persona tokens.

**Caption:** *Persona consistency score over 20 turns showing drift cliff at turn 8 (text) and turns 4-5 (voice).*

**Embed:**
```markdown
![Line chart of persona consistency score over 20 conversation turns showing a sharp drift cliff at turn 8 for text modality and turns 4-5 for voice modality, with root cause analysis of attention decay on persona tokens.](docs/figures/repo-figures/assets/fig-persona-06-drift-cliff-8-turns.jpg)
```

---

### fig-persona-07: Drift Detection Methods Comparison

**Alt text:** Four-panel comparison of persona drift detection methods: embedding-based, probe-based, attention-based, and persona vector monitoring with thresholds and trade-offs.

**Caption:** *Four drift detection methods compared with cost, accuracy, and trade-off analysis.*

**Embed:**
```markdown
![Four-panel comparison of persona drift detection methods: embedding-based, probe-based, attention-based, and persona vector monitoring with thresholds and trade-offs.](docs/figures/repo-figures/assets/fig-persona-07-drift-detection-methods.jpg)
```

---

### fig-persona-08: EchoMode SyncScope Drift Detection System

**Alt text:** Split-panel diagram of the EchoMode SyncScope drift detection system showing EWMA-smoothed SyncScore time series with threshold-based drift detection and a four-state behavioral model with persona repair trigger.

**Caption:** *EchoMode SyncScope system with EWMA-smoothed drift detection and four-state behavioral model.*

**Embed:**
```markdown
![Split-panel diagram of the EchoMode SyncScope drift detection system showing EWMA-smoothed SyncScore time series with threshold-based drift detection and a four-state behavioral model with persona repair trigger.](docs/figures/repo-figures/assets/fig-persona-08-echomode-syncscope.jpg)
```

---

### fig-persona-09: Over-Personalization Failure Taxonomy

**Alt text:** Three-panel taxonomy of over-personalization failure modes: irrelevance, repetition, and sycophancy, with Self-ReCheck mitigations and 26.2-61.1% memory-induced performance drops.

**Caption:** *Three over-personalization failure modes with mitigations and quantified performance impact.*

**Embed:**
```markdown
![Three-panel taxonomy of over-personalization failure modes: irrelevance, repetition, and sycophancy, with Self-ReCheck mitigations and 26.2-61.1% memory-induced performance drops.](docs/figures/repo-figures/assets/fig-persona-09-over-personalization-taxonomy.jpg)
```

---

### fig-persona-10: Intent Legitimation Attack via Personalization

**Alt text:** Split-panel security diagram contrasting normal interaction where a harmful query is blocked versus an intent legitimation attack where accumulated benign memories bypass the safety filter with 15.8-243.7% attack success increase.

**Caption:** *Intent legitimation attack showing how accumulated benign memories shift harmful queries into the model's benign region.*

**Embed:**
```markdown
![Split-panel security diagram contrasting normal interaction where a harmful query is blocked versus an intent legitimation attack where accumulated benign memories bypass the safety filter with 15.8-243.7% attack success increase.](docs/figures/repo-figures/assets/fig-persona-10-intent-legitimation.jpg)
```

---

### fig-persona-11: Memory System Comparison

**Alt text:** Multi-panel comparison of four agent memory systems for persona coherence -- Letta, Zep, Mem0, and MemoryOS -- showing architecture diagrams, performance metrics, and convergence on temporal knowledge graphs.

**Caption:** *Four leading agent memory systems compared across architecture and performance, all converging on temporal knowledge graphs.*

**Embed:**
```markdown
![Multi-panel comparison of four agent memory systems for persona coherence -- Letta, Zep, Mem0, and MemoryOS -- showing architecture diagrams, performance metrics, and convergence on temporal knowledge graphs.](docs/figures/repo-figures/assets/fig-persona-11-memory-system-comparison.jpg)
```

---

### fig-persona-12: Privacy-Preserving Personalization

**Alt text:** Tiered diagram of privacy-preserving personalization showing three data minimization levels achieving 97.2% effectiveness at maximum privacy.

**Caption:** *Three privacy tiers for personalization data minimization retaining 97.2% effectiveness at categories-only.*

**Embed:**
```markdown
![Tiered diagram of privacy-preserving personalization showing three data minimization levels achieving 97.2% effectiveness at maximum privacy.](docs/figures/repo-figures/assets/fig-persona-12-privacy-preserving-personalization.jpg)
```

---

### fig-persona-13: User Modeling Spectrum

**Alt text:** Stepped diagram of user modeling spectrum with five personalization depth levels, with the music attribution scaffold positioned at Level 3 (Proficiency Tiers).

**Caption:** *Five levels of user modeling depth with scaffold positioned at Level 3.*

**Embed:**
```markdown
![Stepped diagram of user modeling spectrum with five personalization depth levels, with the music attribution scaffold positioned at Level 3 (Proficiency Tiers).](docs/figures/repo-figures/assets/fig-persona-13-user-modeling-spectrum.jpg)
```

---

### fig-persona-14: Celebrity Persona Ethics

**Alt text:** Consent-first decision flowchart for celebrity AI persona ethics showing PRAC3 dimension checkpoints gating progression to A3 artist-verified approval for music attribution.

**Caption:** *Decision flowchart for celebrity AI persona ethics with PRAC3 checkpoints.*

**Embed:**
```markdown
![Consent-first decision flowchart for celebrity AI persona ethics showing PRAC3 dimension checkpoints gating progression to A3 artist-verified approval for music attribution.](docs/figures/repo-figures/assets/fig-persona-14-celebrity-persona-ethics.jpg)
```

---

### fig-persona-15: Suno vs Soundverse

**Alt text:** Split-panel comparison of Suno Personas inference-based style capture versus Soundverse DNA consent-by-design attribution, highlighting the quality gap between A1 and A2-A3 provenance.

**Caption:** *Split comparison of inference-based versus consent-by-design attribution approaches.*

**Embed:**
```markdown
![Split-panel comparison of Suno Personas inference-based style capture versus Soundverse DNA consent-by-design attribution, highlighting the quality gap between A1 and A2-A3 provenance.](docs/figures/repo-figures/assets/fig-persona-15-suno-vs-soundverse.jpg)
```

---

### fig-persona-16: Persona Platform Landscape

**Alt text:** 2x2 landscape matrix of persona platforms mapping commercial versus open-source and text versus voice modalities, revealing no single platform bridges persona coherence across both modalities.

**Caption:** *2x2 landscape of persona platforms revealing no single platform bridges both modalities.*

**Embed:**
```markdown
![2x2 landscape matrix of persona platforms mapping commercial versus open-source and text versus voice modalities, revealing no single platform bridges persona coherence across both modalities.](docs/figures/repo-figures/assets/fig-persona-16-persona-platform-landscape.jpg)
```

---

### fig-persona-17: Evaluation Benchmark Comparison

**Alt text:** Multi-panel comparison of five persona evaluation benchmarks -- PersonaGym, CharacterBench, InCharacter, CoSER, and RoleBench -- including the finding that GPT-4 matched LLaMA-3-8B on persona adherence.

**Caption:** *Five persona evaluation benchmarks compared with the key finding about model size vs adherence.*

**Embed:**
```markdown
![Multi-panel comparison of five persona evaluation benchmarks -- PersonaGym, CharacterBench, InCharacter, CoSER, and RoleBench -- including the finding that GPT-4 matched LLaMA-3-8B on persona adherence.](docs/figures/repo-figures/assets/fig-persona-17-evaluation-benchmark-comparison.jpg)
```

---

### fig-persona-18: Confidence as Character

**Alt text:** Four-tier comparison of traditional confidence badges versus persona-appropriate uncertainty language across A3 to A0 assurance levels for transparent confidence in music attribution.

**Caption:** *Four assurance tiers with traditional badges alongside persona-appropriate language expressions.*

**Embed:**
```markdown
![Four-tier comparison of traditional confidence badges versus persona-appropriate uncertainty language across A3 to A0 assurance levels for transparent confidence in music attribution.](docs/figures/repo-figures/assets/fig-persona-18-confidence-as-character.jpg)
```

---

### fig-persona-19: Dual Persona Architecture

**Alt text:** Dual persona architecture diagram showing the Attribution Agent (system voice) and Digital Twin (artist voice) separated by a consent gate barrier, sharing common memory and confidence scoring.

**Caption:** *Dual persona architecture with Attribution Agent and Digital Twin separated by consent barrier.*

**Embed:**
```markdown
![Dual persona architecture diagram showing the Attribution Agent (system voice) and Digital Twin (artist voice) separated by a consent gate barrier, sharing common memory and confidence scoring.](docs/figures/repo-figures/assets/fig-persona-19-dual-persona-architecture.jpg)
```

---

### fig-persona-20: Multi-Agent Attribution

**Alt text:** Multi-agent attribution architecture flowchart with central orchestrator routing queries to four specialized agents then meta-agent synthesis producing a unified persona voice.

**Caption:** *Multi-agent attribution architecture with four specialized agents and unified persona voice.*

**Embed:**
```markdown
![Multi-agent attribution architecture flowchart with central orchestrator routing queries to four specialized agents then meta-agent synthesis producing a unified persona voice.](docs/figures/repo-figures/assets/fig-persona-20-multi-agent-attribution.jpg)
```

---

### fig-persona-21: Cross-Channel State Synchronization

**Alt text:** Flowchart showing cross-channel state synchronization where Chat, Voice, and MCP channels read and write bidirectionally to a shared PostgreSQL and pgvector backend state store.

**Caption:** *Cross-channel state synchronization with three channels converging on shared PostgreSQL + pgvector backend.*

**Embed:**
```markdown
![Flowchart showing cross-channel state synchronization where Chat, Voice, and MCP channels read and write bidirectionally to a shared PostgreSQL and pgvector backend state store.](docs/figures/repo-figures/assets/fig-persona-21-cross-channel-state.jpg)
```

---

### fig-persona-22: PRD Expansion -- 5 Persona Decision Nodes

**Alt text:** PRD decision network expansion showing five new persona coherence nodes forming a persona pillar with dependency arrows to three existing nodes for music attribution.

**Caption:** *PRD expansion showing 5 new persona decision nodes forming a persona pillar.*

**Embed:**
```markdown
![PRD decision network expansion showing five new persona coherence nodes forming a persona pillar with dependency arrows to three existing nodes for music attribution.](docs/figures/repo-figures/assets/fig-persona-22-prd-expansion-5-nodes.jpg)
```

---

### fig-persona-23: PersonaPlex Architecture

**Alt text:** NVIDIA PersonaPlex architecture diagram showing dual-input persona control achieving 0.07s speaker-switch latency versus Gemini Live 1.3s for voice agent persona coherence.

**Caption:** *NVIDIA PersonaPlex architecture with dual-input persona control and 0.07s speaker-switch latency.*

**Embed:**
```markdown
![NVIDIA PersonaPlex architecture diagram showing dual-input persona control achieving 0.07s speaker-switch latency versus Gemini Live 1.3s for voice agent persona coherence.](docs/figures/repo-figures/assets/fig-persona-23-personaplex-architecture.jpg)
```

---

### fig-persona-24: Parasocial Bond Spectrum

**Alt text:** Parasocial bond spectrum for music attribution agents showing five stages from Transactional Tool to Trusted Advisor design target, with guardrails preventing dangerous emotional dependency.

**Caption:** *Five-stage parasocial bond spectrum with Trusted Advisor as target and guardrails against dependency.*

**Embed:**
```markdown
![Parasocial bond spectrum for music attribution agents showing five stages from Transactional Tool to Trusted Advisor design target, with guardrails preventing dangerous emotional dependency.](docs/figures/repo-figures/assets/fig-persona-24-parasocial-bond-spectrum.jpg)
```

---

### fig-persona-25: Warmth-Competence Matrix

**Alt text:** Warmth-competence 2x2 matrix for AI agent persona design showing High Warmth + High Competence as target, with Ibrahim et al. warning that warmth training degrades factual reliability.

**Caption:** *Warmth-competence matrix with target quadrant highlighting the warmth-reliability tension.*

**Embed:**
```markdown
![Warmth-competence 2x2 matrix for AI agent persona design showing High Warmth + High Competence as target, with Ibrahim et al. warning that warmth training degrades factual reliability.](docs/figures/repo-figures/assets/fig-persona-25-warmth-competence-matrix.jpg)
```

---

### fig-persona-26: Brand Voice Validation

**Alt text:** Brand voice validation pipeline showing a deterministic regex and rule-based validator as a last-mile guardrail between LLM output and user delivery, catching persona coherence violations.

**Caption:** *Deterministic brand voice validator as last-mile guardrail catching violation patterns LLM guardrails miss.*

**Embed:**
```markdown
![Brand voice validation pipeline showing a deterministic regex and rule-based validator as a last-mile guardrail between LLM output and user delivery, catching persona coherence violations.](docs/figures/repo-figures/assets/fig-persona-26-brand-voice-validation.jpg)
```

---

### fig-persona-27: Power User Economics

**Alt text:** Power user economics comparison of three AI agent pricing models showing only Credit-Based earning profit, with 5-10% of power users consuming 60-80% of resources.

**Caption:** *Three pricing scenarios demonstrating credit-based pricing as the only sustainable model.*

**Embed:**
```markdown
![Power user economics comparison of three AI agent pricing models showing only Credit-Based earning profit, with 5-10% of power users consuming 60-80% of resources.](docs/figures/repo-figures/assets/fig-persona-27-power-user-economics.jpg)
```

---

### fig-persona-28: Modality Cost Hierarchy (MISMATCH)

**Alt text:** Modality cost hierarchy showing four ascending interaction modalities from Text to Voice to Avatar to Video, spanning a 10,000 to 100,000x cost range mapped to pricing tiers.

**Caption:** *Four interaction modalities in ascending cost spanning 10,000-100,000x range mapped to pricing tiers.*

**Embed:**
```markdown
![Modality cost hierarchy showing four ascending interaction modalities from Text to Voice to Avatar to Video, spanning a 10,000 to 100,000x cost range mapped to pricing tiers.](docs/figures/repo-figures/assets/fig-persona-28-modality-cost-hierarchy.jpg)
```

---

### fig-persona-29: Persona Coherence Recommended Stack

**Alt text:** Six-layer persona coherence recommended technology stack for music attribution building on PydanticAI MVP foundation with Memory, Persona Engine, Drift Detection, Voice Persona, and Evaluation layers.

**Caption:** *Six-layer recommended stack building on existing PydanticAI MVP through evaluation layer.*

**Embed:**
```markdown
![Six-layer persona coherence recommended technology stack for music attribution building on PydanticAI MVP foundation with Memory, Persona Engine, Drift Detection, Voice Persona, and Evaluation layers.](docs/figures/repo-figures/assets/fig-persona-29-persona-coherence-recommended-stack.jpg)
```

---

### fig-persona-30: Drift as Controllable Equilibrium

**Alt text:** Split-panel comparison of persona drift models showing old view of inevitable monotonic decay versus new view from Drift No More research of bounded controllable equilibrium maintained by restoring forces.

**Caption:** *Split-panel comparison of old monotonic decay view versus new bounded equilibrium view of persona drift.*

**Embed:**
```markdown
![Split-panel comparison of persona drift models showing old view of inevitable monotonic decay versus new view from Drift No More research of bounded controllable equilibrium maintained by restoring forces.](docs/figures/repo-figures/assets/fig-persona-30-drift-equilibrium.jpg)
```

---

## Statistics

| Category | Count |
|----------|-------|
| fig-agent | 10 |
| fig-backend | 20 |
| fig-choice | 18 |
| fig-ecosystem | 16 |
| fig-frontend | 15 |
| fig-howto | 9 |
| fig-landscape | 33 |
| fig-persona | 30 |
| fig-prd | 10 |
| fig-repo | 31 |
| fig-scenario | 8 |
| fig-theory | 22 |
| fig-trends | 8 |
| fig-voice | 32 |
| **Total** | **262** |
