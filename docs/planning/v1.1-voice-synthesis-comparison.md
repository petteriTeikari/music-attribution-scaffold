# v1.1 Voice Synthesis Comparison: Free TTS Engines for Short Command Confirmations

**Status:** Planning
**Created:** 2026-02-26
**Version:** 1.0
**Context:** Music Attribution Scaffold v1.1 voice interface — TTS for voice command responses

**Related:**
- `src/music_attribution/voice/config.py` — VoiceConfig (TTSProvider enum: piper, kokoro, elevenlabs, cartesia)
- `src/music_attribution/voice/pipeline.py` — Pipecat pipeline factory
- `docs/prd/decisions/L3-implementation/voice-agent-stack.decision.yaml` — PRD decision node
- `docs/planning/voice-agent-benchmarking-plan.md` — Benchmarking plan (uses Piper for synthetic WAV generation)
- `docs/prd/voice-agent/toc-voice-agent.md` — Voice agent architecture

**Target utterances** (2-8 seconds, latency-critical):
- "Attribution saved for Headlock"
- "Confidence updated to 0.87, verified by MusicBrainz and Discogs"
- "Three tracks queued for review"
- "Found 12 results for Imogen Heap"
- "Permission denied — AI training requires artist consent"

---

## 1. Executive Summary

**Recommendation: Kokoro 82M for MVP, Chatterbox Turbo for production upgrade.**

Kokoro 82M offers the best quality-per-parameter ratio of any open TTS model in early 2026. At 82M parameters with Apache 2.0 licensing, it delivers sub-300ms first-byte latency and has held the #1 position on the Hugging Face TTS Arena leaderboard. For short command confirmations (our primary use case), its compact size means trivial deployment and negligible memory footprint alongside the existing Pipecat voice pipeline.

When the platform scales beyond single-language MVP — particularly when multilingual support, emotion-tagged responses, or audio watermarking become requirements — Chatterbox Turbo (MIT license, 350M params, 23 languages, built-in PerTh watermarking) provides the natural upgrade path. The watermarking capability is especially relevant to the music attribution domain, where provenance of synthesized audio is a first-class concern.

For environments where GPU is unavailable (edge deployment, CI runners, budget Hetzner VPS), Piper with ONNX runtime remains the ultra-light fallback at sub-50ms on CPU. The current `TTSProvider.PIPER` enum value in `VoiceConfig` already supports this path.

| Phase | Engine | Why |
|-------|--------|-----|
| **MVP** | Kokoro 82M | Smallest high-quality model, fastest for all lengths, trivial install |
| **Production** | Chatterbox Turbo | MIT, watermarking, emotion tags, 23 languages |
| **Streaming** | CosyVoice2-0.5B | 150ms first-byte streaming for real-time conversational UI |
| **Edge/CI fallback** | Piper (ONNX) | CPU-only, 50MB, sub-50ms |
| **Zero-compute fallback** | edge-tts | Free Azure Neural voices, no local GPU |

---

## 2. Comparison Matrix

| Model | Params | License | Latency (first byte) | Quality Rating | Languages | Voice Cloning | GPU Required | Python Install Command | Short Utterance Suitability (1-5) |
|-------|--------|---------|---------------------|----------------|-----------|---------------|-------------|----------------------|-----------------------------------|
| **Kokoro 82M** | 82M | Apache 2.0 | <0.3s | 9/10 (#1 HF Arena) | English (+ limited multilingual) | Zero-shot (limited) | Optional (CPU viable) | `uv add kokoro` | **5** |
| **Chatterbox Turbo** | 350M | MIT | Single-step decode (~0.2s) | 9/10 | 23 languages | Zero-shot + emotion | Yes (4GB VRAM) | `uv add chatterbox-tts` | **5** |
| **CosyVoice2-0.5B** | 500M | Apache 2.0 | 150ms (streaming) | 8.5/10 | Mandarin, English, + others | Zero-shot cross-lingual | Yes (6GB VRAM) | `uv add cosyvoice2` | **4** |
| **Orpheus TTS** | 150M-3B | Apache 2.0 | 25-50ms (streaming) | 8/10 | English primary | Limited | Yes (varies by size) | `uv add orpheus-tts` | **4** |
| **Piper** | <50M (ONNX) | MIT | <50ms (CPU) | 7/10 | 30+ languages | No (pretrained voices) | No (CPU ONNX) | `uv add piper-tts` | **4** |
| **piper1-gpl** | <50M (ONNX) | GPL 3.0 | <50ms (CPU) | 7.5/10 | 30+ languages | No | No (CPU ONNX) | `uv add piper1-gpl` | **4** |
| **F5-TTS** | ~600M | Apache-like (CC-BY-NC for some weights) | RTF 0.15 (~0.4s) | 8.5/10 | Multilingual | Zero-shot | Yes (8GB VRAM) | `uv add f5-tts` | **3** |
| **Sesame CSM** | 1B | Apache 2.0 | ~0.5s | 8.5/10 (conversational) | English | Zero-shot | Yes (8GB VRAM) | `uv add sesame-csm` | **3** |
| **Parler TTS** | 938M-2.2B | Open (HF) | ~1s | 7.5/10 | English primary | Style-described (text prompt) | Yes (8-12GB VRAM) | `uv add parler-tts` | **2** |
| **Bark** | ~1B | MIT | 2-5s | 7/10 (expressive) | Multilingual | Prompt-based | Yes (8GB VRAM) | `uv add bark` | **1** |
| **Coqui XTTS v2** | ~1.6B | Coqui Public License | ~1s | 8/10 | 17 languages | Zero-shot (excellent) | Yes (6GB VRAM) | `uv add tts` | **3** |
| **Mars5** | ~1B | AGPL 3.0 | ~0.8s | 8/10 | English | Zero-shot | Yes (8GB VRAM) | `uv add mars5-tts` | **3** |
| **MetaVoice** | 1.2B | Open (Apache-like) | ~1s | 7.5/10 | English | Zero-shot | Yes (8GB VRAM) | `uv add metavoice` | **2** |
| **edge-tts** | Cloud | Free (Azure Neural) | ~0.3s (network) | 8/10 | 300+ voices, 75+ locales | No | No (cloud API) | `uv add edge-tts` | **4** |
| **OpenAI TTS** | Cloud | Paid ($15/M chars) | ~0.3s (network) | 9.5/10 | 50+ languages | No (6 preset voices) | No (cloud API) | `uv add openai` | **5** (reference) |
| **Google Cloud TTS** | Cloud | Free 1M chars/mo WaveNet | ~0.3s (network) | 8.5/10 | 40+ languages | No | No (cloud API) | `uv add google-cloud-texttospeech` | **4** |
| **ElevenLabs** | Cloud | Free 20min/mo | ~0.3s (network) | 9.5/10 | 29 languages | Excellent clone | No (cloud API) | `uv add elevenlabs` | **5** (non-commercial) |

### Suitability Rating Criteria

For short command confirmations (2-8 seconds), the rating weights:
- **Latency** (40%): First-byte time for <10 word utterances
- **Quality at short length** (30%): Some models optimized for long narration degrade on short phrases
- **Install simplicity** (15%): Dependency count, model download size, ONNX vs PyTorch
- **License compatibility** (15%): Apache 2.0 / MIT preferred for this project

---

## 3. License Compatibility Analysis

The Music Attribution Scaffold is distributed under an open-source license. TTS engines must be compatible with Apache 2.0 / MIT redistribution.

### Fully Compatible (Green)

| Model | License | Notes |
|-------|---------|-------|
| Kokoro 82M | Apache 2.0 | Full commercial use, no restrictions |
| Chatterbox Turbo | MIT | Permissive, attribution only |
| CosyVoice2-0.5B | Apache 2.0 | Some model weights may have separate terms — verify per checkpoint |
| Orpheus TTS | Apache 2.0 | Full commercial use |
| Piper | MIT | Permissive, voice models individually licensed (most CC/MIT) |
| Sesame CSM | Apache 2.0 | Code Apache 2.0; verify model weight license separately |
| edge-tts | N/A (cloud) | Uses Azure TTS free tier; subject to Azure ToS (no redistribution of audio) |
| F5-TTS | Apache-like | Code is Apache 2.0; some pretrained weights CC-BY-NC — check specific checkpoint |
| MetaVoice | Open/Apache-like | Verify exact terms for commercial deployment |

### Problematic Licenses (Red/Amber)

| Model | License | Issue | Severity |
|-------|---------|-------|----------|
| **Mars5** | **AGPL 3.0** | Viral copyleft — any network interaction triggers source disclosure obligation. SaaS deployment of Mars5 would require open-sourcing the entire voice service. | **Incompatible with SaaS** |
| **piper1-gpl** | **GPL 3.0** | Copyleft via pedalboard dependency. If linked at runtime, the combined work inherits GPL. Separate process / subprocess boundary may mitigate. | **High risk** |
| **Coqui XTTS v2** | **Coqui Public License** | Explicitly non-commercial. "You may not use the Model or its outputs for commercial purposes." Free for research/personal only. | **Non-commercial only** |
| **ElevenLabs Free** | **Free tier ToS** | 20 min/month, non-commercial use only. Commercial requires paid plan ($5-330/mo). | **Non-commercial only** |
| **Bark** | MIT (code) | Code is MIT but some voice presets may carry additional restrictions. Verify per speaker. | **Low risk** — verify per voice |
| **Parler TTS** | HF/Open | Model weights under varying licenses depending on training data. Check specific checkpoint. | **Medium risk** — verify per checkpoint |

### Recommendation

Use only **Apache 2.0** or **MIT** licensed engines for the default open-source distribution. Commercial engines (ElevenLabs, OpenAI TTS) can be supported via the existing `TTSProvider` enum swap in `VoiceConfig`, documented as opt-in with user-provided API keys.

The `VoiceConfig` already supports this pattern:

```python
class TTSProvider(str, Enum):
    PIPER = "piper"        # MIT — current default
    KOKORO = "kokoro"      # Apache 2.0 — recommended MVP default
    ELEVENLABS = "elevenlabs"  # Commercial (user-provided key)
    CARTESIA = "cartesia"      # Commercial (user-provided key)
```

**Action item for v1.1:** Add `CHATTERBOX = "chatterbox"` and `COSYVOICE = "cosyvoice"` to the enum.

---

## 4. Recommended Architecture

### 4.1 MVP Phase: Kokoro 82M

**Why Kokoro:**
- Smallest high-quality model (82M params, ~160MB on disk)
- Fastest for all utterance lengths (<0.3s first byte, even on CPU)
- #1 on Hugging Face TTS Arena leaderboard (as of Feb 2026)
- Apache 2.0 — no license friction
- Trivial install: single `uv add kokoro` with minimal transitive dependencies
- Strong on English short utterances — exactly our use case
- Runs on CPU for development; GPU for production latency

**Integration pattern:**

```python
"""Kokoro TTS integration for voice command confirmations.

Replaces Piper as default TTS in v1.1.
"""

from __future__ import annotations

import asyncio
import logging
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    pass

logger = logging.getLogger(__name__)


class KokoroTTSService:
    """Async wrapper around Kokoro 82M for Pipecat pipeline integration.

    Designed for short command confirmations (2-8 seconds).
    Lazy-loads model on first synthesis call to avoid import-time cost.
    """

    def __init__(
        self,
        model_id: str = "kokoro-82m",
        voice: str = "af_heart",
        sample_rate: int = 24000,
    ) -> None:
        self._model_id = model_id
        self._voice = voice
        self._sample_rate = sample_rate
        self._pipeline = None  # Lazy init

    async def _ensure_loaded(self) -> None:
        """Lazy-load the Kokoro pipeline on first use."""
        if self._pipeline is not None:
            return

        # Import at call time — avoids requiring kokoro at import
        try:
            from kokoro import KPipeline  # type: ignore[import-untyped]
        except ImportError:
            msg = (
                "kokoro not installed. Install with: uv add kokoro\n"
                "Or switch to piper: VOICE_TTS_PROVIDER=piper"
            )
            raise ImportError(msg) from None

        self._pipeline = await asyncio.to_thread(
            KPipeline, lang_code="a"  # "a" = American English
        )
        logger.info("Kokoro 82M loaded (voice=%s)", self._voice)

    async def synthesize(self, text: str) -> bytes:
        """Synthesize text to WAV bytes.

        Args:
            text: Short command confirmation text (2-8 seconds).

        Returns:
            WAV audio bytes (24kHz, 16-bit mono).
        """
        await self._ensure_loaded()
        assert self._pipeline is not None  # noqa: S101 — guarded by _ensure_loaded

        # Run synthesis in thread pool (Kokoro is synchronous)
        audio_segments = await asyncio.to_thread(
            lambda: list(
                self._pipeline(text, voice=self._voice, speed=1.0)
            )
        )

        # Concatenate audio segments
        import numpy as np

        audio = np.concatenate(
            [seg[2] for seg in audio_segments]  # (graphemes, phonemes, audio)
        )

        # Convert to WAV bytes
        import io
        import wave

        buffer = io.BytesIO()
        with wave.open(buffer, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)  # 16-bit
            wf.setframerate(self._sample_rate)
            wf.writeframes((audio * 32767).astype(np.int16).tobytes())

        return buffer.getvalue()
```

**Fallback chain:**

```
Kokoro 82M (default)
  |-- fails to load (no GPU, missing weights)
  v
edge-tts (zero-compute cloud fallback)
  |-- fails (no internet, rate limited)
  v
Piper ONNX (CPU ultra-light fallback)
```

This fallback is configured via `VoiceConfig`:

```bash
# Default (Kokoro)
VOICE_TTS_PROVIDER=kokoro

# Cloud fallback (no local compute)
VOICE_TTS_PROVIDER=edge_tts

# CPU-only fallback
VOICE_TTS_PROVIDER=piper
```

### 4.2 Production Phase: Chatterbox Turbo

**Why upgrade from Kokoro to Chatterbox Turbo:**

1. **MIT license** — even more permissive than Apache 2.0
2. **PerTh watermarking** — imperceptible audio watermarking built into the model. For a music attribution platform, being able to watermark synthesized voice output is provenance-by-design. This aligns directly with the A0-A3 assurance level framework from the companion paper
3. **Emotion tags** — `[happy]`, `[serious]`, `[concerned]` prefixes allow the voice to match the semantic content of confirmations (e.g., serious tone for "Permission denied", upbeat for "Attribution saved")
4. **23 languages** — multilingual support without switching models
5. **Single-step decode** — Turbo variant uses non-autoregressive decoding for consistent sub-200ms latency regardless of utterance length

**When to upgrade:**
- Voice usage exceeds 1000 requests/day (Kokoro CPU becomes a bottleneck)
- Multilingual support is requested by users
- Watermarking becomes a compliance requirement (EU AI Act Article 50 synthetic content labeling)
- Emotion-tagged responses improve user satisfaction in A/B testing

**Emotion tag mapping for attribution commands:**

```python
EMOTION_MAP: dict[str, str] = {
    "attribution_saved": "[happy]",
    "confidence_updated": "[neutral]",
    "tracks_queued": "[neutral]",
    "search_results": "[neutral]",
    "permission_denied": "[serious]",
    "low_confidence_warning": "[concerned]",
    "verification_complete": "[happy]",
    "error_occurred": "[concerned]",
}
```

### 4.3 Streaming Phase: CosyVoice2-0.5B

**Why CosyVoice2 for streaming:**
- 150ms first-byte streaming latency via chunk-aware causal flow matching
- Designed for real-time conversational interfaces (full-duplex voice)
- Apache 2.0 license
- Strong multilingual support (Mandarin + English + others)

**When to adopt:**
- The voice interface evolves from single-turn commands to multi-turn conversation
- Users expect real-time "digital twin" voice interactions (the Imogen Heap persona use case)
- WebRTC transport replaces WebSocket for production (Pipecat Daily transport)

This phase corresponds to the `custom_websocket` and `livekit_agents` options in the `voice-agent-stack.decision.yaml` PRD node.

---

## 5. Audio Degradation Testing Module

Voice commands in real-world conditions encounter background noise, room reverb, codec compression, and microphone frequency response limitations. This section defines a degradation pipeline to validate the STT recognition under realistic conditions.

### 5.1 Libraries

| Library | License | Purpose | Install |
|---------|---------|---------|---------|
| **audiomentations** | MIT | Primary audio augmentation (noise, gain, pitch, time stretch) | `uv add audiomentations` |
| **pedalboard** | GPL 3.0 | Music-specific effects (reverb, compression, EQ, chorus) | `uv add pedalboard` (GPL — use in test suite only, not redistributed) |
| **pyroomacoustics** | MIT | Room impulse response simulation (shoebox model) | `uv add pyroomacoustics` |

**License note on pedalboard:** GPL 3.0 is acceptable in the test suite because tests are not redistributed as part of the library. The test dependency is declared in `[project.optional-dependencies] test = [...]` in `pyproject.toml`, not in the core dependencies. Users who `uv add music-attribution` never install pedalboard.

### 5.2 Degradation Presets

| Preset | SNR | Reverb RT60 | Codec | Mic Response | Use Case |
|--------|-----|-------------|-------|-------------|----------|
| **Clean** | inf | 0 | WAV 16-bit | Full range (20Hz-20kHz) | Baseline — pristine studio conditions |
| **Office** | 25dB | 0.3s | Opus 64kbps | 80Hz-16kHz | Typical desktop microphone in quiet room |
| **Noisy cafe** | 10dB | 0.6s | MP3 128kbps | 100Hz-12kHz | Mobile phone in a moderately noisy environment |
| **Extreme** | 5dB | 1.2s | MP3 64kbps | 200Hz-8kHz | Stress test — loud environment, poor hardware |

### 5.3 Degradation Pipeline Implementation

```python
"""Audio degradation presets for voice command recognition testing.

Uses audiomentations (MIT) for primary augmentation and
pyroomacoustics (MIT) for room simulation.
"""

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    import numpy as np


class DegradationPreset(Enum):
    """Named degradation levels for voice command testing."""

    CLEAN = "clean"
    OFFICE = "office"
    NOISY_CAFE = "noisy_cafe"
    EXTREME = "extreme"


@dataclass(frozen=True)
class DegradationConfig:
    """Parameters for a single degradation preset."""

    snr_db: float          # Signal-to-noise ratio (inf = no noise)
    reverb_rt60: float     # Reverberation time in seconds (0 = none)
    codec: str             # "wav", "opus_64k", "mp3_128k", "mp3_64k"
    low_cutoff_hz: float   # Mic low-frequency rolloff
    high_cutoff_hz: float  # Mic high-frequency rolloff


PRESETS: dict[DegradationPreset, DegradationConfig] = {
    DegradationPreset.CLEAN: DegradationConfig(
        snr_db=float("inf"), reverb_rt60=0.0,
        codec="wav", low_cutoff_hz=20.0, high_cutoff_hz=20000.0,
    ),
    DegradationPreset.OFFICE: DegradationConfig(
        snr_db=25.0, reverb_rt60=0.3,
        codec="opus_64k", low_cutoff_hz=80.0, high_cutoff_hz=16000.0,
    ),
    DegradationPreset.NOISY_CAFE: DegradationConfig(
        snr_db=10.0, reverb_rt60=0.6,
        codec="mp3_128k", low_cutoff_hz=100.0, high_cutoff_hz=12000.0,
    ),
    DegradationPreset.EXTREME: DegradationConfig(
        snr_db=5.0, reverb_rt60=1.2,
        codec="mp3_64k", low_cutoff_hz=200.0, high_cutoff_hz=8000.0,
    ),
}


def apply_degradation(
    audio: np.ndarray,
    sample_rate: int,
    preset: DegradationPreset,
) -> np.ndarray:
    """Apply a degradation preset to clean audio.

    Args:
        audio: Input audio as float32 numpy array, shape (samples,).
        sample_rate: Sample rate in Hz (typically 16000).
        preset: Named degradation level.

    Returns:
        Degraded audio as float32 numpy array.
    """
    from audiomentations import (  # type: ignore[import-untyped]
        AddGaussianNoise,
        BandPassFilter,
        Compose,
    )

    config = PRESETS[preset]

    transforms = []

    # Additive noise
    if config.snr_db != float("inf"):
        transforms.append(
            AddGaussianNoise(
                min_snr_db=config.snr_db,
                max_snr_db=config.snr_db,
                p=1.0,
            )
        )

    # Microphone frequency response simulation
    if config.low_cutoff_hz > 20 or config.high_cutoff_hz < 20000:
        transforms.append(
            BandPassFilter(
                min_low_cutoff_freq=config.low_cutoff_hz,
                max_low_cutoff_freq=config.low_cutoff_hz,
                min_high_cutoff_freq=config.high_cutoff_hz,
                max_high_cutoff_freq=config.high_cutoff_hz,
                p=1.0,
            )
        )

    pipeline = Compose(transforms)
    degraded = pipeline(audio, sample_rate=sample_rate)

    # Room reverb via pyroomacoustics (if RT60 > 0)
    if config.reverb_rt60 > 0:
        import pyroomacoustics as pra  # type: ignore[import-untyped]

        room_dim = [8.0, 6.0, 3.0]  # Generic office room
        room = pra.ShoeBox(
            room_dim,
            fs=sample_rate,
            rt60_tgt=config.reverb_rt60,
            materials=pra.Material("hard_surface"),
        )
        room.add_source([2.0, 3.0, 1.7], signal=degraded)
        room.add_microphone([5.0, 3.0, 1.5])
        room.simulate()
        degraded = room.mic_signals[0, : len(degraded)]

    return degraded
```

### 5.4 Test Matrix

**Step 1:** Generate 20 command utterances via Kokoro TTS.

| ID | Command Text | Expected Parse |
|----|-------------|---------------|
| 1 | "Attribution saved for Headlock" | action=save, track=Headlock |
| 2 | "Confidence updated to 0.87" | action=update_confidence, value=0.87 |
| 3 | "Three tracks queued for review" | action=queue, count=3 |
| 4 | "Found 12 results for Imogen Heap" | action=search, count=12, artist=Imogen Heap |
| 5 | "Permission denied, AI training requires artist consent" | action=deny, reason=consent |
| 6 | "Search for tracks by Frou Frou" | action=search, artist=Frou Frou |
| 7 | "Assurance level upgraded to A2" | action=upgrade, level=A2 |
| 8 | "MusicBrainz and Discogs sources agree" | action=verify, sources=[musicbrainz, discogs] |
| 9 | "Low confidence warning for track seven" | action=warn, track_num=7 |
| 10 | "Batch review complete, 15 of 20 approved" | action=batch_complete, approved=15, total=20 |
| 11 | "What is the confidence score for Hide and Seek?" | query=confidence, track=Hide and Seek |
| 12 | "Show me low confidence attributions" | query=search, filter=low_confidence |
| 13 | "The songwriter should be Imogen Heap" | action=correct, field=songwriter, value=Imogen Heap |
| 14 | "I rate this attribution nine out of ten" | action=feedback, score=9 |
| 15 | "Correct the artist name to Frou Frou" | action=correct, field=artist, value=Frou Frou |
| 16 | "What does assurance level A2 mean?" | query=explain, level=A2 |
| 17 | "Export attribution report for this album" | action=export, scope=album |
| 18 | "Compare confidence across all tracks" | query=compare, scope=all_tracks |
| 19 | "Flag this attribution for manual review" | action=flag, reason=manual_review |
| 20 | "Cancel the last correction" | action=undo, target=last_correction |

**Step 2:** Apply each degradation preset to all 20 utterances (80 test WAVs total).

**Step 3:** Run through `faster-whisper` (small model) and command parser.

**Step 4:** Assert accuracy thresholds:

| Preset | Target WER | Target Command Parse Accuracy | Rationale |
|--------|-----------|-------------------------------|-----------|
| Clean | <5% | >=95% | Baseline — TTS artifacts only |
| Office | <10% | >=85% | Typical desktop user |
| Noisy cafe | <25% | >=70% | Mobile user in public |
| Extreme | <50% | >=50% | Stress test — graceful degradation |

### 5.5 Integration with CI

```
tests/
  fixtures/
    audio/
      commands/           # Pre-generated Kokoro WAVs (committed to repo)
        cmd_01_clean.wav
        cmd_01_office.wav
        cmd_01_noisy_cafe.wav
        cmd_01_extreme.wav
        ...
  unit/
    voice/
      test_degradation.py
```

**Test file structure:**

```python
"""Tests for audio degradation pipeline and STT accuracy under noise.

Parametrized across degradation presets and command utterances.
Fixtures are pre-generated Kokoro TTS WAVs stored in tests/fixtures/audio/.
"""

from __future__ import annotations

import pytest

from music_attribution.voice.degradation import (
    DegradationPreset,
    apply_degradation,
)

ACCURACY_THRESHOLDS: dict[DegradationPreset, float] = {
    DegradationPreset.CLEAN: 0.95,
    DegradationPreset.OFFICE: 0.85,
    DegradationPreset.NOISY_CAFE: 0.70,
    DegradationPreset.EXTREME: 0.50,
}


@pytest.fixture(scope="session")
def whisper_model():
    """Load faster-whisper model once per test session."""
    from faster_whisper import WhisperModel  # type: ignore[import-untyped]

    return WhisperModel("small", device="cpu", compute_type="int8")


@pytest.mark.parametrize("preset", list(DegradationPreset))
@pytest.mark.parametrize("command_id", range(1, 21))
def test_stt_accuracy_under_degradation(
    preset: DegradationPreset,
    command_id: int,
    whisper_model,
) -> None:
    """STT accuracy meets threshold for each degradation preset."""
    # Load pre-generated fixture
    # Transcribe with faster-whisper
    # Compute WER
    # Assert WER below threshold
    ...
```

**CI integration:**
- Pre-generated WAV fixtures are committed to `tests/fixtures/audio/` (small files, 16kHz mono, ~2KB each)
- Tests marked `@pytest.mark.slow` — excluded from default `make test`, included in `make test-full`
- CI runs degradation tests on GPU runner only (faster-whisper model loading)
- `audiomentations` and `pyroomacoustics` are test-only dependencies in `pyproject.toml`

---

## 6. Platform Integration Notes

### Tier and Pricing

Voice is a **Pro tier feature** as defined in the UX-first philosophy (`.claude/rules/11-ux-first-philosophy.md`):

| Tier | Voice Feature | TTS Engine |
|------|--------------|------------|
| **Free** | Aspirational UI only (mic animation, example queries, "Upgrade to Pro") | None |
| **Pro** | Full voice interaction, 100 queries/day | Kokoro 82M (default) |
| **Enterprise** | Unlimited voice, custom voice cloning, multilingual | Chatterbox Turbo |

**Credit cost:** 10 credits per voice agent query (includes STT + LLM reasoning + TTS round-trip).

### Deployment Architecture

```
┌─────────────┐     ┌──────────────────┐     ┌─────────────────┐
│  Frontend    │     │  API Server      │     │  TTS Service    │
│  (Next.js)  │────>│  (FastAPI)       │────>│  (Kokoro/Piper) │
│             │     │  AG-UI SSE       │     │  Separate proc  │
│  Web Speech │     │  PydanticAI      │     │  GPU container  │
│  API (STT)  │     │  agent           │     │                 │
└─────────────┘     └──────────────────┘     └─────────────────┘
      │                                              │
      │  Mic audio                          WAV audio│
      v                                              v
  Browser STT                               AudioContext
  (MVP)                                     playback
```

**Key design decisions:**
- TTS runs in a **separate service container** — not blocking the API server event loop
- Frontend uses **Web Speech API** for STT (MVP) — zero backend STT cost
- Backend **faster-whisper** endpoint for production STT (when Web Speech API is insufficient)
- TTS container is **stateless** — can be horizontally scaled independently

### Voice Flow (End-to-End)

```
User speaks into mic
  → Web Speech API (browser-native STT, MVP)
    → Text command
      → CopilotKit AG-UI SSE transport
        → PydanticAI agent (tool calls: search, explain, correct, feedback)
          → Response text
            → Kokoro TTS service (async, separate container)
              → WAV audio bytes
                → AudioContext playback in browser
```

**Latency budget for short confirmations:**

| Stage | Target | Notes |
|-------|--------|-------|
| Web Speech API STT | ~500ms | Browser-dependent, not controllable |
| AG-UI transport | ~50ms | SSE over HTTP/2 |
| PydanticAI agent | ~200-800ms | Depends on tool calls, LLM model |
| Kokoro TTS | <300ms | 82M model, short utterance |
| Audio transfer | ~50ms | WAV bytes over WebSocket |
| **Total** | **~1.1-1.7s** | Acceptable for command confirmation |

---

## 7. Risks and Mitigations

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Kokoro quality degrades on non-English | Voice unusable for non-English users | Medium | Fallback to edge-tts which has 300+ language/voice combinations via Azure Neural. Configure per-locale TTS provider in VoiceConfig |
| Chrome-only Web Speech API | Firefox/Safari users cannot use voice input | High | Backend STT endpoint (`/api/v1/voice/stt`) using faster-whisper as progressive enhancement. Feature-detect `webkitSpeechRecognition` and show fallback UI |
| Audio degradation breaks command recognition | False negatives in noisy environments, frustrated users | Medium | Confidence threshold on STT output — do not execute low-confidence commands, ask for repetition. "I didn't catch that — could you repeat?" response pattern |
| TTS latency spikes under load | Bad UX — long silence between command and confirmation | Low (short utterances) | Pre-generate common responses ("Attribution saved", "Search complete") at startup. Priority queue — voice responses skip behind batch jobs |
| Model size too large for edge deployment | Cannot deploy Kokoro on resource-constrained environments | Low | Piper ONNX as ultra-light alternative (~50MB total including voice model). Already supported via `TTSProvider.PIPER` in VoiceConfig |
| Kokoro model weights not cached | Cold start adds 5-10s model download on first request | Medium | Pre-pull weights in Docker image build (`RUN python -c "from kokoro import KPipeline; KPipeline(lang_code='a')"`) or mount from persistent volume |
| Voice cloning misuse | Synthesized voice used to impersonate artists | High (reputational) | Chatterbox Turbo's PerTh watermarking for production. Do NOT enable voice cloning in open-source distribution. Require explicit artist consent (A3 assurance level) for any cloned voice |
| Synthetic content disclosure (EU AI Act) | Legal non-compliance for AI-generated speech | Medium | Article 50 requires disclosure that content is AI-generated. Add `X-Synthetic-Speech: true` header and audio watermark. Chatterbox PerTh watermarking satisfies this |
| GPL contamination from pedalboard | Test dependency infects library license | Low | pedalboard is test-only dependency (`[project.optional-dependencies] test`). Not imported in any production code path. Verified by AST analysis of `src/` imports |

---

## 8. Sources

### Model Repositories

| Model | Repository | Paper/Docs |
|-------|-----------|------------|
| Kokoro 82M | [hexgrad/Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) | [HF Model Card](https://huggingface.co/hexgrad/Kokoro-82M) |
| Chatterbox | [resemble-ai/chatterbox](https://github.com/resemble-ai/chatterbox) | [Resemble AI Blog](https://www.resemble.ai/chatterbox/) |
| CosyVoice2 | [FunAudioLLM/CosyVoice2-0.5B](https://github.com/FunAudioLLM/CosyVoice2) | [arXiv:2412.10117](https://arxiv.org/abs/2412.10117) |
| Orpheus TTS | [canopylabs/orpheus-tts](https://github.com/canopylabs/orpheus-tts) | [Canopy Labs](https://canopylabs.ai/) |
| Piper | [rhasspy/piper](https://github.com/rhasspy/piper) | [Piper Docs](https://rhasspy.github.io/piper-samples/) |
| F5-TTS | [SWivid/F5-TTS](https://github.com/SWivid/F5-TTS) | [arXiv:2410.06885](https://arxiv.org/abs/2410.06885) |
| Sesame CSM | [SesameAILabs/csm](https://github.com/SesameAILabs/csm) | [Sesame AI](https://www.sesame.com/) |
| Parler TTS | [huggingface/parler-tts](https://github.com/huggingface/parler-tts) | [HF Blog](https://huggingface.co/blog/parler-tts) |
| Bark | [suno-ai/bark](https://github.com/suno-ai/bark) | [Suno AI](https://suno.ai/) |
| Coqui XTTS v2 | [coqui-ai/TTS](https://github.com/coqui-ai/TTS) | [Coqui License](https://coqui.ai/cpml) |
| Mars5 | [Camb-ai/MARS5-TTS](https://github.com/Camb-ai/MARS5-TTS) | [arXiv:2409.09201](https://arxiv.org/abs/2409.09201) |
| MetaVoice | [metavoiceio/metavoice-src](https://github.com/metavoiceio/metavoice-src) | [MetaVoice](https://themetavoice.xyz/) |
| edge-tts | [rany2/edge-tts](https://github.com/rany2/edge-tts) | [PyPI](https://pypi.org/project/edge-tts/) |

### Benchmarks and Leaderboards

| Resource | URL |
|----------|-----|
| HF TTS Arena Leaderboard | [huggingface.co/spaces/TTS-AGI/TTS-Arena](https://huggingface.co/spaces/TTS-AGI/TTS-Arena) |
| Open TTS Tracker | [huggingface.co/spaces/Pendrokar/open-tts-tracker](https://huggingface.co/spaces/Pendrokar/open-tts-tracker) |
| AIEWF TTS Evaluation | [aiewf.org/evaluations](https://aiewf.org/) |

### Audio Degradation Libraries

| Library | Repository | License |
|---------|-----------|---------|
| audiomentations | [iver56/audiomentations](https://github.com/iver56/audiomentations) | MIT |
| pedalboard | [spotify/pedalboard](https://github.com/spotify/pedalboard) | GPL 3.0 |
| pyroomacoustics | [LCAV/pyroomacoustics](https://github.com/LCAV/pyroomacoustics) | MIT |

### Regulatory References

| Document | Relevance |
|----------|-----------|
| [EU AI Act, Article 50](https://artificialintelligenceact.eu/article/50/) | Synthetic content disclosure obligation for AI-generated speech |
| [Teikari, P. (2026). "Music Attribution with Transparent Confidence." SSRN No. 6109087](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=6109087) | A0-A3 assurance levels, Oracle Problem, attribution-by-design framework |

### Project-Internal References

| File | Relevance |
|------|-----------|
| `src/music_attribution/voice/config.py` | TTSProvider enum (piper, kokoro, elevenlabs, cartesia) |
| `src/music_attribution/voice/pipeline.py` | Pipecat pipeline factory |
| `docs/prd/decisions/L3-implementation/voice-agent-stack.decision.yaml` | PRD decision node for voice stack |
| `docs/planning/voice-agent-benchmarking-plan.md` | Existing benchmarking plan (Piper + faster-whisper) |
| `.claude/rules/11-ux-first-philosophy.md` | Voice as Pro tier upsell, UX standards |
